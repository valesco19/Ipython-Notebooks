{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from operator import itemgetter\n",
    "import zipfile\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import os\n",
    "random.seed(2016)\n",
    "\n",
    "path = ('~/Datasets/Avito')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feature_map(features):\n",
    "    outfile = open('xgb.fmap', 'w')\n",
    "    for i, feat in enumerate(features):\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "    outfile.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_importance(gbm, features):\n",
    "    create_feature_map(features)\n",
    "    importance = gbm.get_fscore(fmap = 'xgb.fmap')\n",
    "    importance = sorted(importance.items(), key = itemgetter(1), reverse = True)\n",
    "    return importance   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intersect(a, b):\n",
    "    return list(set(a) & set(b))\n",
    "\n",
    "def print_features_importances(imp):\n",
    "    for i in range(len(imp)):\n",
    "        print(\"# \" + str(imp[i][1]))\n",
    "        print('output.remove(\\'' + imp[i][0] + '\\')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_default_test(train, test, features, target, random_state = 0):\n",
    "    eta = 0.1\n",
    "    max_depth = 5\n",
    "    subsample = 0.8\n",
    "    colsample_bytree = 0.8\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('XGBoost params. ETA: {}, MAX_DEPTH: {}, SUBSAMPLE: {}, COLSAMPLE_BY_TREE: {}'.format(eta,\n",
    "          max_depth, subsample, colsample_bytree))\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'booster': 'gbtree',\n",
    "        'eval_metric': 'auc',\n",
    "        'eta': eta,\n",
    "        'max_depth': max_depth,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'silent': 1,\n",
    "        'seed': random_state\n",
    "    }\n",
    "    num_boost_round = 260\n",
    "    early_stopping_rounds = 20\n",
    "    test_size = 0.1\n",
    "    \n",
    "    X_train, X_valid = train_test_split(train, test_size = test_size, random_state = random_state)\n",
    "    y_train = X_train[target]\n",
    "    y_valid = X_valid[target]\n",
    "    dtrain = xgb.DMatrix(X_train[features], y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid[features], y_valid)\n",
    "    \n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    gbm = xgb.train(params, dtrain, num_boost_round, evals = watchlist,\n",
    "        early_stopping_rounds = early_stopping_rounds, verbose_eval = True)\n",
    "    \n",
    "    print('Validating...')\n",
    "    check = gbm.predict(xgb.DMatrix(X_valid[features]), ntree_limit = gbm.best_ntree_limit)\n",
    "    score = roc_auc_score(X_valid[target].values, check)\n",
    "    print('Check error value: {:.6f}'.format(score))\n",
    "    \n",
    "    imp = get_importance(gbm, features)\n",
    "    print('Importances array: ', imp)\n",
    "    \n",
    "    print('Predict test set...')\n",
    "    test_prediction = gbm.predict(xgb.DMatrix(test[features]), ntree_limit = gbm.best_ntree_limit)\n",
    "    \n",
    "    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "    return test_prediction.tolist(), score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_submission(score, test, prediction):\n",
    "    # Make submission\n",
    "    now = datetime.datetime.now()\n",
    "    sub_file = 'submission_' + str(score) + '_' + str(now.strftime('%Y-%m-%d-%H-%M')) + '.csv'\n",
    "    print('Writing submission: ', sub_file)\n",
    "    f = open(sub_file, 'w')\n",
    "    f.write('id,probability\\n')\n",
    "    total = 0\n",
    "    for id in test['id']:\n",
    "        str1 = str(id) + ',' + str(prediction[total])\n",
    "        str1 += '\\n'\n",
    "        total += 1\n",
    "        f.write(str1)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(train, test):\n",
    "    trainval = list(train.columns.values)\n",
    "    testval = list(test.columns.values)\n",
    "    output = intersect(trainval, testval)\n",
    "    output.remove('itemID_1')\n",
    "    output.remove('itemID_2')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prep_train():\n",
    "    testing = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    types1 = {\n",
    "        'itemID_1': np.dtype(int),\n",
    "        'itemID_2': np.dtype(int),\n",
    "        'isDuplicate': np.dtype(int),\n",
    "        'generationMethod': np.dtype(int)\n",
    "    }\n",
    "    \n",
    "    types2 = {\n",
    "        'itemID': np.dtype(int),\n",
    "        'categoryID': np.dtype(int),\n",
    "        'title': np.dtype(str),\n",
    "        'description': np.dtype(str),\n",
    "        'images_array': np.dtype(str),\n",
    "        'attrsJSON': np.dtype(str),\n",
    "        'price': np.dtype(float),\n",
    "        'locationID': np.dtype(int),\n",
    "        'metroID': np.dtype(float),\n",
    "        'lat': np.dtype(float),\n",
    "        'lon': np.dtype(float),\n",
    "    }\n",
    "    \n",
    "    print('Load ItemPairs_train.csv')\n",
    "    pairs = pd.read_csv('~/Datasets/Avito/ItemPairs_train.csv', dtype = types1)\n",
    "    # Add 'id' column for easy merge\n",
    "    print('Load ItemInfo_train.csv')\n",
    "    items = pd.read_csv('~/Datasets/Avito/ItemInfo_train.csv', dtype = types2)\n",
    "    items.fillna(-1, inplace = True)\n",
    "    location = pd.read_csv('~/Datasets/Avito/Location.csv')\n",
    "    category = pd.read_csv('~/Datasets/Avito/Category.csv')\n",
    "    \n",
    "    train = pairs\n",
    "    train = train.drop(['generationMethod'], axis = 1)\n",
    "    \n",
    "    print('Add text features...')\n",
    "    train['len_title'] = items['title'].str.len()\n",
    "    train['len_description'] = items['description'].str.len()\n",
    "    train['len_attrsJSON'] = items['attrsJSON'].str.len()\n",
    "    \n",
    "    print('Merge items 1...')\n",
    "    item1 = items[['itemID', 'categoryID', 'price', 'locationID', 'metroID', 'lat', 'lon']]\n",
    "    item1 = pd.merge(item1, category, how = 'left', on = 'categoryID', left_index = True)\n",
    "    item1 = pd.merge(item1, location, how = 'left', on = 'locationID', left_index = True)\n",
    "    \n",
    "    item1 = item1.rename(\n",
    "        columns = {\n",
    "            'itemID': 'itemID_1',\n",
    "            'categoryID': 'categoryID_1',\n",
    "            'parentCategoryID': 'parentCategoryID_1',\n",
    "            'price': 'price_1',\n",
    "            'locationID': 'locationID_1',\n",
    "            'regionID': 'regionID_1',\n",
    "            'metroID': 'metroID_1',\n",
    "            'lat': 'lat_1',\n",
    "            'lon': 'lon_1'\n",
    "        })\n",
    "    \n",
    "     # Add item 1 data\n",
    "    train = pd.merge(train, item1, how='left', on='itemID_1', left_index=True)\n",
    "\n",
    "    print('Merge item 2...')\n",
    "    item2 = items[['itemID', 'categoryID', 'price', 'locationID', 'metroID', 'lat', 'lon']]\n",
    "    item2 = pd.merge(item2, category, how='left', on='categoryID', left_index=True)\n",
    "    item2 = pd.merge(item2, location, how='left', on='locationID', left_index=True)\n",
    "\n",
    "    item2 = item2.rename(\n",
    "        columns={\n",
    "            'itemID': 'itemID_2',\n",
    "            'categoryID': 'categoryID_2',\n",
    "            'parentCategoryID': 'parentCategoryID_2',\n",
    "            'price': 'price_2',\n",
    "            'locationID': 'locationID_2',\n",
    "            'regionID': 'regionID_2',\n",
    "            'metroID': 'metroID_2',\n",
    "            'lat': 'lat_2',\n",
    "            'lon': 'lon_2'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Add item 2 data\n",
    "    train = pd.merge(train, item2, how='left', on='itemID_2', left_index=True)\n",
    "    \n",
    "    # Create same arrays\n",
    "    print('Create same arrays')\n",
    "    train['price_same'] = np.equal(train['price_1'], train['price_2']).astype(np.int32)\n",
    "    train['locationID_same'] = np.equal(train['locationID_1'], train['locationID_2']).astype(np.int32)\n",
    "    train['categoryID_same'] = np.equal(train['categoryID_1'], train['categoryID_2']).astype(np.int32)\n",
    "    train['regionID_same'] = np.equal(train['regionID_1'], train['regionID_2']).astype(np.int32)\n",
    "    train['MetroID_same'] = np.equal(train['metroID_1'], train['metroID_2']).astype(np.int32)\n",
    "    train['lat_same'] = np.equal(train['lat_1'], train['lat_2']).astype(np.int32)\n",
    "    train['lon_same'] = np.equal(train['lon_1'], train['lon_2']).astype(np.int32)\n",
    "    \n",
    "    print('Create train data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prep_test():\n",
    "    start_time = time.time()\n",
    "\n",
    "    types1 = {\n",
    "        'itemID_1': np.dtype(int),\n",
    "        'itemID_2': np.dtype(int),\n",
    "        'id': np.dtype(int),\n",
    "    }\n",
    "\n",
    "    types2 = {\n",
    "        'itemID': np.dtype(int),\n",
    "        'categoryID': np.dtype(int),\n",
    "        'title': np.dtype(str),\n",
    "        'description': np.dtype(str),\n",
    "        'images_array': np.dtype(str),\n",
    "        'attrsJSON': np.dtype(str),\n",
    "        'price': np.dtype(float),\n",
    "        'locationID': np.dtype(int),\n",
    "        'metroID': np.dtype(float),\n",
    "        'lat': np.dtype(float),\n",
    "        'lon': np.dtype(float),\n",
    "    }\n",
    "    \n",
    "    print('Load ItemPairs_test.csv')\n",
    "    pairs = pd.read_csv('~/Datasets/Avito/ItemPairs_test.csv', dtype = types1)\n",
    "    print('Load ItemInfo_testcsv')\n",
    "    items = pd.read_csv('~/Datasets/Avito/ItemInfo_test.csv', dtype = types2)\n",
    "    items.fillna(-1, inplace = True)\n",
    "    location = pd.read_csv('~/Datasets/Avito/Location.csv')\n",
    "    category = pd.read_csv('~/Datasets/Avito/Category.csv')\n",
    "    \n",
    "    test = pairs\n",
    "    \n",
    "    print('Add text features...')\n",
    "    test['len_title'] = items['title'].str.len()\n",
    "    test['len_description'] = items['description'].str.len()\n",
    "    test['len_attrsJSON'] = items['attrsJSON'].str.len()\n",
    "    \n",
    "    print('Merge item 1...')\n",
    "    item1 = items[['itemID', 'categoryID', 'price', 'locationID', 'metroID', 'lat', 'lon']]\n",
    "    item1 = pd.merge(item1, category, how = 'left', on = 'categoryID', left_index = True)\n",
    "    item1 = pd.merge(item1, location, how = 'left', on = 'locationID', left_index = True)\n",
    "    \n",
    "    item1 = item1.rename(\n",
    "        columns = {\n",
    "            'itemID': 'itemID_1',\n",
    "            'categoryID': 'categoryID_1',\n",
    "            'parentCategoryID': 'parentCategoryID_1',\n",
    "            'price': 'price_1',\n",
    "            'locationID': 'locationID_1',\n",
    "            'regionID': 'regionID_1',\n",
    "            'metroID': 'metroID_1',\n",
    "            'lat': 'lat_1',\n",
    "            'lon': 'lon_1'\n",
    "        })\n",
    "    \n",
    "    # Add item 1 data\n",
    "    test = pd.merge(test, item1, how = 'left', on = 'itemID_1', left_index = True)\n",
    "    \n",
    "    print('Merge item 2...')\n",
    "    item2 = items[['itemID', 'categoryID', 'price', 'locationID', 'metroID', 'lat', 'lon']]\n",
    "    item2 = pd.merge(item2, category, how = 'left', on = 'categoryID', left_index = True)\n",
    "    item2 = pd.merge(item2, location, how = 'left', on = 'locationID', left_index = True)\n",
    "    \n",
    "    item2 = item2.rename(\n",
    "        columns = {\n",
    "            'itemID': 'itemID_2',\n",
    "            'categoryID': 'categoryID_2',\n",
    "            'parentCategoryID': 'parentCategoryID_2',\n",
    "            'price': 'price_2',\n",
    "            'locationID': 'locationID_2',\n",
    "            'regionID': 'regionID_2',\n",
    "            'metroID': 'metroID_2',\n",
    "            'lat': 'lat_2',\n",
    "            'lon': 'lon_2'\n",
    "        })\n",
    "    \n",
    "    # Add item 2 data\n",
    "    test = pd.merge(test, item2, how ='left', on = 'itemID_2', left_index = True)\n",
    "    \n",
    "    # Create same arrays\n",
    "    print('Create same arrays')\n",
    "    test['price_same'] = np.equal(test['price_1'], test['price_2']).astype(np.int32)\n",
    "    test['locationID_same'] = np.equal(test['locationID_1'], test['locationID_2']).astype(np.int32)\n",
    "    test['categoryID_same'] = np.equal(test['categoryID_1'], test['categoryID_2']).astype(np.int32)\n",
    "    test['regionID_same'] = np.equal(test['regionID_1'], test['regionID_2']).astype(np.int32)\n",
    "    test['metorID_same'] = np.equal(test['metroID_1'], test['metroID_2']).astype(np.int32)\n",
    "    test['lat_same'] = np.equal(test['lat_1'], test['lat_2']).astype(np.int32)\n",
    "    test['lon_same'] = np.equal(test['lat_1'], test['lat_2']).astype(np.int32)\n",
    "    \n",
    "    print(test.describe())\n",
    "    print('Create test data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_test_train():\n",
    "    train = prep_train()\n",
    "    test = prep_test()\n",
    "    train.fillna(-1, inplace = True)\n",
    "    test.fillna(-1, inplace = True)\n",
    "    \n",
    "    # Get only subset of data\n",
    "    if 1:\n",
    "        len_old = len(train.index)\n",
    "        train = train.sample(frac = 0.5)\n",
    "        len_new = len(train.index)\n",
    "        print('Reduce train from {} to {}'.format(len_old, len_new))\n",
    "    features = get_features(train, test)\n",
    "    return train, test, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load ItemPairs_train.csv\n",
      "Load ItemInfo_train.csv\n",
      "Add text features...\n",
      "Merge items 1...\n",
      "Merge item 2...\n",
      "Create same arrays\n",
      "Create train data time: 43.52 seconds\n",
      "Load ItemPairs_test.csv\n",
      "Load ItemInfo_testcsv\n",
      "Add text features...\n",
      "Merge item 1...\n",
      "Merge item 2...\n",
      "Create same arrays\n",
      "                 id      itemID_1      itemID_2     len_title  \\\n",
      "count  1.044196e+06  1.044196e+06  1.044196e+06  1.044195e+06   \n",
      "mean   5.220975e+05  2.027628e+06  4.065207e+06  2.278319e+01   \n",
      "std    3.014336e+05  1.438573e+06  1.443087e+06  1.187001e+01   \n",
      "min    0.000000e+00  5.000000e+00  5.847000e+03  1.000000e+00   \n",
      "25%    2.610488e+05  8.107795e+05  3.040622e+06  1.400000e+01   \n",
      "50%    5.220975e+05  1.780676e+06  4.309976e+06  2.100000e+01   \n",
      "75%    7.831462e+05  3.037476e+06  5.285806e+06  3.000000e+01   \n",
      "max    1.044195e+06  6.105866e+06  6.111991e+06  5.000000e+01   \n",
      "\n",
      "       len_description  len_attrsJSON  categoryID_1       price_1  \\\n",
      "count     1.044158e+06   1.010558e+06  1.044196e+06  1.044196e+06   \n",
      "mean      2.227149e+02   1.005616e+02  4.893708e+01  2.714250e+07   \n",
      "std       3.268549e+02   1.476119e+02  3.630723e+01  2.457133e+09   \n",
      "min       1.000000e+00   1.600000e+01  9.000000e+00 -1.000000e+00   \n",
      "25%       5.800000e+01   3.100000e+01  2.400000e+01  4.000000e+02   \n",
      "50%       1.160000e+02   6.600000e+01  2.900000e+01  1.950000e+03   \n",
      "75%       2.390000e+02   9.500000e+01  8.400000e+01  9.350000e+03   \n",
      "max       3.000000e+03   4.308000e+03  1.160000e+02  1.000000e+12   \n",
      "\n",
      "       locationID_1     metroID_1      ...              lon_2  \\\n",
      "count  1.044196e+06  1.044196e+06      ...       1.044196e+06   \n",
      "mean   6.434949e+05  8.171188e+04      ...       4.756286e+01   \n",
      "std    1.069202e+04  2.588881e+05      ...       1.851589e+01   \n",
      "min    6.215520e+05 -1.000000e+00      ...       1.461449e-01   \n",
      "25%    6.376400e+05 -1.000000e+00      ...       3.761903e+01   \n",
      "50%    6.417800e+05 -1.000000e+00      ...       3.987692e+01   \n",
      "75%    6.532400e+05  3.330000e+02      ...       5.242466e+01   \n",
      "max    6.628190e+05  5.000001e+06      ...       1.775210e+02   \n",
      "\n",
      "       parentCategoryID_2    regionID_2    price_same  locationID_same  \\\n",
      "count        1.044196e+06  1.044196e+06  1.044196e+06     1.044196e+06   \n",
      "mean         1.846619e+01  6.431735e+05  4.571307e-01     8.973871e-01   \n",
      "std          3.449717e+01  1.077373e+04  4.981591e-01     3.034528e-01   \n",
      "min          1.000000e+00  6.215500e+05  0.000000e+00     0.000000e+00   \n",
      "25%          2.000000e+00  6.376400e+05  0.000000e+00     1.000000e+00   \n",
      "50%          5.000000e+00  6.414700e+05  0.000000e+00     1.000000e+00   \n",
      "75%          7.000000e+00  6.532400e+05  1.000000e+00     1.000000e+00   \n",
      "max          1.130000e+02  6.628110e+05  1.000000e+00     1.000000e+00   \n",
      "\n",
      "       categoryID_same  regionID_same  metorID_same      lat_same  \\\n",
      "count        1044196.0   1.044196e+06  1.044196e+06  1.044196e+06   \n",
      "mean               1.0   9.614603e-01  9.407879e-01  8.316935e-01   \n",
      "std                0.0   1.924953e-01  2.360213e-01  3.741384e-01   \n",
      "min                1.0   0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%                1.0   1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "50%                1.0   1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "75%                1.0   1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "max                1.0   1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "\n",
      "           lon_same  \n",
      "count  1.044196e+06  \n",
      "mean   8.316935e-01  \n",
      "std    3.741384e-01  \n",
      "min    0.000000e+00  \n",
      "25%    1.000000e+00  \n",
      "50%    1.000000e+00  \n",
      "75%    1.000000e+00  \n",
      "max    1.000000e+00  \n",
      "\n",
      "[8 rows x 29 columns]\n",
      "Create test data time: 17.06 seconds\n",
      "Reduce train from 2991396 to 1495698\n",
      "Length of train:  1495698\n",
      "Length of test:  1044196\n",
      "Features [25]: ['categoryID_1', 'categoryID_2', 'categoryID_same', 'lat_1', 'lat_2', 'lat_same', 'len_attrsJSON', 'len_description', 'len_title', 'locationID_1', 'locationID_2', 'locationID_same', 'lon_1', 'lon_2', 'lon_same', 'metroID_1', 'metroID_2', 'parentCategoryID_1', 'parentCategoryID_2', 'price_1', 'price_2', 'price_same', 'regionID_1', 'regionID_2', 'regionID_same']\n",
      "XGBoost params. ETA: 0.1, MAX_DEPTH: 5, SUBSAMPLE: 0.8, COLSAMPLE_BY_TREE: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 20 rounds.\n",
      "[0]\ttrain-auc:0.727787\teval-auc:0.729461\n",
      "[1]\ttrain-auc:0.733909\teval-auc:0.735503\n",
      "[2]\ttrain-auc:0.748158\teval-auc:0.749490\n",
      "[3]\ttrain-auc:0.747957\teval-auc:0.749187\n",
      "[4]\ttrain-auc:0.750892\teval-auc:0.751945\n",
      "[5]\ttrain-auc:0.751542\teval-auc:0.752646\n",
      "[6]\ttrain-auc:0.757013\teval-auc:0.758105\n",
      "[7]\ttrain-auc:0.758712\teval-auc:0.759816\n",
      "[8]\ttrain-auc:0.759496\teval-auc:0.760618\n",
      "[9]\ttrain-auc:0.759137\teval-auc:0.760306\n",
      "[10]\ttrain-auc:0.762078\teval-auc:0.763310\n",
      "[11]\ttrain-auc:0.762726\teval-auc:0.763851\n",
      "[12]\ttrain-auc:0.765118\teval-auc:0.766132\n",
      "[13]\ttrain-auc:0.765850\teval-auc:0.766916\n",
      "[14]\ttrain-auc:0.766623\teval-auc:0.767655\n",
      "[15]\ttrain-auc:0.767495\teval-auc:0.768521\n",
      "[16]\ttrain-auc:0.768561\teval-auc:0.769497\n",
      "[17]\ttrain-auc:0.768841\teval-auc:0.769759\n",
      "[18]\ttrain-auc:0.769222\teval-auc:0.770082\n",
      "[19]\ttrain-auc:0.770019\teval-auc:0.770910\n",
      "[20]\ttrain-auc:0.770614\teval-auc:0.771515\n",
      "[21]\ttrain-auc:0.771027\teval-auc:0.771917\n",
      "[22]\ttrain-auc:0.772769\teval-auc:0.773726\n",
      "[23]\ttrain-auc:0.773345\teval-auc:0.774306\n",
      "[24]\ttrain-auc:0.774121\teval-auc:0.775003\n",
      "[25]\ttrain-auc:0.774654\teval-auc:0.775544\n",
      "[26]\ttrain-auc:0.775038\teval-auc:0.775927\n",
      "[27]\ttrain-auc:0.775387\teval-auc:0.776283\n",
      "[28]\ttrain-auc:0.776378\teval-auc:0.777266\n",
      "[29]\ttrain-auc:0.776707\teval-auc:0.777579\n",
      "[30]\ttrain-auc:0.777124\teval-auc:0.778023\n",
      "[31]\ttrain-auc:0.777573\teval-auc:0.778476\n",
      "[32]\ttrain-auc:0.778223\teval-auc:0.779120\n",
      "[33]\ttrain-auc:0.778627\teval-auc:0.779433\n",
      "[34]\ttrain-auc:0.780444\teval-auc:0.781271\n",
      "[35]\ttrain-auc:0.780753\teval-auc:0.781537\n",
      "[36]\ttrain-auc:0.781430\teval-auc:0.782194\n",
      "[37]\ttrain-auc:0.781993\teval-auc:0.782742\n",
      "[38]\ttrain-auc:0.782342\teval-auc:0.783098\n",
      "[39]\ttrain-auc:0.782763\teval-auc:0.783533\n",
      "[40]\ttrain-auc:0.783173\teval-auc:0.783936\n",
      "[41]\ttrain-auc:0.783481\teval-auc:0.784241\n",
      "[42]\ttrain-auc:0.783869\teval-auc:0.784646\n",
      "[43]\ttrain-auc:0.784128\teval-auc:0.784879\n",
      "[44]\ttrain-auc:0.784578\teval-auc:0.785326\n",
      "[45]\ttrain-auc:0.784990\teval-auc:0.785700\n",
      "[46]\ttrain-auc:0.785168\teval-auc:0.785869\n",
      "[47]\ttrain-auc:0.785581\teval-auc:0.786275\n",
      "[48]\ttrain-auc:0.785877\teval-auc:0.786570\n",
      "[49]\ttrain-auc:0.786477\teval-auc:0.787169\n",
      "[50]\ttrain-auc:0.787024\teval-auc:0.787713\n",
      "[51]\ttrain-auc:0.787705\teval-auc:0.788383\n",
      "[52]\ttrain-auc:0.787980\teval-auc:0.788643\n",
      "[53]\ttrain-auc:0.788156\teval-auc:0.788802\n",
      "[54]\ttrain-auc:0.788700\teval-auc:0.789318\n",
      "[55]\ttrain-auc:0.788840\teval-auc:0.789473\n",
      "[56]\ttrain-auc:0.789103\teval-auc:0.789725\n",
      "[57]\ttrain-auc:0.789699\teval-auc:0.790333\n",
      "[58]\ttrain-auc:0.789847\teval-auc:0.790464\n",
      "[59]\ttrain-auc:0.790162\teval-auc:0.790790\n",
      "[60]\ttrain-auc:0.790457\teval-auc:0.791060\n",
      "[61]\ttrain-auc:0.790753\teval-auc:0.791354\n",
      "[62]\ttrain-auc:0.791358\teval-auc:0.791938\n",
      "[63]\ttrain-auc:0.791569\teval-auc:0.792122\n",
      "[64]\ttrain-auc:0.791737\teval-auc:0.792299\n",
      "[65]\ttrain-auc:0.792031\teval-auc:0.792588\n",
      "[66]\ttrain-auc:0.792171\teval-auc:0.792734\n",
      "[67]\ttrain-auc:0.792436\teval-auc:0.792982\n",
      "[68]\ttrain-auc:0.792746\teval-auc:0.793274\n",
      "[69]\ttrain-auc:0.792940\teval-auc:0.793467\n",
      "[70]\ttrain-auc:0.793218\teval-auc:0.793735\n",
      "[71]\ttrain-auc:0.793302\teval-auc:0.793828\n",
      "[72]\ttrain-auc:0.793730\teval-auc:0.794251\n",
      "[73]\ttrain-auc:0.793769\teval-auc:0.794299\n",
      "[74]\ttrain-auc:0.794111\teval-auc:0.794618\n",
      "[75]\ttrain-auc:0.794426\teval-auc:0.794934\n",
      "[76]\ttrain-auc:0.794483\teval-auc:0.794999\n",
      "[77]\ttrain-auc:0.794729\teval-auc:0.795244\n",
      "[78]\ttrain-auc:0.794827\teval-auc:0.795325\n",
      "[79]\ttrain-auc:0.795028\teval-auc:0.795535\n",
      "[80]\ttrain-auc:0.795172\teval-auc:0.795687\n",
      "[81]\ttrain-auc:0.795769\teval-auc:0.796279\n",
      "[82]\ttrain-auc:0.796031\teval-auc:0.796538\n",
      "[83]\ttrain-auc:0.796255\teval-auc:0.796745\n",
      "[84]\ttrain-auc:0.796470\teval-auc:0.796946\n",
      "[85]\ttrain-auc:0.796528\teval-auc:0.797005\n",
      "[86]\ttrain-auc:0.796705\teval-auc:0.797178\n",
      "[87]\ttrain-auc:0.796899\teval-auc:0.797374\n",
      "[88]\ttrain-auc:0.797219\teval-auc:0.797696\n",
      "[89]\ttrain-auc:0.797807\teval-auc:0.798325\n",
      "[90]\ttrain-auc:0.797913\teval-auc:0.798446\n",
      "[91]\ttrain-auc:0.797990\teval-auc:0.798509\n",
      "[92]\ttrain-auc:0.798131\teval-auc:0.798655\n",
      "[93]\ttrain-auc:0.798309\teval-auc:0.798835\n",
      "[94]\ttrain-auc:0.798351\teval-auc:0.798872\n",
      "[95]\ttrain-auc:0.798748\teval-auc:0.799263\n",
      "[96]\ttrain-auc:0.798908\teval-auc:0.799398\n",
      "[97]\ttrain-auc:0.799013\teval-auc:0.799518\n",
      "[98]\ttrain-auc:0.799225\teval-auc:0.799746\n",
      "[99]\ttrain-auc:0.799377\teval-auc:0.799892\n",
      "[100]\ttrain-auc:0.799629\teval-auc:0.800107\n",
      "[101]\ttrain-auc:0.799810\teval-auc:0.800290\n",
      "[102]\ttrain-auc:0.800177\teval-auc:0.800678\n",
      "[103]\ttrain-auc:0.800217\teval-auc:0.800705\n",
      "[104]\ttrain-auc:0.800557\teval-auc:0.801077\n",
      "[105]\ttrain-auc:0.800661\teval-auc:0.801174\n",
      "[106]\ttrain-auc:0.800767\teval-auc:0.801265\n",
      "[107]\ttrain-auc:0.800826\teval-auc:0.801311\n",
      "[108]\ttrain-auc:0.801084\teval-auc:0.801554\n",
      "[109]\ttrain-auc:0.801376\teval-auc:0.801839\n",
      "[110]\ttrain-auc:0.801639\teval-auc:0.802085\n",
      "[111]\ttrain-auc:0.801823\teval-auc:0.802231\n",
      "[112]\ttrain-auc:0.801908\teval-auc:0.802312\n",
      "[113]\ttrain-auc:0.802218\teval-auc:0.802595\n",
      "[114]\ttrain-auc:0.802340\teval-auc:0.802692\n",
      "[115]\ttrain-auc:0.802384\teval-auc:0.802729\n",
      "[116]\ttrain-auc:0.802463\teval-auc:0.802776\n",
      "[117]\ttrain-auc:0.802565\teval-auc:0.802884\n",
      "[118]\ttrain-auc:0.802958\teval-auc:0.803257\n",
      "[119]\ttrain-auc:0.803082\teval-auc:0.803386\n",
      "[120]\ttrain-auc:0.803230\teval-auc:0.803505\n",
      "[121]\ttrain-auc:0.803354\teval-auc:0.803622\n",
      "[122]\ttrain-auc:0.803724\teval-auc:0.803972\n",
      "[123]\ttrain-auc:0.803835\teval-auc:0.804097\n",
      "[124]\ttrain-auc:0.803925\teval-auc:0.804195\n",
      "[125]\ttrain-auc:0.804083\teval-auc:0.804333\n",
      "[126]\ttrain-auc:0.804269\teval-auc:0.804525\n",
      "[127]\ttrain-auc:0.804390\teval-auc:0.804633\n",
      "[128]\ttrain-auc:0.804505\teval-auc:0.804755\n",
      "[129]\ttrain-auc:0.804652\teval-auc:0.804908\n",
      "[130]\ttrain-auc:0.804723\teval-auc:0.804976\n",
      "[131]\ttrain-auc:0.804855\teval-auc:0.805116\n",
      "[132]\ttrain-auc:0.804932\teval-auc:0.805192\n",
      "[133]\ttrain-auc:0.805107\teval-auc:0.805346\n",
      "[134]\ttrain-auc:0.805116\teval-auc:0.805357\n",
      "[135]\ttrain-auc:0.805240\teval-auc:0.805446\n",
      "[136]\ttrain-auc:0.805361\teval-auc:0.805555\n",
      "[137]\ttrain-auc:0.805458\teval-auc:0.805628\n",
      "[138]\ttrain-auc:0.806110\teval-auc:0.806248\n",
      "[139]\ttrain-auc:0.806270\teval-auc:0.806416\n",
      "[140]\ttrain-auc:0.806399\teval-auc:0.806566\n",
      "[141]\ttrain-auc:0.806453\teval-auc:0.806624\n",
      "[142]\ttrain-auc:0.806591\teval-auc:0.806753\n",
      "[143]\ttrain-auc:0.806773\teval-auc:0.806949\n",
      "[144]\ttrain-auc:0.806838\teval-auc:0.807004\n",
      "[145]\ttrain-auc:0.807047\teval-auc:0.807204\n",
      "[146]\ttrain-auc:0.807139\teval-auc:0.807291\n",
      "[147]\ttrain-auc:0.807305\teval-auc:0.807440\n",
      "[148]\ttrain-auc:0.807558\teval-auc:0.807696\n",
      "[149]\ttrain-auc:0.807712\teval-auc:0.807812\n",
      "[150]\ttrain-auc:0.807770\teval-auc:0.807875\n",
      "[151]\ttrain-auc:0.807780\teval-auc:0.807881\n",
      "[152]\ttrain-auc:0.808284\teval-auc:0.808386\n",
      "[153]\ttrain-auc:0.808433\teval-auc:0.808539\n",
      "[154]\ttrain-auc:0.808487\teval-auc:0.808592\n",
      "[155]\ttrain-auc:0.808556\teval-auc:0.808645\n",
      "[156]\ttrain-auc:0.808681\teval-auc:0.808771\n",
      "[157]\ttrain-auc:0.808773\teval-auc:0.808844\n",
      "[158]\ttrain-auc:0.808841\teval-auc:0.808912\n",
      "[159]\ttrain-auc:0.808985\teval-auc:0.809051\n",
      "[160]\ttrain-auc:0.809207\teval-auc:0.809279\n",
      "[161]\ttrain-auc:0.809232\teval-auc:0.809286\n",
      "[162]\ttrain-auc:0.809265\teval-auc:0.809312\n",
      "[163]\ttrain-auc:0.809473\teval-auc:0.809531\n",
      "[164]\ttrain-auc:0.809547\teval-auc:0.809601\n",
      "[165]\ttrain-auc:0.809606\teval-auc:0.809644\n",
      "[166]\ttrain-auc:0.809743\teval-auc:0.809767\n",
      "[167]\ttrain-auc:0.809869\teval-auc:0.809891\n",
      "[168]\ttrain-auc:0.810036\teval-auc:0.810070\n",
      "[169]\ttrain-auc:0.810150\teval-auc:0.810158\n",
      "[170]\ttrain-auc:0.810511\teval-auc:0.810549\n",
      "[171]\ttrain-auc:0.810613\teval-auc:0.810670\n",
      "[172]\ttrain-auc:0.810688\teval-auc:0.810727\n",
      "[173]\ttrain-auc:0.810749\teval-auc:0.810777\n",
      "[174]\ttrain-auc:0.810849\teval-auc:0.810880\n",
      "[175]\ttrain-auc:0.811011\teval-auc:0.811037\n",
      "[176]\ttrain-auc:0.811067\teval-auc:0.811078\n",
      "[177]\ttrain-auc:0.811309\teval-auc:0.811286\n",
      "[178]\ttrain-auc:0.811515\teval-auc:0.811465\n",
      "[179]\ttrain-auc:0.812079\teval-auc:0.812039\n",
      "[180]\ttrain-auc:0.812115\teval-auc:0.812070\n",
      "[181]\ttrain-auc:0.812187\teval-auc:0.812145\n",
      "[182]\ttrain-auc:0.812302\teval-auc:0.812265\n",
      "[183]\ttrain-auc:0.812389\teval-auc:0.812348\n",
      "[184]\ttrain-auc:0.812846\teval-auc:0.812813\n",
      "[185]\ttrain-auc:0.813385\teval-auc:0.813314\n",
      "[186]\ttrain-auc:0.813498\teval-auc:0.813423\n",
      "[187]\ttrain-auc:0.813635\teval-auc:0.813538\n",
      "[188]\ttrain-auc:0.813724\teval-auc:0.813613\n",
      "[189]\ttrain-auc:0.813883\teval-auc:0.813778\n",
      "[190]\ttrain-auc:0.814079\teval-auc:0.813966\n",
      "[191]\ttrain-auc:0.814207\teval-auc:0.814082\n",
      "[192]\ttrain-auc:0.814275\teval-auc:0.814145\n",
      "[193]\ttrain-auc:0.814411\teval-auc:0.814264\n",
      "[194]\ttrain-auc:0.814520\teval-auc:0.814363\n",
      "[195]\ttrain-auc:0.815163\teval-auc:0.814957\n",
      "[196]\ttrain-auc:0.815230\teval-auc:0.815021\n",
      "[197]\ttrain-auc:0.815356\teval-auc:0.815132\n",
      "[198]\ttrain-auc:0.815439\teval-auc:0.815207\n",
      "[199]\ttrain-auc:0.815577\teval-auc:0.815342\n",
      "[200]\ttrain-auc:0.815684\teval-auc:0.815446\n",
      "[201]\ttrain-auc:0.815738\teval-auc:0.815481\n",
      "[202]\ttrain-auc:0.815800\teval-auc:0.815552\n",
      "[203]\ttrain-auc:0.815850\teval-auc:0.815583\n",
      "[204]\ttrain-auc:0.815936\teval-auc:0.815666\n",
      "[205]\ttrain-auc:0.816055\teval-auc:0.815756\n",
      "[206]\ttrain-auc:0.816571\teval-auc:0.816310\n",
      "[207]\ttrain-auc:0.816676\teval-auc:0.816405\n",
      "[208]\ttrain-auc:0.816726\teval-auc:0.816450\n",
      "[209]\ttrain-auc:0.816790\teval-auc:0.816508\n",
      "[210]\ttrain-auc:0.816912\teval-auc:0.816641\n",
      "[211]\ttrain-auc:0.816996\teval-auc:0.816719\n",
      "[212]\ttrain-auc:0.817038\teval-auc:0.816753\n",
      "[213]\ttrain-auc:0.817236\teval-auc:0.816941\n",
      "[214]\ttrain-auc:0.817273\teval-auc:0.816968\n",
      "[215]\ttrain-auc:0.817321\teval-auc:0.817016\n",
      "[216]\ttrain-auc:0.817378\teval-auc:0.817071\n",
      "[217]\ttrain-auc:0.817461\teval-auc:0.817151\n",
      "[218]\ttrain-auc:0.817630\teval-auc:0.817316\n",
      "[219]\ttrain-auc:0.817733\teval-auc:0.817415\n",
      "[220]\ttrain-auc:0.817833\teval-auc:0.817506\n",
      "[221]\ttrain-auc:0.817916\teval-auc:0.817571\n",
      "[222]\ttrain-auc:0.818051\teval-auc:0.817718\n",
      "[223]\ttrain-auc:0.818088\teval-auc:0.817749\n",
      "[224]\ttrain-auc:0.818308\teval-auc:0.817974\n",
      "[225]\ttrain-auc:0.818381\teval-auc:0.818045\n",
      "[226]\ttrain-auc:0.818563\teval-auc:0.818219\n",
      "[227]\ttrain-auc:0.819001\teval-auc:0.818615\n",
      "[228]\ttrain-auc:0.819585\teval-auc:0.819176\n",
      "[229]\ttrain-auc:0.819684\teval-auc:0.819273\n",
      "[230]\ttrain-auc:0.819793\teval-auc:0.819387\n",
      "[231]\ttrain-auc:0.819807\teval-auc:0.819394\n",
      "[232]\ttrain-auc:0.819903\teval-auc:0.819494\n",
      "[233]\ttrain-auc:0.819993\teval-auc:0.819566\n",
      "[234]\ttrain-auc:0.820056\teval-auc:0.819619\n",
      "[235]\ttrain-auc:0.820184\teval-auc:0.819733\n",
      "[236]\ttrain-auc:0.820396\teval-auc:0.819946\n",
      "[237]\ttrain-auc:0.820474\teval-auc:0.820009\n",
      "[238]\ttrain-auc:0.820491\teval-auc:0.820023\n",
      "[239]\ttrain-auc:0.820562\teval-auc:0.820072\n",
      "[240]\ttrain-auc:0.820645\teval-auc:0.820156\n",
      "[241]\ttrain-auc:0.820891\teval-auc:0.820384\n",
      "[242]\ttrain-auc:0.821010\teval-auc:0.820496\n",
      "[243]\ttrain-auc:0.821107\teval-auc:0.820592\n",
      "[244]\ttrain-auc:0.821216\teval-auc:0.820689\n",
      "[245]\ttrain-auc:0.821262\teval-auc:0.820742\n",
      "[246]\ttrain-auc:0.821530\teval-auc:0.820967\n",
      "[247]\ttrain-auc:0.821697\teval-auc:0.821132\n",
      "[248]\ttrain-auc:0.821733\teval-auc:0.821167\n",
      "[249]\ttrain-auc:0.821805\teval-auc:0.821232\n",
      "[250]\ttrain-auc:0.821890\teval-auc:0.821308\n",
      "[251]\ttrain-auc:0.821976\teval-auc:0.821395\n",
      "[252]\ttrain-auc:0.822012\teval-auc:0.821431\n",
      "[253]\ttrain-auc:0.822159\teval-auc:0.821581\n",
      "[254]\ttrain-auc:0.822416\teval-auc:0.821825\n",
      "[255]\ttrain-auc:0.822531\teval-auc:0.821945\n",
      "[256]\ttrain-auc:0.822603\teval-auc:0.822017\n",
      "[257]\ttrain-auc:0.822699\teval-auc:0.822097\n",
      "[258]\ttrain-auc:0.822788\teval-auc:0.822186\n",
      "[259]\ttrain-auc:0.822903\teval-auc:0.822297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n",
      "Check error value: 0.822297\n",
      "Importances array:  [('categoryID_2', 1115), ('price_2', 1085), ('price_1', 948), ('parentCategoryID_2', 677), ('lat_1', 479), ('lon_2', 461), ('lon_1', 461), ('lat_2', 459), ('locationID_1', 310), ('locationID_2', 297), ('categoryID_1', 248), ('price_same', 183), ('metroID_2', 168), ('metroID_1', 164), ('locationID_same', 111), ('regionID_same', 90), ('regionID_1', 85), ('len_attrsJSON', 71), ('regionID_2', 68), ('parentCategoryID_1', 68), ('len_title', 67), ('lon_same', 61), ('lat_same', 57), ('len_description', 47)]\n",
      "Predict test set...\n",
      "Training time: 4.44 minutes\n",
      "Real score = 0.8222974730546484\n",
      "Writing submission:  submission_0.822297473055_2016-05-15-00-25.csv\n"
     ]
    }
   ],
   "source": [
    "train, test, features = read_test_train()\n",
    "print('Length of train: ', len(train))\n",
    "print('Length of test: ', len(test))\n",
    "print('Features [{}]: {}'.format(len(features), sorted(features)))\n",
    "test_prediction, score = run_default_test(train, test, features, 'isDuplicate')\n",
    "print('Real score = {}'.format(score))\n",
    "create_submission(score, test, test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import io\n",
    "from PIL import Image\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/valesco/Datasets/Avito/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing zip file 0\n",
      "Total elements 10\n",
      "Doing zip file 1\n",
      "Total elements 10\n",
      "Doing zip file 2\n",
      "Total elements 10\n",
      "Doing zip file 3\n",
      "Total elements 10\n",
      "Doing zip file 4\n",
      "Total elements 10\n",
      "Doing zip file 5\n",
      "Total elements 10\n",
      "Doing zip file 6\n",
      "Total elements 10\n",
      "Doing zip file 7\n",
      "Total elements 10\n",
      "Doing zip file 8\n",
      "Total elements 10\n",
      "Doing zip file 9\n",
      "Total elements 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def dhash(image,hash_size = 16):\n",
    "    image = image.convert('LA').resize((hash_size+1, hash_size), Image.ANTIALIAS)\n",
    "    mat = np.array(\n",
    "        list(map(lambda x: x[0], image.getdata()))\n",
    "    ).reshape(hash_size, hash_size+1)\n",
    "    \n",
    "    return ''.join(\n",
    "        map(\n",
    "            lambda x: hex(x)[2:].rjust(2,'0'),\n",
    "            np.packbits(np.fliplr(np.diff(mat) < 0))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "\n",
    "for zip_counter in [0,1,2,3,4,5,6,7,8,9]:\n",
    "    imgzipfile = zipfile.ZipFile('Images_'+str(zip_counter)+'.zip')\n",
    "    print ('Doing zip file ' + str(zip_counter))\n",
    "    #namelist = imgzipfile.namelist()\n",
    "    # Comment this line below and uncomment the above line when you do for the whole set\n",
    "    namelist = imgzipfile.namelist()[:10]\n",
    "    print ('Total elements ' + str(len(namelist)))\n",
    "\n",
    "    img_id_hash = []\n",
    "    counter = 1\n",
    "    for name in namelist:\n",
    "        #print name\n",
    "        #try:\n",
    "        imgdata = imgzipfile.read(name)\n",
    "        if len(imgdata) >0:\n",
    "            img_id = name[:-4]\n",
    "            stream = io.BytesIO(imgdata)\n",
    "            img = Image.open(stream)\n",
    "            img_hash = dhash(img)\n",
    "            img_id_hash.append([img_id,img_hash])\n",
    "            counter+=1\n",
    "        # Uncomment the lines below to get an idea of progress when you do for the whole set\n",
    "        if counter%10000==0:\n",
    "            print('Done ' + str(counter) , datetime.datetime.now())\n",
    "        #except:\n",
    "            #print ('Could not read ' + str(name) + ' in zip file ' + str(zip_counter))\n",
    "    df = pd.DataFrame(img_id_hash,columns=['image_id','image_hash'])\n",
    "    df.to_csv('image_hash_' + str(zip_counter) + '.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hash_file = pickle.load(open('dhash.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10824317"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hash_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
