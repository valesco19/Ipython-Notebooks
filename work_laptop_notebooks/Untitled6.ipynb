{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import theano\n",
    "from keras.models import Sequential  \n",
    "from keras.layers.core import Dense, Activation, Dense, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_id</th>\n",
       "      <th>player_name</th>\n",
       "      <th>date_started</th>\n",
       "      <th>tournament_id</th>\n",
       "      <th>course_num</th>\n",
       "      <th>year</th>\n",
       "      <th>round_num</th>\n",
       "      <th>hole_num</th>\n",
       "      <th>hole_id</th>\n",
       "      <th>par</th>\n",
       "      <th>stroke_num</th>\n",
       "      <th>stroke_gained</th>\n",
       "      <th>shot_loc</th>\n",
       "      <th>shot_dist</th>\n",
       "      <th>hole_dist_yds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25632.0</td>\n",
       "      <td>Jimmy Walker</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.160</td>\n",
       "      <td>Tee</td>\n",
       "      <td>13680.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25632.0</td>\n",
       "      <td>Jimmy Walker</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.063</td>\n",
       "      <td>Fairway</td>\n",
       "      <td>3810.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25632.0</td>\n",
       "      <td>Jimmy Walker</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>Green</td>\n",
       "      <td>156.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25632.0</td>\n",
       "      <td>Jimmy Walker</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>Green</td>\n",
       "      <td>25.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29476.0</td>\n",
       "      <td>Kyle Reifers</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.160</td>\n",
       "      <td>Tee</td>\n",
       "      <td>13680.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>29476.0</td>\n",
       "      <td>Kyle Reifers</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>Fairway</td>\n",
       "      <td>3317.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29476.0</td>\n",
       "      <td>Kyle Reifers</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>Green</td>\n",
       "      <td>227.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29476.0</td>\n",
       "      <td>Kyle Reifers</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.053</td>\n",
       "      <td>Green</td>\n",
       "      <td>39.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30110.0</td>\n",
       "      <td>Kyle Stanley</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.160</td>\n",
       "      <td>Tee</td>\n",
       "      <td>13680.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30110.0</td>\n",
       "      <td>Kyle Stanley</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>Fairway</td>\n",
       "      <td>3509.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30110.0</td>\n",
       "      <td>Kyle Stanley</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>Green</td>\n",
       "      <td>353.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>30110.0</td>\n",
       "      <td>Kyle Stanley</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>Green</td>\n",
       "      <td>24.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>37189.0</td>\n",
       "      <td>Harold Varner III</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.160</td>\n",
       "      <td>Tee</td>\n",
       "      <td>13680.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>37189.0</td>\n",
       "      <td>Harold Varner III</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>Fairway</td>\n",
       "      <td>3428.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>37189.0</td>\n",
       "      <td>Harold Varner III</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.391</td>\n",
       "      <td>Fairway</td>\n",
       "      <td>652.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>37189.0</td>\n",
       "      <td>Harold Varner III</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>Green</td>\n",
       "      <td>27.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>27466.0</td>\n",
       "      <td>Spencer Levin</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.160</td>\n",
       "      <td>Tee</td>\n",
       "      <td>13680.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>27466.0</td>\n",
       "      <td>Spencer Levin</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>Fairway</td>\n",
       "      <td>3956.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>27466.0</td>\n",
       "      <td>Spencer Levin</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.399</td>\n",
       "      <td>Fairway</td>\n",
       "      <td>386.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>27466.0</td>\n",
       "      <td>Spencer Levin</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Green</td>\n",
       "      <td>2.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>29485.0</td>\n",
       "      <td>Brett Stegmaier</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.160</td>\n",
       "      <td>Tee</td>\n",
       "      <td>13680.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>29485.0</td>\n",
       "      <td>Brett Stegmaier</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>Fairway</td>\n",
       "      <td>3486.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>29485.0</td>\n",
       "      <td>Brett Stegmaier</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>Fairway</td>\n",
       "      <td>488.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>29485.0</td>\n",
       "      <td>Brett Stegmaier</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>Green</td>\n",
       "      <td>119.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>29485.0</td>\n",
       "      <td>Brett Stegmaier</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Green</td>\n",
       "      <td>2.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25240.0</td>\n",
       "      <td>D.A. Points</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>Tee</td>\n",
       "      <td>13680.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25240.0</td>\n",
       "      <td>D.A. Points</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.351</td>\n",
       "      <td>Rough</td>\n",
       "      <td>3368.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>25240.0</td>\n",
       "      <td>D.A. Points</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>Green</td>\n",
       "      <td>131.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>25240.0</td>\n",
       "      <td>D.A. Points</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>Green</td>\n",
       "      <td>19.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>24846.0</td>\n",
       "      <td>Ricky Barnes</td>\n",
       "      <td>03_31_2016</td>\n",
       "      <td>20.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016_020_729_1_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.120</td>\n",
       "      <td>Tee</td>\n",
       "      <td>13680.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502337</th>\n",
       "      <td>34213.0</td>\n",
       "      <td>Grayson Murray</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>Tee</td>\n",
       "      <td>15984.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502338</th>\n",
       "      <td>34213.0</td>\n",
       "      <td>Grayson Murray</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>Rough</td>\n",
       "      <td>4968.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502339</th>\n",
       "      <td>34213.0</td>\n",
       "      <td>Grayson Murray</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>Fairway</td>\n",
       "      <td>598.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502340</th>\n",
       "      <td>34213.0</td>\n",
       "      <td>Grayson Murray</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>Green</td>\n",
       "      <td>129.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502341</th>\n",
       "      <td>34213.0</td>\n",
       "      <td>Grayson Murray</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>Green</td>\n",
       "      <td>25.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502342</th>\n",
       "      <td>28679.0</td>\n",
       "      <td>Fabian Gomez</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>Tee</td>\n",
       "      <td>15984.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502343</th>\n",
       "      <td>28679.0</td>\n",
       "      <td>Fabian Gomez</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.317</td>\n",
       "      <td>Rough</td>\n",
       "      <td>6245.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502344</th>\n",
       "      <td>28679.0</td>\n",
       "      <td>Fabian Gomez</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>Green</td>\n",
       "      <td>387.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502345</th>\n",
       "      <td>28679.0</td>\n",
       "      <td>Fabian Gomez</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.147</td>\n",
       "      <td>Green</td>\n",
       "      <td>52.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502346</th>\n",
       "      <td>27436.0</td>\n",
       "      <td>Graham DeLaet</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230</td>\n",
       "      <td>Tee</td>\n",
       "      <td>15984.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502347</th>\n",
       "      <td>27436.0</td>\n",
       "      <td>Graham DeLaet</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>Fairway</td>\n",
       "      <td>4569.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502348</th>\n",
       "      <td>27436.0</td>\n",
       "      <td>Graham DeLaet</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>Green</td>\n",
       "      <td>224.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502349</th>\n",
       "      <td>27436.0</td>\n",
       "      <td>Graham DeLaet</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Green</td>\n",
       "      <td>7.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502350</th>\n",
       "      <td>27214.0</td>\n",
       "      <td>Kevin Streelman</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>Tee</td>\n",
       "      <td>15984.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502351</th>\n",
       "      <td>27214.0</td>\n",
       "      <td>Kevin Streelman</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.216</td>\n",
       "      <td>Rough</td>\n",
       "      <td>4789.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502352</th>\n",
       "      <td>27214.0</td>\n",
       "      <td>Kevin Streelman</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.934</td>\n",
       "      <td>Green</td>\n",
       "      <td>300.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502353</th>\n",
       "      <td>30110.0</td>\n",
       "      <td>Kyle Stanley</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.170</td>\n",
       "      <td>Tee</td>\n",
       "      <td>15984.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502354</th>\n",
       "      <td>30110.0</td>\n",
       "      <td>Kyle Stanley</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>Fairway</td>\n",
       "      <td>5342.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502355</th>\n",
       "      <td>30110.0</td>\n",
       "      <td>Kyle Stanley</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.521</td>\n",
       "      <td>Sand</td>\n",
       "      <td>262.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502356</th>\n",
       "      <td>30110.0</td>\n",
       "      <td>Kyle Stanley</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>Green</td>\n",
       "      <td>23.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502357</th>\n",
       "      <td>25396.0</td>\n",
       "      <td>Kevin Na</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>Tee</td>\n",
       "      <td>15984.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502358</th>\n",
       "      <td>25396.0</td>\n",
       "      <td>Kevin Na</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>Sand</td>\n",
       "      <td>5190.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502359</th>\n",
       "      <td>25396.0</td>\n",
       "      <td>Kevin Na</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.043</td>\n",
       "      <td>Fairway</td>\n",
       "      <td>887.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502360</th>\n",
       "      <td>25396.0</td>\n",
       "      <td>Kevin Na</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.652</td>\n",
       "      <td>Green</td>\n",
       "      <td>78.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502361</th>\n",
       "      <td>25396.0</td>\n",
       "      <td>Kevin Na</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>Green</td>\n",
       "      <td>22.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502362</th>\n",
       "      <td>33141.0</td>\n",
       "      <td>Keegan Bradley</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>Tee</td>\n",
       "      <td>15984.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502363</th>\n",
       "      <td>33141.0</td>\n",
       "      <td>Keegan Bradley</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>Sand</td>\n",
       "      <td>4861.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502364</th>\n",
       "      <td>33141.0</td>\n",
       "      <td>Keegan Bradley</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>Rough</td>\n",
       "      <td>709.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502365</th>\n",
       "      <td>33141.0</td>\n",
       "      <td>Keegan Bradley</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>Green</td>\n",
       "      <td>120.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502366</th>\n",
       "      <td>33141.0</td>\n",
       "      <td>Keegan Bradley</td>\n",
       "      <td>06_25_2017</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2017_034_503_4_18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Green</td>\n",
       "      <td>14.0</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1502367 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         player_id        player_name date_started  tournament_id  course_num  \\\n",
       "0          25632.0       Jimmy Walker   03_31_2016           20.0       729.0   \n",
       "1          25632.0       Jimmy Walker   03_31_2016           20.0       729.0   \n",
       "2          25632.0       Jimmy Walker   03_31_2016           20.0       729.0   \n",
       "3          25632.0       Jimmy Walker   03_31_2016           20.0       729.0   \n",
       "4          29476.0       Kyle Reifers   03_31_2016           20.0       729.0   \n",
       "5          29476.0       Kyle Reifers   03_31_2016           20.0       729.0   \n",
       "6          29476.0       Kyle Reifers   03_31_2016           20.0       729.0   \n",
       "7          29476.0       Kyle Reifers   03_31_2016           20.0       729.0   \n",
       "8          30110.0       Kyle Stanley   03_31_2016           20.0       729.0   \n",
       "9          30110.0       Kyle Stanley   03_31_2016           20.0       729.0   \n",
       "10         30110.0       Kyle Stanley   03_31_2016           20.0       729.0   \n",
       "11         30110.0       Kyle Stanley   03_31_2016           20.0       729.0   \n",
       "12         37189.0  Harold Varner III   03_31_2016           20.0       729.0   \n",
       "13         37189.0  Harold Varner III   03_31_2016           20.0       729.0   \n",
       "14         37189.0  Harold Varner III   03_31_2016           20.0       729.0   \n",
       "15         37189.0  Harold Varner III   03_31_2016           20.0       729.0   \n",
       "16         27466.0      Spencer Levin   03_31_2016           20.0       729.0   \n",
       "17         27466.0      Spencer Levin   03_31_2016           20.0       729.0   \n",
       "18         27466.0      Spencer Levin   03_31_2016           20.0       729.0   \n",
       "19         27466.0      Spencer Levin   03_31_2016           20.0       729.0   \n",
       "20         29485.0    Brett Stegmaier   03_31_2016           20.0       729.0   \n",
       "21         29485.0    Brett Stegmaier   03_31_2016           20.0       729.0   \n",
       "22         29485.0    Brett Stegmaier   03_31_2016           20.0       729.0   \n",
       "23         29485.0    Brett Stegmaier   03_31_2016           20.0       729.0   \n",
       "24         29485.0    Brett Stegmaier   03_31_2016           20.0       729.0   \n",
       "25         25240.0        D.A. Points   03_31_2016           20.0       729.0   \n",
       "26         25240.0        D.A. Points   03_31_2016           20.0       729.0   \n",
       "27         25240.0        D.A. Points   03_31_2016           20.0       729.0   \n",
       "28         25240.0        D.A. Points   03_31_2016           20.0       729.0   \n",
       "29         24846.0       Ricky Barnes   03_31_2016           20.0       729.0   \n",
       "...            ...                ...          ...            ...         ...   \n",
       "1502337    34213.0     Grayson Murray   06_25_2017           34.0       503.0   \n",
       "1502338    34213.0     Grayson Murray   06_25_2017           34.0       503.0   \n",
       "1502339    34213.0     Grayson Murray   06_25_2017           34.0       503.0   \n",
       "1502340    34213.0     Grayson Murray   06_25_2017           34.0       503.0   \n",
       "1502341    34213.0     Grayson Murray   06_25_2017           34.0       503.0   \n",
       "1502342    28679.0       Fabian Gomez   06_25_2017           34.0       503.0   \n",
       "1502343    28679.0       Fabian Gomez   06_25_2017           34.0       503.0   \n",
       "1502344    28679.0       Fabian Gomez   06_25_2017           34.0       503.0   \n",
       "1502345    28679.0       Fabian Gomez   06_25_2017           34.0       503.0   \n",
       "1502346    27436.0      Graham DeLaet   06_25_2017           34.0       503.0   \n",
       "1502347    27436.0      Graham DeLaet   06_25_2017           34.0       503.0   \n",
       "1502348    27436.0      Graham DeLaet   06_25_2017           34.0       503.0   \n",
       "1502349    27436.0      Graham DeLaet   06_25_2017           34.0       503.0   \n",
       "1502350    27214.0    Kevin Streelman   06_25_2017           34.0       503.0   \n",
       "1502351    27214.0    Kevin Streelman   06_25_2017           34.0       503.0   \n",
       "1502352    27214.0    Kevin Streelman   06_25_2017           34.0       503.0   \n",
       "1502353    30110.0       Kyle Stanley   06_25_2017           34.0       503.0   \n",
       "1502354    30110.0       Kyle Stanley   06_25_2017           34.0       503.0   \n",
       "1502355    30110.0       Kyle Stanley   06_25_2017           34.0       503.0   \n",
       "1502356    30110.0       Kyle Stanley   06_25_2017           34.0       503.0   \n",
       "1502357    25396.0           Kevin Na   06_25_2017           34.0       503.0   \n",
       "1502358    25396.0           Kevin Na   06_25_2017           34.0       503.0   \n",
       "1502359    25396.0           Kevin Na   06_25_2017           34.0       503.0   \n",
       "1502360    25396.0           Kevin Na   06_25_2017           34.0       503.0   \n",
       "1502361    25396.0           Kevin Na   06_25_2017           34.0       503.0   \n",
       "1502362    33141.0     Keegan Bradley   06_25_2017           34.0       503.0   \n",
       "1502363    33141.0     Keegan Bradley   06_25_2017           34.0       503.0   \n",
       "1502364    33141.0     Keegan Bradley   06_25_2017           34.0       503.0   \n",
       "1502365    33141.0     Keegan Bradley   06_25_2017           34.0       503.0   \n",
       "1502366    33141.0     Keegan Bradley   06_25_2017           34.0       503.0   \n",
       "\n",
       "           year  round_num  hole_num            hole_id  par  stroke_num  \\\n",
       "0        2016.0        1.0      10.0  2016_020_729_1_10  4.0         1.0   \n",
       "1        2016.0        1.0      10.0  2016_020_729_1_10  4.0         2.0   \n",
       "2        2016.0        1.0      10.0  2016_020_729_1_10  4.0         3.0   \n",
       "3        2016.0        1.0      10.0  2016_020_729_1_10  4.0         4.0   \n",
       "4        2016.0        1.0      10.0  2016_020_729_1_10  4.0         1.0   \n",
       "5        2016.0        1.0      10.0  2016_020_729_1_10  4.0         2.0   \n",
       "6        2016.0        1.0      10.0  2016_020_729_1_10  4.0         3.0   \n",
       "7        2016.0        1.0      10.0  2016_020_729_1_10  4.0         4.0   \n",
       "8        2016.0        1.0      10.0  2016_020_729_1_10  4.0         1.0   \n",
       "9        2016.0        1.0      10.0  2016_020_729_1_10  4.0         2.0   \n",
       "10       2016.0        1.0      10.0  2016_020_729_1_10  4.0         3.0   \n",
       "11       2016.0        1.0      10.0  2016_020_729_1_10  4.0         4.0   \n",
       "12       2016.0        1.0      10.0  2016_020_729_1_10  4.0         1.0   \n",
       "13       2016.0        1.0      10.0  2016_020_729_1_10  4.0         2.0   \n",
       "14       2016.0        1.0      10.0  2016_020_729_1_10  4.0         3.0   \n",
       "15       2016.0        1.0      10.0  2016_020_729_1_10  4.0         4.0   \n",
       "16       2016.0        1.0      10.0  2016_020_729_1_10  4.0         1.0   \n",
       "17       2016.0        1.0      10.0  2016_020_729_1_10  4.0         2.0   \n",
       "18       2016.0        1.0      10.0  2016_020_729_1_10  4.0         3.0   \n",
       "19       2016.0        1.0      10.0  2016_020_729_1_10  4.0         4.0   \n",
       "20       2016.0        1.0      10.0  2016_020_729_1_10  4.0         1.0   \n",
       "21       2016.0        1.0      10.0  2016_020_729_1_10  4.0         2.0   \n",
       "22       2016.0        1.0      10.0  2016_020_729_1_10  4.0         3.0   \n",
       "23       2016.0        1.0      10.0  2016_020_729_1_10  4.0         4.0   \n",
       "24       2016.0        1.0      10.0  2016_020_729_1_10  4.0         5.0   \n",
       "25       2016.0        1.0      10.0  2016_020_729_1_10  4.0         1.0   \n",
       "26       2016.0        1.0      10.0  2016_020_729_1_10  4.0         2.0   \n",
       "27       2016.0        1.0      10.0  2016_020_729_1_10  4.0         3.0   \n",
       "28       2016.0        1.0      10.0  2016_020_729_1_10  4.0         4.0   \n",
       "29       2016.0        1.0      10.0  2016_020_729_1_10  4.0         1.0   \n",
       "...         ...        ...       ...                ...  ...         ...   \n",
       "1502337  2017.0        4.0      18.0  2017_034_503_4_18  4.0         1.0   \n",
       "1502338  2017.0        4.0      18.0  2017_034_503_4_18  4.0         2.0   \n",
       "1502339  2017.0        4.0      18.0  2017_034_503_4_18  4.0         3.0   \n",
       "1502340  2017.0        4.0      18.0  2017_034_503_4_18  4.0         4.0   \n",
       "1502341  2017.0        4.0      18.0  2017_034_503_4_18  4.0         5.0   \n",
       "1502342  2017.0        4.0      18.0  2017_034_503_4_18  4.0         1.0   \n",
       "1502343  2017.0        4.0      18.0  2017_034_503_4_18  4.0         2.0   \n",
       "1502344  2017.0        4.0      18.0  2017_034_503_4_18  4.0         3.0   \n",
       "1502345  2017.0        4.0      18.0  2017_034_503_4_18  4.0         4.0   \n",
       "1502346  2017.0        4.0      18.0  2017_034_503_4_18  4.0         1.0   \n",
       "1502347  2017.0        4.0      18.0  2017_034_503_4_18  4.0         2.0   \n",
       "1502348  2017.0        4.0      18.0  2017_034_503_4_18  4.0         3.0   \n",
       "1502349  2017.0        4.0      18.0  2017_034_503_4_18  4.0         4.0   \n",
       "1502350  2017.0        4.0      18.0  2017_034_503_4_18  4.0         1.0   \n",
       "1502351  2017.0        4.0      18.0  2017_034_503_4_18  4.0         2.0   \n",
       "1502352  2017.0        4.0      18.0  2017_034_503_4_18  4.0         3.0   \n",
       "1502353  2017.0        4.0      18.0  2017_034_503_4_18  4.0         1.0   \n",
       "1502354  2017.0        4.0      18.0  2017_034_503_4_18  4.0         2.0   \n",
       "1502355  2017.0        4.0      18.0  2017_034_503_4_18  4.0         3.0   \n",
       "1502356  2017.0        4.0      18.0  2017_034_503_4_18  4.0         4.0   \n",
       "1502357  2017.0        4.0      18.0  2017_034_503_4_18  4.0         1.0   \n",
       "1502358  2017.0        4.0      18.0  2017_034_503_4_18  4.0         2.0   \n",
       "1502359  2017.0        4.0      18.0  2017_034_503_4_18  4.0         3.0   \n",
       "1502360  2017.0        4.0      18.0  2017_034_503_4_18  4.0         4.0   \n",
       "1502361  2017.0        4.0      18.0  2017_034_503_4_18  4.0         5.0   \n",
       "1502362  2017.0        4.0      18.0  2017_034_503_4_18  4.0         1.0   \n",
       "1502363  2017.0        4.0      18.0  2017_034_503_4_18  4.0         2.0   \n",
       "1502364  2017.0        4.0      18.0  2017_034_503_4_18  4.0         3.0   \n",
       "1502365  2017.0        4.0      18.0  2017_034_503_4_18  4.0         4.0   \n",
       "1502366  2017.0        4.0      18.0  2017_034_503_4_18  4.0         5.0   \n",
       "\n",
       "         stroke_gained shot_loc  shot_dist  hole_dist_yds  \n",
       "0                0.160      Tee    13680.0          380.0  \n",
       "1                0.063  Fairway     3810.0          380.0  \n",
       "2               -0.272    Green      156.0          380.0  \n",
       "3                0.009    Green       25.0          380.0  \n",
       "4                0.160      Tee    13680.0          380.0  \n",
       "5               -0.063  Fairway     3317.0          380.0  \n",
       "6               -0.190    Green      227.0          380.0  \n",
       "7                0.053    Green       39.0          380.0  \n",
       "8                0.160      Tee    13680.0          380.0  \n",
       "9               -0.170  Fairway     3509.0          380.0  \n",
       "10              -0.039    Green      353.0          380.0  \n",
       "11               0.009    Green       24.0          380.0  \n",
       "12               0.160      Tee    13680.0          380.0  \n",
       "13              -0.600  Fairway     3428.0          380.0  \n",
       "14               0.391  Fairway      652.0          380.0  \n",
       "15               0.009    Green       27.0          380.0  \n",
       "16               0.160      Tee    13680.0          380.0  \n",
       "17              -0.600  Fairway     3956.0          380.0  \n",
       "18               0.399  Fairway      386.0          380.0  \n",
       "19               0.001    Green        2.0          380.0  \n",
       "20               0.160      Tee    13680.0          380.0  \n",
       "21              -0.600  Fairway     3486.0          380.0  \n",
       "22              -0.226  Fairway      488.0          380.0  \n",
       "23              -0.375    Green      119.0          380.0  \n",
       "24               0.001    Green        2.0          380.0  \n",
       "25              -0.060      Tee    13680.0          380.0  \n",
       "26               0.351    Rough     3368.0          380.0  \n",
       "27              -0.340    Green      131.0          380.0  \n",
       "28               0.009    Green       19.0          380.0  \n",
       "29              -1.120      Tee    13680.0          380.0  \n",
       "...                ...      ...        ...            ...  \n",
       "1502337         -0.070      Tee    15984.0          444.0  \n",
       "1502338         -0.250    Rough     4968.0          444.0  \n",
       "1502339         -0.269  Fairway      598.0          444.0  \n",
       "1502340         -0.340    Green      129.0          444.0  \n",
       "1502341          0.009    Green       25.0          444.0  \n",
       "1502342         -0.230      Tee    15984.0          444.0  \n",
       "1502343          0.317    Rough     6245.0          444.0  \n",
       "1502344         -0.154    Green      387.0          444.0  \n",
       "1502345          0.147    Green       52.0          444.0  \n",
       "1502346          0.230      Tee    15984.0          444.0  \n",
       "1502347         -0.013  Fairway     4569.0          444.0  \n",
       "1502348         -0.138    Green      224.0          444.0  \n",
       "1502349          0.001    Green        7.0          444.0  \n",
       "1502350         -0.070      Tee    15984.0          444.0  \n",
       "1502351          0.216    Rough     4789.0          444.0  \n",
       "1502352          0.934    Green      300.0          444.0  \n",
       "1502353          0.170      Tee    15984.0          444.0  \n",
       "1502354         -0.620  Fairway     5342.0          444.0  \n",
       "1502355          0.521     Sand      262.0          444.0  \n",
       "1502356          0.009    Green       23.0          444.0  \n",
       "1502357         -0.140      Tee    15984.0          444.0  \n",
       "1502358         -0.180     Sand     5190.0          444.0  \n",
       "1502359          0.043  Fairway      887.0          444.0  \n",
       "1502360         -0.652    Green       78.0          444.0  \n",
       "1502361          0.009    Green       22.0          444.0  \n",
       "1502362         -0.140      Tee    15984.0          444.0  \n",
       "1502363         -0.370     Sand     4861.0          444.0  \n",
       "1502364         -0.036    Rough      709.0          444.0  \n",
       "1502365         -0.375    Green      120.0          444.0  \n",
       "1502366          0.001    Green       14.0          444.0  \n",
       "\n",
       "[1502367 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_df = pd.read_csv('D:/data/quicken_loans_sg.csv')\n",
    "sg_df.rename(columns = {'shot_dist_x': 'shot_dist', 'shot_dist_y': 'hole_dist_yds'}, inplace = True)\n",
    "sg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "player_ls = sg_df['player_name'].unique()\n",
    "\n",
    "grouped_df = sg_df[['player_id', 'player_name', 'date_started', 'tournament_id', 'course_num', 'hole_num',\n",
    "                    'hole_id','round_num', 'stroke_gained', 'par','hole_dist_yds']].groupby(['player_id', \n",
    "                    'player_name','date_started', 'hole_id', 'tournament_id', 'course_num', 'round_num', \n",
    "                    'hole_num', 'par', 'hole_dist_yds']).sum()\n",
    "\n",
    "grouped_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_id</th>\n",
       "      <th>player_name</th>\n",
       "      <th>date_started</th>\n",
       "      <th>hole_id</th>\n",
       "      <th>tournament_id</th>\n",
       "      <th>course_num</th>\n",
       "      <th>round_num</th>\n",
       "      <th>hole_num</th>\n",
       "      <th>par</th>\n",
       "      <th>hole_dist_yds</th>\n",
       "      <th>stroke_gained</th>\n",
       "      <th>days_elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017_006_6_1_1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>0.28</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017_006_6_1_10</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017_006_6_1_11</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017_006_6_1_12</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017_006_6_1_13</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>0.28</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017_006_6_1_14</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>430.000000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017_006_6_1_15</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017_006_6_1_16</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>417.000000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017_006_6_1_17</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017_006_6_1_18</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>551.000000</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017_006_6_1_2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017_006_6_1_3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>422.000000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017_006_6_1_4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017_006_6_1_5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>467.000000</td>\n",
       "      <td>0.17</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017_006_6_1_6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>460.000000</td>\n",
       "      <td>0.17</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017_006_6_1_7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017_006_6_1_8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>454.000000</td>\n",
       "      <td>1.17</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2017_006_6_1_9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>0.41</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>2017_006_6_2_1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>0.28</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>2017_006_6_2_10</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>0.92</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>2017_006_6_2_11</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>2017_006_6_2_12</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>2017_006_6_2_13</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>0.28</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>2017_006_6_2_14</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>430.000000</td>\n",
       "      <td>1.08</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>2017_006_6_2_15</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>0.99</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>2017_006_6_2_16</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>417.000000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>2017_006_6_2_17</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>1.12</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>2017_006_6_2_18</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>551.000000</td>\n",
       "      <td>0.74</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>2017_006_6_2_2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>19846.0</td>\n",
       "      <td>Brian Gay</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>2017_006_6_2_3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>422.000000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383623</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>2017_034_503_1_15</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383624</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>2017_034_503_1_16</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383625</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>2017_034_503_1_17</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>-2.98</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383626</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>2017_034_503_1_18</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>444.000000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383627</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>2017_034_503_1_2</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383628</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>2017_034_503_1_3</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>427.897131</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383629</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>2017_034_503_1_4</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>481.000000</td>\n",
       "      <td>0.28</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383630</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>2017_034_503_1_5</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383631</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>2017_034_503_1_6</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>574.000000</td>\n",
       "      <td>0.79</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383632</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>2017_034_503_1_7</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>443.000000</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383633</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>2017_034_503_1_8</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383634</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>2017_034_503_1_9</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>406.000000</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383635</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>2017_034_503_2_1</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>434.000000</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383636</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>2017_034_503_2_10</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>0.17</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383637</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>2017_034_503_2_11</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383638</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>2017_034_503_2_12</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383639</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>2017_034_503_2_13</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>-2.46</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383640</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>2017_034_503_2_14</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>421.000000</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383641</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>2017_034_503_2_15</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383642</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>2017_034_503_2_16</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383643</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>2017_034_503_2_17</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383644</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>2017_034_503_2_18</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>444.000000</td>\n",
       "      <td>1.08</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383645</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>2017_034_503_2_2</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383646</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>2017_034_503_2_3</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383647</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>2017_034_503_2_4</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>481.000000</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383648</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>2017_034_503_2_5</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>0.17</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383649</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>2017_034_503_2_6</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>574.000000</td>\n",
       "      <td>0.79</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383650</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>2017_034_503_2_7</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>443.000000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383651</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>2017_034_503_2_8</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383652</th>\n",
       "      <td>51766.0</td>\n",
       "      <td>Wyndham Clark</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>2017_034_503_2_9</td>\n",
       "      <td>34.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>406.000000</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>383653 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        player_id    player_name date_started            hole_id  \\\n",
       "0         19846.0      Brian Gay   2017-01-12     2017_006_6_1_1   \n",
       "1         19846.0      Brian Gay   2017-01-12    2017_006_6_1_10   \n",
       "2         19846.0      Brian Gay   2017-01-12    2017_006_6_1_11   \n",
       "3         19846.0      Brian Gay   2017-01-12    2017_006_6_1_12   \n",
       "4         19846.0      Brian Gay   2017-01-12    2017_006_6_1_13   \n",
       "5         19846.0      Brian Gay   2017-01-12    2017_006_6_1_14   \n",
       "6         19846.0      Brian Gay   2017-01-12    2017_006_6_1_15   \n",
       "7         19846.0      Brian Gay   2017-01-12    2017_006_6_1_16   \n",
       "8         19846.0      Brian Gay   2017-01-12    2017_006_6_1_17   \n",
       "9         19846.0      Brian Gay   2017-01-12    2017_006_6_1_18   \n",
       "10        19846.0      Brian Gay   2017-01-12     2017_006_6_1_2   \n",
       "11        19846.0      Brian Gay   2017-01-12     2017_006_6_1_3   \n",
       "12        19846.0      Brian Gay   2017-01-12     2017_006_6_1_4   \n",
       "13        19846.0      Brian Gay   2017-01-12     2017_006_6_1_5   \n",
       "14        19846.0      Brian Gay   2017-01-12     2017_006_6_1_6   \n",
       "15        19846.0      Brian Gay   2017-01-12     2017_006_6_1_7   \n",
       "16        19846.0      Brian Gay   2017-01-12     2017_006_6_1_8   \n",
       "17        19846.0      Brian Gay   2017-01-12     2017_006_6_1_9   \n",
       "18        19846.0      Brian Gay   2017-01-13     2017_006_6_2_1   \n",
       "19        19846.0      Brian Gay   2017-01-13    2017_006_6_2_10   \n",
       "20        19846.0      Brian Gay   2017-01-13    2017_006_6_2_11   \n",
       "21        19846.0      Brian Gay   2017-01-13    2017_006_6_2_12   \n",
       "22        19846.0      Brian Gay   2017-01-13    2017_006_6_2_13   \n",
       "23        19846.0      Brian Gay   2017-01-13    2017_006_6_2_14   \n",
       "24        19846.0      Brian Gay   2017-01-13    2017_006_6_2_15   \n",
       "25        19846.0      Brian Gay   2017-01-13    2017_006_6_2_16   \n",
       "26        19846.0      Brian Gay   2017-01-13    2017_006_6_2_17   \n",
       "27        19846.0      Brian Gay   2017-01-13    2017_006_6_2_18   \n",
       "28        19846.0      Brian Gay   2017-01-13     2017_006_6_2_2   \n",
       "29        19846.0      Brian Gay   2017-01-13     2017_006_6_2_3   \n",
       "...           ...            ...          ...                ...   \n",
       "383623    51766.0  Wyndham Clark   2017-06-22  2017_034_503_1_15   \n",
       "383624    51766.0  Wyndham Clark   2017-06-22  2017_034_503_1_16   \n",
       "383625    51766.0  Wyndham Clark   2017-06-22  2017_034_503_1_17   \n",
       "383626    51766.0  Wyndham Clark   2017-06-22  2017_034_503_1_18   \n",
       "383627    51766.0  Wyndham Clark   2017-06-22   2017_034_503_1_2   \n",
       "383628    51766.0  Wyndham Clark   2017-06-22   2017_034_503_1_3   \n",
       "383629    51766.0  Wyndham Clark   2017-06-22   2017_034_503_1_4   \n",
       "383630    51766.0  Wyndham Clark   2017-06-22   2017_034_503_1_5   \n",
       "383631    51766.0  Wyndham Clark   2017-06-22   2017_034_503_1_6   \n",
       "383632    51766.0  Wyndham Clark   2017-06-22   2017_034_503_1_7   \n",
       "383633    51766.0  Wyndham Clark   2017-06-22   2017_034_503_1_8   \n",
       "383634    51766.0  Wyndham Clark   2017-06-22   2017_034_503_1_9   \n",
       "383635    51766.0  Wyndham Clark   2017-06-23   2017_034_503_2_1   \n",
       "383636    51766.0  Wyndham Clark   2017-06-23  2017_034_503_2_10   \n",
       "383637    51766.0  Wyndham Clark   2017-06-23  2017_034_503_2_11   \n",
       "383638    51766.0  Wyndham Clark   2017-06-23  2017_034_503_2_12   \n",
       "383639    51766.0  Wyndham Clark   2017-06-23  2017_034_503_2_13   \n",
       "383640    51766.0  Wyndham Clark   2017-06-23  2017_034_503_2_14   \n",
       "383641    51766.0  Wyndham Clark   2017-06-23  2017_034_503_2_15   \n",
       "383642    51766.0  Wyndham Clark   2017-06-23  2017_034_503_2_16   \n",
       "383643    51766.0  Wyndham Clark   2017-06-23  2017_034_503_2_17   \n",
       "383644    51766.0  Wyndham Clark   2017-06-23  2017_034_503_2_18   \n",
       "383645    51766.0  Wyndham Clark   2017-06-23   2017_034_503_2_2   \n",
       "383646    51766.0  Wyndham Clark   2017-06-23   2017_034_503_2_3   \n",
       "383647    51766.0  Wyndham Clark   2017-06-23   2017_034_503_2_4   \n",
       "383648    51766.0  Wyndham Clark   2017-06-23   2017_034_503_2_5   \n",
       "383649    51766.0  Wyndham Clark   2017-06-23   2017_034_503_2_6   \n",
       "383650    51766.0  Wyndham Clark   2017-06-23   2017_034_503_2_7   \n",
       "383651    51766.0  Wyndham Clark   2017-06-23   2017_034_503_2_8   \n",
       "383652    51766.0  Wyndham Clark   2017-06-23   2017_034_503_2_9   \n",
       "\n",
       "        tournament_id  course_num  round_num  hole_num  par  hole_dist_yds  \\\n",
       "0                 6.0         6.0        1.0       1.0  4.0     480.000000   \n",
       "1                 6.0         6.0        1.0      10.0  4.0     351.000000   \n",
       "2                 6.0         6.0        1.0      11.0  3.0     194.000000   \n",
       "3                 6.0         6.0        1.0      12.0  4.0     440.000000   \n",
       "4                 6.0         6.0        1.0      13.0  4.0     477.000000   \n",
       "5                 6.0         6.0        1.0      14.0  4.0     430.000000   \n",
       "6                 6.0         6.0        1.0      15.0  4.0     398.000000   \n",
       "7                 6.0         6.0        1.0      16.0  4.0     417.000000   \n",
       "8                 6.0         6.0        1.0      17.0  3.0     194.000000   \n",
       "9                 6.0         6.0        1.0      18.0  5.0     551.000000   \n",
       "10                6.0         6.0        1.0       2.0  4.0     423.000000   \n",
       "11                6.0         6.0        1.0       3.0  4.0     422.000000   \n",
       "12                6.0         6.0        1.0       4.0  3.0     204.000000   \n",
       "13                6.0         6.0        1.0       5.0  4.0     467.000000   \n",
       "14                6.0         6.0        1.0       6.0  4.0     460.000000   \n",
       "15                6.0         6.0        1.0       7.0  3.0     176.000000   \n",
       "16                6.0         6.0        1.0       8.0  4.0     454.000000   \n",
       "17                6.0         6.0        1.0       9.0  5.0     506.000000   \n",
       "18                6.0         6.0        2.0       1.0  4.0     480.000000   \n",
       "19                6.0         6.0        2.0      10.0  4.0     351.000000   \n",
       "20                6.0         6.0        2.0      11.0  3.0     194.000000   \n",
       "21                6.0         6.0        2.0      12.0  4.0     440.000000   \n",
       "22                6.0         6.0        2.0      13.0  4.0     477.000000   \n",
       "23                6.0         6.0        2.0      14.0  4.0     430.000000   \n",
       "24                6.0         6.0        2.0      15.0  4.0     398.000000   \n",
       "25                6.0         6.0        2.0      16.0  4.0     417.000000   \n",
       "26                6.0         6.0        2.0      17.0  3.0     194.000000   \n",
       "27                6.0         6.0        2.0      18.0  5.0     551.000000   \n",
       "28                6.0         6.0        2.0       2.0  4.0     423.000000   \n",
       "29                6.0         6.0        2.0       3.0  4.0     422.000000   \n",
       "...               ...         ...        ...       ...  ...            ...   \n",
       "383623           34.0       503.0        1.0      15.0  4.0     296.000000   \n",
       "383624           34.0       503.0        1.0      16.0  3.0     171.000000   \n",
       "383625           34.0       503.0        1.0      17.0  4.0     420.000000   \n",
       "383626           34.0       503.0        1.0      18.0  4.0     444.000000   \n",
       "383627           34.0       503.0        1.0       2.0  4.0     341.000000   \n",
       "383628           34.0       503.0        1.0       3.0  4.0     427.897131   \n",
       "383629           34.0       503.0        1.0       4.0  4.0     481.000000   \n",
       "383630           34.0       503.0        1.0       5.0  3.0     223.000000   \n",
       "383631           34.0       503.0        1.0       6.0  5.0     574.000000   \n",
       "383632           34.0       503.0        1.0       7.0  4.0     443.000000   \n",
       "383633           34.0       503.0        1.0       8.0  3.0     202.000000   \n",
       "383634           34.0       503.0        1.0       9.0  4.0     406.000000   \n",
       "383635           34.0       503.0        2.0       1.0  4.0     434.000000   \n",
       "383636           34.0       503.0        2.0      10.0  4.0     462.000000   \n",
       "383637           34.0       503.0        2.0      11.0  3.0     158.000000   \n",
       "383638           34.0       503.0        2.0      12.0  4.0     411.000000   \n",
       "383639           34.0       503.0        2.0      13.0  5.0     523.000000   \n",
       "383640           34.0       503.0        2.0      14.0  4.0     421.000000   \n",
       "383641           34.0       503.0        2.0      15.0  4.0     296.000000   \n",
       "383642           34.0       503.0        2.0      16.0  3.0     171.000000   \n",
       "383643           34.0       503.0        2.0      17.0  4.0     420.000000   \n",
       "383644           34.0       503.0        2.0      18.0  4.0     444.000000   \n",
       "383645           34.0       503.0        2.0       2.0  4.0     341.000000   \n",
       "383646           34.0       503.0        2.0       3.0  4.0     431.000000   \n",
       "383647           34.0       503.0        2.0       4.0  4.0     481.000000   \n",
       "383648           34.0       503.0        2.0       5.0  3.0     223.000000   \n",
       "383649           34.0       503.0        2.0       6.0  5.0     574.000000   \n",
       "383650           34.0       503.0        2.0       7.0  4.0     443.000000   \n",
       "383651           34.0       503.0        2.0       8.0  3.0     202.000000   \n",
       "383652           34.0       503.0        2.0       9.0  4.0     406.000000   \n",
       "\n",
       "        stroke_gained  days_elapsed  \n",
       "0                0.28           166  \n",
       "1               -0.08           166  \n",
       "2                0.12           166  \n",
       "3                0.08           166  \n",
       "4                0.28           166  \n",
       "5                0.08           166  \n",
       "6               -0.01           166  \n",
       "7                0.02           166  \n",
       "8                0.12           166  \n",
       "9               -0.26           166  \n",
       "10               0.02           166  \n",
       "11               0.02           166  \n",
       "12              -0.88           166  \n",
       "13               0.17           166  \n",
       "14               0.17           166  \n",
       "15               0.05           166  \n",
       "16               1.17           166  \n",
       "17               0.41           166  \n",
       "18               0.28           165  \n",
       "19               0.92           165  \n",
       "20               0.12           165  \n",
       "21               0.08           165  \n",
       "22               0.28           165  \n",
       "23               1.08           165  \n",
       "24               0.99           165  \n",
       "25               0.02           165  \n",
       "26               1.12           165  \n",
       "27               0.74           165  \n",
       "28               0.02           165  \n",
       "29               0.02           165  \n",
       "...               ...           ...  \n",
       "383623          -1.29             5  \n",
       "383624          -0.95             5  \n",
       "383625          -2.98             5  \n",
       "383626           0.08             5  \n",
       "383627          -0.14             5  \n",
       "383628          -0.92             5  \n",
       "383629           0.28             5  \n",
       "383630           0.17             5  \n",
       "383631           0.79             5  \n",
       "383632          -0.92             5  \n",
       "383633           0.12             5  \n",
       "383634          -0.01             5  \n",
       "383635          -0.92             4  \n",
       "383636           0.17             4  \n",
       "383637          -1.01             4  \n",
       "383638          -0.98             4  \n",
       "383639          -2.46             4  \n",
       "383640          -0.98             4  \n",
       "383641          -0.29             4  \n",
       "383642           0.05             4  \n",
       "383643           0.02             4  \n",
       "383644           1.08             4  \n",
       "383645           0.86             4  \n",
       "383646           0.08             4  \n",
       "383647          -0.72             4  \n",
       "383648           0.17             4  \n",
       "383649           0.79             4  \n",
       "383650           0.08             4  \n",
       "383651           0.12             4  \n",
       "383652          -0.01             4  \n",
       "\n",
       "[383653 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "def create_datetime(x):\n",
    "    date = datetime.strptime(x, '%m_%d_%Y')\n",
    "    return date\n",
    "\n",
    "def days_elapsed(date):\n",
    "    return (now - date).days\n",
    "\n",
    "grouped_df['date_started'] = grouped_df['date_started'].apply(lambda x: create_datetime(x))\n",
    "grouped_df['days_elapsed'] = grouped_df['date_started'].apply(lambda x: days_elapsed(x))\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3676 samples, validate on 194 samples\n",
      "Epoch 1/75\n",
      "3676/3676 [==============================] - 15s - loss: 0.0743 - val_loss: 0.0551\n",
      "Epoch 2/75\n",
      "3676/3676 [==============================] - 10s - loss: 0.0197 - val_loss: 0.0075\n",
      "Epoch 3/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0130 - val_loss: 0.0726\n",
      "Epoch 4/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0104 - val_loss: 0.0027\n",
      "Epoch 5/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0085 - val_loss: 0.0281\n",
      "Epoch 6/75\n",
      "3676/3676 [==============================] - 10s - loss: 0.0069 - val_loss: 1.7003e-04\n",
      "Epoch 7/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0059 - val_loss: 0.0089\n",
      "Epoch 8/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0050 - val_loss: 2.8146e-04\n",
      "Epoch 9/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0043 - val_loss: 0.0251\n",
      "Epoch 10/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0045 - val_loss: 0.0010\n",
      "Epoch 11/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0034 - val_loss: 0.0073\n",
      "Epoch 12/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0038 - val_loss: 0.0051\n",
      "Epoch 13/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0032 - val_loss: 0.0079\n",
      "Epoch 14/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0031 - val_loss: 0.0018\n",
      "Epoch 15/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0025 - val_loss: 0.0201\n",
      "Epoch 16/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0029 - val_loss: 6.7110e-04\n",
      "Epoch 17/75\n",
      "3676/3676 [==============================] - 10s - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 18/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 19/75\n",
      "3676/3676 [==============================] - 10s - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 20/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 21/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0021 - val_loss: 1.5054e-04\n",
      "Epoch 22/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 23/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0020 - val_loss: 3.9860e-04\n",
      "Epoch 24/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 25/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0019 - val_loss: 0.0070\n",
      "Epoch 26/75\n",
      "3676/3676 [==============================] - 10s - loss: 0.0018 - val_loss: 0.0057\n",
      "Epoch 27/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0017 - val_loss: 5.4299e-04\n",
      "Epoch 28/75\n",
      "3676/3676 [==============================] - 10s - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 29/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0017 - val_loss: 4.2276e-04\n",
      "Epoch 30/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0016 - val_loss: 0.0040\n",
      "Epoch 31/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 32/75\n",
      "3676/3676 [==============================] - 10s - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 33/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0014 - val_loss: 3.1868e-04\n",
      "Epoch 34/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0016 - val_loss: 1.3423e-04\n",
      "Epoch 35/75\n",
      "3676/3676 [==============================] - 10s - loss: 0.0014 - val_loss: 0.0042\n",
      "Epoch 36/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0014 - val_loss: 6.6049e-04\n",
      "Epoch 37/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0014 - val_loss: 0.0048\n",
      "Epoch 38/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0013 - val_loss: 0.0033\n",
      "Epoch 39/75\n",
      "3676/3676 [==============================] - 10s - loss: 0.0013 - val_loss: 5.5858e-04\n",
      "Epoch 40/75\n",
      "3676/3676 [==============================] - 10s - loss: 0.0013 - val_loss: 7.0494e-04\n",
      "Epoch 41/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0012 - val_loss: 0.0037\n",
      "Epoch 42/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 43/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0012 - val_loss: 7.0439e-04\n",
      "Epoch 44/75\n",
      "3676/3676 [==============================] - 10s - loss: 0.0012 - val_loss: 0.0036\n",
      "Epoch 45/75\n",
      "3676/3676 [==============================] - 10s - loss: 0.0012 - val_loss: 0.0023\n",
      "Epoch 46/75\n",
      "3676/3676 [==============================] - 10s - loss: 0.0011 - val_loss: 3.8390e-04\n",
      "Epoch 47/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0012 - val_loss: 0.0034\n",
      "Epoch 48/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 49/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0012 - val_loss: 1.2899e-04\n",
      "Epoch 50/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0011 - val_loss: 1.3709e-04\n",
      "Epoch 51/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0011 - val_loss: 0.0051\n",
      "Epoch 52/75\n",
      "3676/3676 [==============================] - 10s - loss: 0.0011 - val_loss: 9.7287e-04\n",
      "Epoch 53/75\n",
      "3676/3676 [==============================] - 10s - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 54/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0010 - val_loss: 7.1992e-04\n",
      "Epoch 55/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0010 - val_loss: 1.1401e-04\n",
      "Epoch 56/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0010 - val_loss: 0.0062\n",
      "Epoch 57/75\n",
      "3676/3676 [==============================] - 9s - loss: 0.0011 - val_loss: 0.0034\n",
      "Epoch 58/75\n",
      "3676/3676 [==============================] - 10s - loss: 0.0010 - val_loss: 1.9712e-04\n",
      "Epoch 59/75\n",
      "3676/3676 [==============================] - 10s - loss: 9.6114e-04 - val_loss: 4.0011e-04\n",
      "Epoch 60/75\n",
      "3676/3676 [==============================] - 9s - loss: 9.8528e-04 - val_loss: 0.0012\n",
      "Epoch 61/75\n",
      "3676/3676 [==============================] - 9s - loss: 9.7895e-04 - val_loss: 0.0023\n",
      "Epoch 62/75\n",
      "3676/3676 [==============================] - 9s - loss: 9.3325e-04 - val_loss: 0.0024\n",
      "Epoch 63/75\n",
      "3676/3676 [==============================] - 9s - loss: 9.5118e-04 - val_loss: 0.0010\n",
      "Epoch 64/75\n",
      "3676/3676 [==============================] - 9s - loss: 9.3501e-04 - val_loss: 4.7919e-04\n",
      "Epoch 65/75\n",
      "3676/3676 [==============================] - 10s - loss: 8.9398e-04 - val_loss: 0.0011\n",
      "Epoch 66/75\n",
      "3676/3676 [==============================] - 10s - loss: 8.9598e-04 - val_loss: 4.9661e-04\n",
      "Epoch 67/75\n",
      "3676/3676 [==============================] - 10s - loss: 8.7444e-04 - val_loss: 0.0015\n",
      "Epoch 68/75\n",
      "3676/3676 [==============================] - 10s - loss: 9.2760e-04 - val_loss: 1.0477e-04\n",
      "Epoch 69/75\n",
      "3676/3676 [==============================] - 10s - loss: 8.8453e-04 - val_loss: 4.0552e-04\n",
      "Epoch 70/75\n",
      "3676/3676 [==============================] - 9s - loss: 8.9603e-04 - val_loss: 1.4673e-04\n",
      "Epoch 71/75\n",
      "3676/3676 [==============================] - 10s - loss: 8.4890e-04 - val_loss: 9.8243e-04\n",
      "Epoch 72/75\n",
      "3676/3676 [==============================] - 10s - loss: 8.3168e-04 - val_loss: 1.7811e-04\n",
      "Epoch 73/75\n",
      "3676/3676 [==============================] - 9s - loss: 8.6423e-04 - val_loss: 3.5166e-04\n",
      "Epoch 74/75\n",
      "3676/3676 [==============================] - 9s - loss: 8.4191e-04 - val_loss: 4.7761e-04\n",
      "Epoch 75/75\n",
      "3676/3676 [==============================] - 10s - loss: 8.5183e-04 - val_loss: 0.0018\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd8XNW16PHfmtGMerEluSC52xhsinHDYCB0bCfBCWl2\nAiRwE3raLbnkpbyQ5Cbcm5v7QidACBCK6WCIwReMCbbB3eBe5C65SLKtLk3d7485MxqNZqQZFfvI\nXt/PRx9pzjkzs23M0pq1915HjDEopZQ6dThO9ACUUkodXxr4lVLqFKOBXymlTjEa+JVS6hSjgV8p\npU4xGviVUuoUo4FfKaVOMRr4lVLqFKOBXymlTjFpJ3oA8RQVFZnhw4ef6GEopVSfsWbNmmpjTHEy\n19oy8A8fPpzVq1ef6GEopVSfISJ7k71WSz1KKXWK0cCvlFKnGA38Sil1irFljV8ppU4En89HeXk5\nLS0tJ3ooCWVkZFBaWorL5erya2jgV0opS3l5Obm5uQwfPhwROdHDaccYw5EjRygvL2fEiBFdfh0t\n9SillKWlpYXCwkJbBn0AEaGwsLDbn0g08CulVBS7Bv2wnhhfnwz8r68rp8HjP9HDUEqpPqnPBf7y\nY038+MXPWLjx0IkeilJK9Yp3332XsWPHMnr0aO69994ef/0+F/hbfIHQd3/gBI9EKaV6XiAQ4M47\n7+Sdd95h8+bNvPDCC2zevLlH36PPBX6PPwiAz/qulFInk5UrVzJ69GhGjhyJ2+1mzpw5vPnmmz36\nHn1uOacvYNp8V0qp3nDPW5vYfKCuR19z3Gl5/N8vju/wmoqKCoYMGRJ5XFpayooVK3p0HEll/CIy\nQ0S2iUiZiNwd57yIyP3W+fUiMtE6PlZEPo36qhORH3VnwF4r0/cGNONXSqmu6DTjFxEn8BBwFVAO\nrBKR+caY6KLTTGCM9XU+8AhwvjFmGzAh6nUqgNe7M2CfFfC9WupRSvWizjLz3lJSUsL+/fsjj8vL\nyykpKenR90gm458KlBljdhljvMA8YHbMNbOBZ0zIcqBARAbHXHMFsNMYk3Tr0HjCAd+nGb9S6iQ0\nZcoUduzYwe7du/F6vcybN49rr722R98jmRp/CbA/6nE5oay+s2tKgINRx+YALyR6ExG5BbgFYOjQ\noQkHEy7xaOBXSp2M0tLSePDBB7nmmmsIBALcfPPNjB/fs58+jsvkroi4gWuBnya6xhjzGPAYwOTJ\nkxPO3LZm/Dq5q5Q6Oc2aNYtZs2b12usnU+qpAIZEPS61jqVyzUxgrTHmcFcGGS1S49eMXymluiSZ\nwL8KGCMiI6zMfQ4wP+aa+cCN1uqeaUCtMSa6zDOXDso8qfDqOn6llOqWTks9xhi/iNwFLAScwJPG\nmE0icpt1/lFgATALKAOagJvCzxeRbEIrgm7tiQH7tMavlOpFxhhbN2ozpvtl7qRq/MaYBYSCe/Sx\nR6N+NsCdCZ7bCBR2Y4xteLTGr5TqJRkZGRw5csS2rZnD/fgzMjK69Tp9dueu1viVUj2ttLSU8vJy\nqqqqTvRQEgrfgas7+lzg13X8Sqne4nK5unVnq76izzVp0xq/Ukp1T58L/JENXH6t8SulVFf0vcBv\nlXo8mvErpVSX9L3AH9B1/Eop1R19LvD7dHJXKaW6pc8Ffm3SppRS3dP3Ar9u4FJKqW7pc4Ffm7Qp\npVT39LnA79Eav1JKdUufC/w+XdWjlFLd0ucCv9b4lVKqe/pc4I9u0tYT7UmVUupU0+cCvzeqxOMP\nauBXSqlU9bnAHz2pqxO8SimVuj4X+D1RGb82alNKqdT1ucAfneV7AoETOBKllOqbkgr8IjJDRLaJ\nSJmI3B3nvIjI/db59SIyMepcgYi8IiJbRWSLiFzQnQF7A0HS00LD1pU9SimVuk4Dv4g4gYeAmcA4\nYK6IjIu5bCYwxvq6BXgk6tx9wLvGmDOAc4Et3Rmwzx8kJz0t8rNSSqnUJJPxTwXKjDG7jDFeYB4w\nO+aa2cAzJmQ5UCAig0UkH7gE+AuAMcZrjKnpzoC9gSBZ6U5AJ3eVUqorkgn8JcD+qMfl1rFkrhkB\nVAF/FZF1IvKEiGR3dbDGGHwBQ7Y7lPFrvx6llEpdb0/upgETgUeMMecBjUC7OQIAEblFRFaLyOpE\nd7gPB/pIqUdr/EoplbJkAn8FMCTqcal1LJlryoFyY8wK6/grhH4RtGOMecwYM9kYM7m4uDjuQMKB\nPjsS+DXjV0qpVCUT+FcBY0RkhIi4gTnA/Jhr5gM3Wqt7pgG1xpiDxphDwH4RGWtddwWwuauDDe/a\nzQ7X+HVyVymlUpbW2QXGGL+I3AUsBJzAk8aYTSJym3X+UWABMAsoA5qAm6Je4vvAc9YvjV0x51IS\nzvC1xq+UUl3XaeAHMMYsIBTco489GvWzAe5M8NxPgcndGGNEa8avNX6llOqqPrVzN5zhZ+tyTqWU\n6rK+FfjbZfwa+JVSKlV9KvDH1vg9OrmrlFIp61OBXzN+pZTqvr4Z+N26nFMppbqqbwX+gK7qUUqp\n7upbgT9mA5eu41dKqdTZMvA3tPjjHg9n+FlurfErpVRX2TLw+038Eo7XuuNWhsuJ0yEa+JVSqgts\nGfhNgsAfvseuO82B2+nQGr9SSnWBLQN/MEE891gZvsspuJwSqfkrpZRKni0Df+KMPxTo051O3GkO\nLfUopVQX2DLwJ8r4w6t4XGmCy6mBXymlusKmgb/jjN/tdFiBX2v8SimVKlsG/gRxH28giAg4HVaN\nXzN+pZRKmS0Df6KM3xsI4nY6EAmVenRyVymlUmfLwJ8w4/eHAj+gk7tKKdVFtgz8CWv8gSDutNCQ\ndXJXKaW6JqnALyIzRGSbiJSJyN1xzouI3G+dXy8iE6PO7RGRDSLyqYisTub9Osr4Xc5w4JfIhi6l\nlFLJ6/SeuyLiBB4CrgLKgVUiMt8YsznqspnAGOvrfOAR63vYZcaY6mQHlTjjN20y/npf/J4+Siml\nEksm458KlBljdhljvMA8YHbMNbOBZ0zIcqBARAZ3dVCJNnB5/a2lHreWepRSqkuSCfwlwP6ox+XW\nsWSvMcD7IrJGRG5JZlAdbeBqLfVo4FdKqa7otNTTAy4yxlSIyADgPRHZaoz5KPYi65fCLQBZg0fF\nfaE2GX+abuBSSqmuSCbjrwCGRD0utY4ldY0xJvy9EnidUOmoHWPMY8aYycaYyQ6HM+5AfIEgbqcA\n6Dp+pZTqomQC/ypgjIiMEBE3MAeYH3PNfOBGa3XPNKDWGHNQRLJFJBdARLKBq4GNnb1hwg1cbTJ+\n7cevlFJd0WmpxxjjF5G7gIWAE3jSGLNJRG6zzj8KLABmAWVAE3CT9fSBwOsiEn6v540x73b2nolq\n/L5AkJyM0JC1xq+UUl2TVI3fGLOAUHCPPvZo1M8GuDPO83YB56Y6qESrejxRO3e1SZtSSnWNLXfu\nGiAQJ+33BoK4otbxa5M2pZRKnS0DP4DHH2h3zBcIkh7u1WPdgSvRpwOllFLx2Tbwt/jaZ/NtWzaE\nvvsTTQgopZSKy7aBP37GH9WywfquE7xKKZUa2wb+ZDN+bdSmlFKpsXHgb5/xewPRvXokckwppVTy\n+kzgN8ZYN2Jp3bkLWupRSqlU2Tjwtw3o4Unc6F49oIFfKaVSZd/AHzO5G+7L067Gr4FfKaVSYtvA\n74kp9YQDfPSNWAC8OrmrlFIpsW3gjy31xGb87rRQrV8zfqWUSo2NA39MqSdBxq+BXymlUtN3Ar+V\n8afHlno08CulVErsG/hjbrIS7sTZfnJXa/xKKZUK+wb+BBm/O9KkzdHmuFJKqeTYMvALcSZ3rZJO\npC2zTu4qpVSX2DPwi7Rr0hab8evkrlJKdY0tA79D2mf8rev4Q5m+lnqUUqprkgr8IjJDRLaJSJmI\n3B3nvIjI/db59SIyMea8U0TWicjbSb5fuw1crRm/E9DJXaWU6qpOA7+IOIGHgJnAOGCuiIyLuWwm\nMMb6ugV4JOb8D4EtSQ9K2rds8EVq/OEmbVrjV0qprkgm458KlBljdhljvMA8YHbMNbOBZ0zIcqBA\nRAYDiEgp8HngiaQHJZJwcjeyqkebtCmlVJckE/hLgP1Rj8utY8le8yfgJ0DSEVqk/XJOT4ImbbqB\nSymlUtOrk7si8gWg0hizJolrbxGR1SKy2u/ztQv84cw+dueu3oFLKaVSk0zgrwCGRD0utY4lc810\n4FoR2UOoRHS5iDwb702MMY8ZYyYbYyZnpLs7bdLmdAhOh2ipRymlUpRM4F8FjBGRESLiBuYA82Ou\nmQ/caK3umQbUGmMOGmN+aowpNcYMt573gTHm+s7eUEQSTu6Ga/sQmuDVwK+UUqlJ6+wCY4xfRO4C\nFgJO4EljzCYRuc06/yiwAJgFlAFNwE3dGZRDBE8nGX/4Z63xK6VUajoN/ADGmAWEgnv0sUejfjbA\nnZ28xofAh8m8X7zJXW+kSZtEjrmdDt3ApZRSKbLpzl2J26TN7XQg0hr4XU6HlnqUUipFtgz8IvHa\nMgfb1PchtJlLd+4qpVRqbBn4HSIEggZ/VDbv9QfblHlAa/xKKdUVtgz84WpOdNYfL+N3Ox34tMav\nlFIpsWXgd1iRP7rOH8r4Y0o9WuNXSqmU2TPwW9/bBP54NX6n1viVUipVtgz8Esn429b43TEZvztN\na/xKKZUqWwZ+R7jGH5Xxx13Vo6UepZRKmS0Dfzjjj779ojcQJ+PXwK+UUimzZeBvzfijVvX4TfzJ\nXe3OqZRSKbFp4G+/qscTdwOXZvxKKZUqWwb+eJO7vrjLOUUnd5VSKkW2DPzxJne9gWDkJixh2qRN\nKaVSZ8vAH8n4/W1X9cRr2aClHqWUSo0tA3+8yV2vP9FyTp3cVUqpVNg08LdfzhnK+Nt359Qav1JK\npcaWgV/iZPyeOBl/eB1/6D4wSimlkmHLwA+QnubAE9OkLXYDl8vpwBgIBDXwK6VUsmwb+DNczqRa\nNoTOaeBXSqlkJRX4RWSGiGwTkTIRuTvOeRGR+63z60VkonU8Q0RWishnIrJJRO5JdmAZLkek1OMP\nBAka2tX4w78ItM6vlFLJ6zTwi4gTeAiYCYwD5orIuJjLZgJjrK9bgEes4x7gcmPMucAEYIaITEtm\nYBkuZ2Q5Zzijb1/jF+u8Bn6llEpWMhn/VKDMGLPLGOMF5gGzY66ZDTxjQpYDBSIy2HrcYF3jsr6S\nqstkpLWWesKbtOL16gEN/EoplYpkAn8JsD/qcbl1LKlrRMQpIp8ClcB7xpgV8d5ERG4RkdUisrqq\nqqpNqSdcyklY49dGbUoplbRen9w1xgSMMROAUmCqiJyV4LrHjDGTjTGTi4uLSY+a3I0E/tidu1rj\nV0qplCUT+CuAIVGPS61jKV1jjKkBFgMzkhlYqMYfCujhG6prjV8ppbovmcC/ChgjIiNExA3MAebH\nXDMfuNFa3TMNqDXGHBSRYhEpABCRTOAqYGsyA8uIWsffmvE721wTLvVoozallEpeWmcXGGP8InIX\nsBBwAk8aYzaJyG3W+UeBBcAsoAxoAm6ynj4YeNpaGeQAXjLGvJ3MwKLX8bdO7rZv0gbxM/4WX4D0\nNEek4ZtSSqmQTgM/gDFmAaHgHn3s0aifDXBnnOetB87rysDS0xx4/MlN7sbW+Bs9fqb9fhH/9ZVz\nmHn24K68vVJKnbT6xM7dSI2/3QaucI2/7aqeqnoP9S1+dlU3HoeRKqVU32LjwJ/Kcs62GX9di6/N\nd6WUUq1sHPhDO3eNMZEafrIbuGqbQwG/vsV/HEaqlFJ9i60DvzGhbN+bYDlnohp/OPDXNWvGr5RS\nsWwb+MP3123xBfFaNfzYjD98TWyNv645lOlrxq+UUu3ZNvBnuEJr9j2+QCTjj73ZeuelHs34lVIq\nlu0Df4sv2EGNP/7OXa3xK6VUYjYO/Fapxx9IXONPi79zN1Lj74GM/7tPr2LhpkPdfh2llLIL+wb+\ntHDGH4jK+NvuwnUnuANXOOB3N+Nv8QV4f0sln+w80q3XUUopO7Fv4I8q9Xg6W9UTu47fyvibvAH8\n3WjgFn6dWl0dpJQ6idg48IdX9URl/I62w3U6BIckrvEDNHi6nvXXWK9T0+Tt8msopZTd2Dbwp0eV\nerz+IC6n4HC0b7jmcjo6DPzdKffUasavlDoJ2TbwhzN+jz9oBf74Q3U7He02cNU1+xiYlw50L2jX\nNlkZvwZ+pdRJxMaBv+3kbmx9P8yV1jbjN8ZQ1+KntF8W0EMZf5MGfqXUycO2gT89spwziDeQOON3\nOaXNPXcbPH4CQUNpv0yge5u4oks9oc7TSinV99k28LfduWvatWQOi63xh4N1a+DvfsbvDxoavYEu\nv45SStmJfQN/9ORuINiuXUNYbI0/3KcnXOrpziau6PkBXdmjlDpZ2Dbwu5yhpZotviC+jiZ30+Jn\n/CUF3c/4o7t76soepdTJIqnALyIzRGSbiJSJyN1xzouI3G+dXy8iE63jQ0RksYhsFpFNIvLDZAcm\nIpG7cHk7mtx1Otrs3A0H6MIcNxkuR7dq/NGreXSCVyl1sug08Fs3Sn8ImAmMA+aKyLiYy2YCY6yv\nW4BHrON+4F+MMeOAacCdcZ6bUPhmLL5AsF27hjCXU9pk/OEsPT/TRW6Gq9s1/n5ZLkCXdCqlTh7J\nZPxTgTJjzC5jjBeYB8yOuWY28IwJWQ4UiMhgY8xBY8xaAGNMPbAFKEl2cBlpjkjLho4y/uiWDeGa\nfl6mi7yMtG4H/mGF2ZGfe9ue6kYq61p6/X2UUqe2ZAJ/CbA/6nE57YN3p9eIyHDgPGBFsoMLl3p8\nHSznjFfjdwjkuNPIzXB1e3J3WGFokrjmOJR6bnt2Db/9+5Zefx+l1KntuEzuikgO8CrwI2NMXYJr\nbhGR1SKyuqqqCoB0lzN0By5/4lU9rphVPbXNPvIyXTgcQm5GGnXdzPgH5WXgdjqOS8ZfcayZg7XN\nvf4+SqlTWzKBvwIYEvW41DqW1DUi4iIU9J8zxryW6E2MMY8ZYyYbYyYXFxcDoTtuefwdZ/yxG7hq\nm33kZ4bq8nkZri5P7oZ7BOVlusjPclHb3LvLOVt8Aeo9fo406LJRpVTvSibwrwLGiMgIEXEDc4D5\nMdfMB260VvdMA2qNMQdFRIC/AFuMMf+T6uAyXI5IAO54VU/byd28DCvwZ3a9xl8bNUmcn+nq9VJP\nVb0HgOoGT6++j1JKdRr4jTF+4C5gIaHJ2ZeMMZtE5DYRuc26bAGwCygDHgfusI5PB24ALheRT62v\nWckOLsPlxOMP4guYpJu0RWf8ud3I+KMDf0Gmq9dLPeGAX9fib3d/AaWU6klpyVxkjFlAKLhHH3s0\n6mcD3BnneUuB+Oswk5CRFprc7WxVT+zk7uD80Oat3PS0yBxBoucnEs7w8zNdFGS5OFDTu6ttqqNK\nPEcbvQzKz+jV91NKnbpsu3MXwqWe0M3WE/bqSZOYDVx+8iIZf+j3Wley/nCGX5DlIu84ZvwARxq1\n3KOU6j02D/zO5Gr8Mev48zJDAT/XqvV3pc7fttTj7v3AXx8V+HWCVynVi/pE4O9o5647rbXGH/4l\nEVnVk9lDgT/LRYPH3+5OXz2pSjN+pdRxYuvAn+5y0OwL4A8a3E5n3GvcUTX+6GANraWermziCr9W\nboYr8nq9mfVXN3gozg3dNUwzfqVUb7J14M9Ic0bq9660RL16HAQNBIImYeDvSo2/rtlHbkYaTodQ\nkHUcAn+9lxFF2bic0maiVymlepq9A7+rNcvv6EYsAL5AMNKgLbKO3/reld270ctCw997cy1/OOMv\nzE7niK7lV0r1IpsH/tbhJZ7cDX0S8AaC7TL+vG5O7sYG/t7cvVtV76E4J53CHDdHGzXjV0r1HpsH\n/s4z/vAvBJ+/feDP6Uapp6bJGynxFGS5gd4r9YTbNRTluCnMSadaA79SqhfZPPC3Di9xr57QcW9U\nqScc+J0OIdvtjNyOMRXRGX9BL5d6wmv4i3LSKcp2a6lHKdWrbB3409OiMv4O1vED+PyGWivAhyd1\nQz93rW1DbbO/3bLQ3gv8oQy/ODdU6tFVPUqp3mTrwN+VGn9OehppUZ8OcrtwMxZjTKjZW9Qnh9yM\ntF4r9YQ3bxXlpFOYk06zL0CTt+vtpJVSqiP2DvxpSdT4o1b1RJdnwvIyXdR7UgvYLb4g3kCwzWsV\nZPVe24bw5q2i3HT6Z4fmEzTrV0r1FlsH/nRXCqWeQNBq19A28Hcl44+dJA7/XNPUO8E4nPEXZrsp\nygkFfm3PrJTqLbYO/ElN7qa1zfjzMto2HM3NcEUmfZMVL/D3Zr+e6gYPeRlpZLicFGaHdu/qkk6l\nVG+xeeBPJuO3avz+UF0+ttTTnYy/INMdOZaf5aKm1wK/lyKrXUNhjpZ6lFK9q88E/kRN2tLTOqnx\nZ7hSDvzhkk5sqSfVTw7Jqqr3UJRjBX4r46/WRm1KqV5i78AfleV3dLN1aG3ZEC/j9waCtPgCSb9v\n/FJP6PaLoXvO9KzqhtCuXYBMt5Nst1MzfqVUr7F34G+T8Xcc+Ju8ARq9gXaTu3ld6NAZN/BnufAH\nDY3e5H+BJKsqqjMnQP8c3cSllOo9SQV+EZkhIttEpExE7o5zXkTkfuv8ehGZGHXuSRGpFJGNqQ4u\nuRp/6Hg4ULbP+FPv11PX7EOk7Uaw3mrN3OILUN/ij6zmgVC554hO7iqlekmngV9EnMBDwExgHDBX\nRMbFXDYTGGN93QI8EnXuKWBGVwbndEiktt/RzdahdfdrvFIPpBb4a5t95Kan4XC0zivkWxO9Pb2k\nMxzgwzX+0M9ubc2slOo1yWT8U4EyY8wuY4wXmAfMjrlmNvCMCVkOFIjIYABjzEfA0a4OMLyJK2HG\nb/XpD9+1Kt4GLkitUVtts4/8rLavE+nJ38NtG6qidu2GFWanc1Qnd5VSvSSZwF8C7I96XG4dS/Wa\nLkm31vJ31o+/qj6UIYfvtxvW1Yw/eikn9F6pJ7x5K7rGH+7X0xsTyUopZZvJXRG5RURWi8jqqqqq\nyPFwo7bOJnerOqnxp7IUM96y0HDG39Nr+auj2jWEFeak4w+aLnUVVUqpziQT+CuAIVGPS61jqV7T\nIWPMY8aYycaYycXFxZHjGS4HTofgdCS42Xq4xm9lzvFaNkBqGX9NvMAfqfH3TuAvzG79hBFp26Dl\nHqVUL0gm8K8CxojICBFxA3OA+THXzAdutFb3TANqjTEHe2KAGS5nwjIPtG7sCgfQ8F23wnLcaYi0\nr/EbY/jDwq1sOVjX7jWjO3O2jsOB2+no8VJPVb2HXKtdQ5g2alNK9aZOA78xxg/cBSwEtgAvGWM2\nichtInKbddkCYBdQBjwO3BF+voi8AHwCjBWRchH5p1QGmOFyJty1C6GVPyLg8QdJT3O0CaAADoeQ\nk57W7r67+48289Dinby4an+b48aYuKUeESE/y9Xjt1+sbvC2qe9D6+5dXcuvlOoNaZ1fAsaYBYSC\ne/SxR6N+NsCdCZ47tzsDzHA5cKc5E54XEVxOB15/sF2wDsvLcLXbwLX5YC0Amw7Utjne7AvgC5i4\nrxXevduTqho8bVb0QHSpRzN+pVTPs83kbiIZaU7cHWT80FrnTxT44zVq23SgLvI9GGxdPRNv125Y\nfmbXevL7A0G+9cRyHvtoZ7tz0e0awvpZpZ6jWupRSvUC2wf+TLezTV/+eMJr/DsO/G0DdjjwN3kD\n7D7SGDke6cyZFSfjz4qf8W86UNthG+WnP9nLsrIj7cpKEJqUjt61C6GVSgVZrsjeBKWU6km2D/y3\nXjKKX3zhzA6vCc8BxE7IhsXr0LnpQC1nDMq1fm6d4A1v0Iqf8bfvye/xB/j6o59w/RMr4jaCO1zX\nwv97bztZbic7qxqpqGmOnGvxBahr8ber8UNolU9fmNz9YOthfvnmxi7d11gpdWLYPvCfXZrP5WcM\n7PAaV4qlnuoGD4frPMyeUILb6WBTRWudP9VSz9q9NTR6A2w+WMcv32zfjug//r4FbyDIfXPOA2Dp\njtY9CvHaNYQVZqf3ibtw3b+ojGc+2cuXH/6Y3dWNnT9BKXXC2T7wJ6PzGn/byd1whn/ukHzGDspt\nk/HXdBD4C7JcNHj8+ALByLFlZdU4HcJN04fz0upyXooq53xcVs38zw5w++dGceWZAxiYl85H26sj\n56vjtGsIK8xx275RW1W9h8/Ka7hm/ECONHiY/eBS/rG9qvMnqlNWk9fPq2vKCQR1V/qJdFIE/nDG\nn6jUE874wy0Qwit5xg/OZ/xpeWw8UBs5F97hG++1Iv16orL+JWXVTBhSwM8/P46LRhfxizc3srGi\nFq8/yC/e3MjQ/lncfukoRISLxxSztKw68o8+3q7dsMI+0Jp58bZKjIEfXDGG+XddxGkFmdz015U8\n/tEubTeh4npx1X7+5eXPeH7lvhM9lFPayRH4rUZtsffbDcvLdBEIGpqtGvymA3WU9sskP8vF+JJ8\napp8HKhtAUJBXQRy09u/Vmy/ntomHxvKa5g+uginQ7hvzgT6Z7u547m1/On97eysauSea8dH9hZc\ncnoxtc0+1pfXAK0N2uLX+NM51uTDH/Xpwm4WbTnM4PwMxg3OY0j/LF69/UKuGT+I/1iwhZdXl5/o\n4SkbWrIj9In3j/+7jWM2/0R7Mjs5An8SNX5obduw+UAd40/LA4h832jV+UM3bHe1ackcFn798Mqe\nT3YdIWjgotFFQKjHzoPfnMiBmmYe/nAnV48byGVnDIg8/6LRRYi0/uOP164hLLzS51gP7xvoKR5/\ngCU7qrn8jAGIhP6ustPTePhbExlVnM0bn6bUsUPZzOvryimrbOjR1/T6gyzfdYQLRxVS3+Lnj+9t\n69HXV8k7RQJ/a2vmBo+fPUcaGX9aPgBnDsrDIa11/9pmX9ylnAAFWaFgHC4HLSurJsvtZMKQgsg1\nk4b1457Z4yntl8kvv9j2tgX9s92cXZLPEmuCt7rB265dQ1ihVfe365LO5buO0uQNcOWZbSfeRYSZ\nZw1mxe6jHS5xtbvaZh8PLS7r03+GrjpY28yPX/yM37y9uUdfd+2+YzR5A3znwuHcMG0Yz6/Y124D\npTo+Too8vh4vAAAZe0lEQVTAn8wGLoDaZj9bD9ZhTGumn+l2Mqo4J7KyJ167hrBIxm+1bVhWVs20\nkYXt7hXwrfOHseQnl1HaL6vda1w8poi1+2qoa/GFbrkYZ2IX7N+vZ9GWw2S4HFwwqrDduRlnDSIQ\nNLy/+fAJGFnPeHLpbv6wcBtfeeRj9pxiq5Xe2XAIgI92VLH/aFOPve6SHVU4HcK0UYX8+MrTKchy\nc8/8zTofdAKcFIG/83X84VKPL5LZj7MCP8BZJfltMv5Egb8gqtRTUdPMrupGpltlnljh8kesi8cU\nEwgaPtl5JLR5K059H6LaNthwgtcYw6ItlVw0ujjup5Xxp+VR2i+Tdzcd6tb7bCiv7fEb3wB8vLOa\n6fd+QPmx+EHNHwjy4qr9nDk4j5omL19+eBlr9nb5XkJ9zoINBykpyESAl1a333TYVUt3VHPekALy\nMlzkZ7n4yTVjWbnnKPM/O9Bj76GSc5IE/o4z/ryo++5uOlBL/2w3g/IyIufHn5bHoboWquo9oRp/\noteJmtxdVhaq01+UIPAnMnFoP7LdTj7aXtVhxt/aqM1+Gf+2w/VU1DRz5ZkD4p4XEWaMH8TSHdVd\n3thVUdPMlx9exo9eXNedocb12Ee7qKhp5vGPdsU9v3hbFYfqWvjhFWN4/Y7p5Ge6mPv4Cv6+PvWG\ns01ePw8s2tFm414yyiobWLy1MuX3665DtS2s3nuMb0wZwqVjB/Diqv09ssDgWKOX9RW1XDymteX6\n1yYP4eySfH63YAuNHr33xPF0cgT+Tls2RAf+0MRudEYervdvOhDKMBO9jtMh5GakUdMUCvxFOemc\nPjAnpbG600LlkSU7quO2awjLz3ThdIgta/yLtoQC0uVnxA/8ECr3eANBPuhi8Hr8o134g4bF26p6\nNADuP9rEP7ZXkZuexrxV++N+onp+xV4G5KZzxZkDGF6UzWt3TOecknzufH4tf1u+N6X3e+TDnfzx\nve185eGP2XqofQvweLz+IN97ZjU3PbWKn7+xAa8/ucDrCwS5952tfLq/JqUxRntnY+iX26yzBzN3\n6lAq6z1d/m8YbdnOaoyBi8a0JkpOh/Cra8dzuM7DQ4vLuv0eKnknReB3Ox2kOYQsd/yePuEa/9FG\nD9sP17cp80Br2WfTgboOSz0Q7tfjZVlZNReNLkxY0unIJacXs+9oE3Ut/ribtyDUTrq/Tds2LNpy\nmHNK8xkQ9akp1sSh/SjOTefdjamXe442epm3ah+zJ5zGyKJsfvP25qSDX2eeW7EPhwiP3jAJbyDI\nU8v2tDlffqyJD7dX8Y0pQyKfJPtnu3n2u+dz6dhifvP2ZvYdSa7ufbC2mceX7GL66EIMhq89+gnL\ndx3p9Hl/W76X3dWNXDVuIM8u38c3H19OZX1Lp8+bt3Ifj/5jJzf+ZQU7DtcnNcZYCzYcZOzAXEYP\nyOGyscUMzEvnhR5Yc79kezW5GWmcW5rf5vikYf2YPeE0nly2m8q6zv+MqmecFIE/wxVqapYoCGe5\nnTgdwtp9NfgCJpLhh+VnuhjaP4tVe47iD8ZvyRxWkOlm1Z5jVDd4E9b3OxP9cTfeGv6wwmz77d6t\nbvCwbn8NV3TSRsPhEK4ZP5APt1XR7G3fw6gjT328hxZfkO9fPppffGEcu6obefrjPd0YdYjHH+Dl\n1fu54owBTB9dxDXjBvHMJ3valKPCjfS+MWVIm+dmuJzce905pDmEX7+9Kan3+++F2wkG4d7rzuG1\nO6YzMC+DG/+yssOSUU2Tl/sX7eDiMUU8dsMkHph7HpsO1PHFB5aydt+xhM+ra/Hx/97fwbml+aS7\nnHz7yZUcrE2tvHS4LlTmmXX2YADSnA6+MXkIH26vSjgfkgxjDEvLqpk+qoi0ODdV+vGVp+MLmB7N\n+oPB0H017DhHZgcnReC/efoI/vMr5yQ8LxIq0azaHZqgGx+T8YePrbTOdxT48zNdkXptVwP/8MIs\nSvtlAvHbNYQV5aR3e/euxx/gsY92csUfP2RFEtlmZxZvDe3WvSJBfT/ajPGDafYF+GhH8m0cGj1+\nnv54D1ePG8joAblcdsYALhtbzP2LdkQ2vIU1ePz898JtfJRkm4h3Nx7iSKOX66cNA+D2S0dR1+KP\nZLQ+a1L30tOL467IGpSfwQ+uGMP7Wyr5YGvHK5Y2VtTy2rpybpo+nCH9sygpyOSV2y7g7NJ87nph\nLU8t2x33eX96fwf1LT5+/vlxiAhfPPc0Xr39QtxpDub8eTn/m2DC/OHFOznW5OU/vnw2T900hboW\nP995clVKbcTf2XAQY+Dz5wyKHPu69QvwpTidZZO1qzrUnDC6zBNteFE2X588hOdX7uvyL5j6Fh+/\nfHMjl//xQyb+5j3G/Pwdzr3nf5n82/f55Zsb27RZUSdJ4B8zMJcrzuw4A83NSKPe4yfL7WREYXa7\n82eV5NNkZaYFHQV+a43/yOJsTivI7NJ4RYRLTg9l/YlW9UCoxBCb8a/dd4zX1pbzyc4j7D3SiMcf\nP5s2xvDuxoNc9T8f8bsFWzlY28JdL6xrFzxTtWhLJYPyMuL+8ox1/sj+FGS54pZ7Gj3+uMv4Xli5\nj9pmH7ddOipy7OdfGEezL8B/L2zd8LNy91Fm3vcRDy4u43vPrGZdB9lw2HMr9jGsMCsyIX/ukAKm\njy7kiSW78fgDLNpSSWW9h2+ePyzha9w8fQQji7O5563NcbuxQujv/ncLtlCQ6eKOy0ZHjhdkuXnu\nu+dz5ZkD+dVbm/mvd7e2+TvYWdXAs8v3MmfqUMZanWMhVIp8666LOHNwLj+Yty6y8zts/9Emnly2\nmy+fV8JZJfmMPy2fx26YxK7qBr73zOqE44y1YMMhTh+Yw+gBre9d2i+Lz51ezIuruz7Ju8T6xXxJ\n1CfdWD+4YjQiwn3v70j59ZfuqGbGn5bw7PK9jCrOYdbZg7j9c6P4+efP5PppQ3nmk718568rqWmy\n16fnnpTq/9cnReBPRm56KGCfOTgv7q7c6Lp/x6We0LlUV/PE+tKEEkr7ZTKiqP0vobBQv57QP9YG\nj5+fvb6B6x7+mH9+6TPmPr6cz/3hQ8b+/F0m//Y9Zj+4lNufXcNv397ME0t2Meex5dz27FoyXA6e\nuXkqr91xIXXNPn44b13cBlkVNc389LX1kU898RysbWbJjiouP3NAUnMbLqeDK88cyPtbDkdq9MGg\n4Yklu5j4m/e48cmVbT7ReP1Bnliym2kj+zNxaL/I8VHFOaEmeGv2s3rPUX63YAvfeOwTBOGxGyYx\nMC+D7z69usM159sP17Ny91G+OXVom//+t39uNJX1Hl5bW8HzK/cxKC+Dy8YmDlDuNAf3XDuevUea\neGJJolVBlXy88wg/vGJMu39LGS4nj3xrInOnDuXhD3fyk1fWRwLq7/6+hQyXk3++6vR2r1mQ5eaJ\nb0+hMDudf3p6dZtVQn9YuA2HwL9dMzZy7MLRRfzx6xNYufsoP3hhXafBv7KuhVV7j0bKPNHmTh3K\n4ToPi7eFAniDx89ra8u59W+r+f07Wzotpywtq2ZYYRZDC9t/igobnJ/J9ecP49W15eysSm7HcKPH\nz8/f2MD1f1lBusvBK7dfyOM3Tua3Xzqbf71mLN+9eCS//dLZ/PfXzmXV7mN86aFllFV2be4j1uJt\nlTz98R5bNJv7dH8NX3xgaUrPSerWiyeD8ARvokz1rKi6f6LlnND6S6GrZZ6wqSP6s/TfL+/wmqKc\ndBo8fhZvq+QXb2ykoqaZ7140gm9MGcLhOg8Ha5s5WNvCwdpmyo81s/1wPYu3VdLiC1KY7ea3XzqL\nOVOGROqqv/nSWfzklfXct2hHm+CyZu9Rbv3bGqobvMxbtZ+bp4/g364ZG1mjb4zh5dXl/ObtzQQN\nfH3ykLjjjWfmWYN4ZU05n+w6wsiibP715c9YsfsoU4f3Z8Xuo3z+/qU88M3zmDK8P298WsGhuhb+\n86vty3bfv2IMr6+r4Ot//oSggW+dP5T/M+tMstPTGFmcw1ce+Zjv/HUlr90+PfKpLNrzK/bhdjr4\n6qTSNsenjy7k7JJ87nt/B4frW/jB5WPi1qGjXTymmJlnDeLBxWV8eWIpJVGf/PyBIL9bsJURRdkJ\nPzmkOR387stnMSA3nfsW7eBIo5e5U4eyaGsl/z7jjITlv+LcdJ66aQrXPfIxN/91Fa/cfgFllQ3M\n/+wA3798NIPz234Cvfbc0zja4OFXb21m7uPL+fMNkxiQG39C/p2Nh0JlnjiB//IzBjAgN50HF5fx\nxqcVvL/5MB5/kIF56by3+TDPfLyXGy8Yxi2XjIzsOA/z+oN8svMIXzqvpMO/U4A7LhvFvFX7+J/3\ntvPQNycmvM7rD/LmpxXcZy2T/d7FI/iXq8fG3VMC8NVJpYwoyuLWv63hyw99zH9cdzYzxg9qt/ES\nQstOX19XQU2Tl2snnNbm0w/A7upGfv3WpsgvwSU7qvjTnPPIidPbKxktvgBHGr1t/g2l4qVV+/n5\nGxsZkJe4chBPUqMVkRnAfYATeMIYc2/MebHOzwKagO8YY9Ym89zjJbykM1HgL85NZ0BuOpX1ng4z\n/uFF2eSmpzFtZPsdqz0t3MPnpr+uYnhhFi/fegGTh/cHQuWteIwxHG30kp3evhXE1ycPYeXuozzw\nwQ4mD+vHJacX8/Lq/fzs9Y0MLsjgL9+ewstr9vOXpbtZvK2SP37tXAblZ3D3qxv4x/Yqpo7ozx++\neg7D4pTKEpk+uohst5M/LNzK7qpGRIT/+uo5fG1SKZsP1nHnc2uZ89hy/vXqsby8Zj/jBudxSZxa\ncF6Gi3uuPYv7Fm3np7PO5LKxrXMMowfk8OcbJnHDX1Zw27NrePrmqW3+pw63Ap519qB2gUlEuP3S\nUdzx3FocAnOmJvdL7WefP5PF2yr59Vub+Nerx1LV4KG6wcuq3Ucpq2zg0esnxQ0s0e/746tOpzg3\nnV++uZHF2yop7ZfJTdOHd/i+Ywbm8si3JvGdv67kjufW0uQNUJSTzq2fGxX3+u9MH8HAvAz++aXP\nmP3gMh6/cTJnleS3u+7vGw4yZkBO3H9XLqeDOVOGcP8HZZQfbeIbU4Ywe8JpTBzaj13VjTywaAeP\nL9nFM5/s5YYLhjFnyhBGFoeWOa/bd4xGb6DNgoZEinLSuXn6CB5cXMYdl9a2W4TR5PUzb+V+nliy\niwO1LZw5OI+Xb50Q+X+iI5OG9efNuy7ie0+v5gcvrCMvI41rxg/iC+eexgUjC1m15yjzVu1n4cZD\neANBHAL3f1DGeUML+PrkIVxxxgCe+ngPTyzZjcsp/J9ZZ+B2OvjN37dw3cPLeOLGKR1+oonV4gsw\nb+U+Hv5wJ5X1Hi45vZjvXz6aKUn8WSD0y+/Xb2/i2eX7uGh0EQ/MPY/+dyf99khn26VFxAlsB64C\nyoFVwFxjzOaoa2YB3ycU+M8H7jPGnJ/Mc+OZPHmyWb16dfJ/iiT880uf8traCt7+/kVx/+ED3PzU\nKj7YWsn6X10d2fQVKxg0NHj9Cc/3pHX7jvGVRz7m2xcO5yfXnEFmguWqqWj2BvjSQ8uoavAw6+xB\nPLt8H9NHF/LQNydGehEt3VHNv7+6noO1zWS6nAQN3D3zDG6YNixumawzdz2/lrfXH+SCkYX84Wvn\ntJk4rW/xcferG/j7htBKlwfmnscXzz2tS3+219eV8+MXP+PzZw/muoklDM7PZHB+Bgs3HeLu1zbw\nym0XxA0SgaBh1n1LGDUgm4e/NSnp93tg0Q7++N72dsevPHMAj984Oemlvu9uPMjPXt/IvV85h6vG\ndTxXFfbiqn38+6sbAPj9dWczd+rQDq/fdKCW7z29mqNNXv74tQl8/pzWzL6yroXzf7+IH1w+hh/H\nKTNBaJHApgN1nFOSH/cTUVllAw98sIO3PjtA0MCEIQVcN7GEnZUNPLtiH2t/cVWHCVVYbbOPi//z\nA84b2o8fXDGGippmKo41s/9YE+9sOMixJh9TR/Tn9ktHcenpxSkvp/YFgizdUc1b6w/w3qbD1Hv8\nuJyCLxBazffl80r4xpQhFOWk88a6Cl5es5/th1tLT9dNLOHuGWdEljIvK6uOJA2PXD+JScP6sauq\nkS0H69hyqI5jjV5GFOUwsjibUcU5DM7P4NW15Ty8eCeH6lqYOqI/00YW8vyKvVQ3eDl/RH++f/kY\nRhZnc6iuhcO1LZHNpceavBxr9HG0yUvFsWYqapq59ZKR/Ns1Y0lzOhCRNcaYycn8PSQT+C8AfmWM\nucZ6/FMAY8zvo675M/ChMeYF6/E24FJgeGfPjac3Av+v39rM35bvYeM915CeFj+A/vkfO/nzR7tY\n/bMruxTgeoMvEIysJ+8pO6sauPaBpTR6A9x4wTB+8YVx7d6jvsXH79/ZyqHaFv7vF8ellOXHOljb\nzGf7a7h63KC4f6/GGJ5dsY9P99Xwn185u9NSS0cSBeOxA3N590cXJwwUzd4ADgcJ/23E4/UHeePT\nCjJcTopy3BTlpFOUk06/DpYWJ2KMSfk5D39Yxrp9NTx6/SScSfx7rar3cOvfVrN2Xw1FOW6MAQP4\n/EHqPX7+98eXcHqCT5LJqqxr4c1PD/Dq2nK2HgrV0ycOLeC1O6Yn/RoPLS7jDwvbdu4syHIxeVh/\nbr90JJOGJZcVd6bFF+Af26tYVlbNpGH9uGb8oHafko0xrC+vZdHWSj53elHc995d3ch3n17FniNN\nOEXwWnM2bqeDvExX3DmQKcP78eMrT+eCUaG9QM3eAC+s3MefP9rJ4br216c5hIIsN/2zXRRkuemX\n5eJLE0qYGVWa6+nA/1VghjHmu9bjG4DzjTF3RV3zNnCvMWap9XgR8O+EAn+Hz416jVuAWwCGDh06\nae/e1HZIdqb8WBM7qxr53OmJP3L6A0EaPYG4NeKTzcrdR6lu8MSdzOvrKutbqDjWzKHaFg7UtnC4\nroWrxw1MqiRwsvP4A/z5H7s4WNuCCDgEBGFo/yy+d8nIHn2vLQfr+Pv6g1w4qpALU5gT8/qDvPXZ\nAfpluygpyKKkX2aXa+jHS12Lj/vf34HTKYwbnMcZg/IYWZyNy+mgvsXHrqpGdlY1sPdIE1OG92d6\ngs2fHn+ABRsO0uILMigvgwF56QzKy6B/trvTxKBPBv5ovZHxK6XUySyVwJ/Mr9EKIHrGq9Q6lsw1\nriSeq5RS6jhKppi6ChgjIiNExA3MAebHXDMfuFFCpgG1xpiDST5XKaXUcdRpxm+M8YvIXcBCQksy\nnzTGbBKR26zzjwILCK3oKSO0nPOmjp7bK38SpZRSSem0xn8iaI1fKaVSk0qN/5Rp2aCUUipEA79S\nSp1iNPArpdQpRgO/UkqdYmw5uSsi9cC2Ti+0lyKg+kQPIkU65uOnL45bx3x89NSYhxljOu+Gh33b\nMm9LdnbaLkRktY659/XFMUPfHLeO+fg4EWPWUo9SSp1iNPArpdQpxq6B/7ETPYAu0DEfH31xzNA3\nx61jPj6O+5htObmrlFKq99g141dKKdVLbBX4RWSGiGwTkTIRSeEOkseXiDwpIpUisjHqWH8ReU9E\ndljf+53IMUYTkSEislhENovIJhH5oXXctmMGEJEMEVkpIp9Z477HOm7rcUPolqUiss66V4Xtxywi\ne0Rkg4h8KiKrrWN2H3OBiLwiIltFZIuIXNAHxjzW+jsOf9WJyI+O97htE/it+/M+BMwExgFzRWTc\niR1VQk8BM2KO3Q0sMsaMARZZj+3CD/yLMWYcMA240/q7tfOYATzA5caYc4EJwAyr7bfdxw3wQ2BL\n1OO+MObLjDETopYW2n3M9wHvGmPOAM4l9Pdt6zEbY7ZZf8cTgEmEuhm/zvEetzHGFl/ABcDCqMc/\nBX56osfVwXiHAxujHm8DBls/Dya0F+GEjzPB2N8ErupjY84C1gLn233chG44tAi4HHi7L/z7APYA\nRTHHbDtmIB/YjTVP2RfGHOfPcDWw7ESM2zYZP1AC7I96XG4d6ysGmtDNZwAOAQNP5GASEZHhwHnA\nCvrAmK2SyadAJfCeMaYvjPtPwE+AYNQxu4/ZAO+LyBrr/tdg7zGPAKqAv1oltSdEJBt7jznWHOAF\n6+fjOm47Bf6Thgn92rbdcikRyQFeBX5kjKmLPmfXMRtjAib0sbgUmCoiZ8Wct9W4ReQLQKUxZk2i\na+w2ZstF1t/zTEKlwEuiT9pwzGnAROARY8x5QCMx5REbjjnCuiPhtcDLseeOx7jtFPiTubevnR0W\nkcEA1vfKEzyeNkTERSjoP2eMec06bOsxRzPG1ACLCc2t2Hnc04FrRWQPMA+4XESexd5jxhhTYX2v\nJFRznoq9x1wOlFufAAFeIfSLwM5jjjYTWGuMOWw9Pq7jtlPg7+v3550PfNv6+duE6ui2ICIC/AXY\nYoz5n6hTth0zgIgUi0iB9XMmoXmJrdh43MaYnxpjSo0xwwn9G/7AGHM9Nh6ziGSLSG74Z0K1543Y\neMzGmEPAfhEZax26AtiMjcccYy6tZR443uM+0RMcMZMds4DtwE7gZyd6PB2M8wXgIOAjlHn8E1BI\naEJvB/A+0P9EjzNqvBcR+ui4HvjU+ppl5zFb4z4HWGeNeyPwS+u4rccdNf5LaZ3cte2YgZHAZ9bX\npvD/e3YeszW+CcBq69/HG0A/u4/ZGnc2cATIjzp2XMetO3eVUuoUY6dSj1JKqeNAA79SSp1iNPAr\npdQpRgO/UkqdYjTwK6XUKUYDv1JKnWI08Cul1ClGA79SSp1i/j+NdadGTFYnVgAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d49c5c4dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/Mz/B9u9smT4RSg8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d49cd16a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_df(player):\n",
    "    df = grouped_df[grouped_df['player_name'] == player]\n",
    "    \n",
    "    df.sort_values(['date_started', 'round_num', 'hole_num'], inplace = True)\n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    df['cum_sum'] = df['stroke_gained'].cumsum()\n",
    "    \n",
    "    num_cols = ['cum_sum', 'days_elapsed']#'tournament_id', 'course_num', 'round_num', 'hole_num',\n",
    "             #'par', 'hole_dist_yds', 'days_elapsed']\n",
    "\n",
    "    input_df = df[num_cols]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range = (0,1))\n",
    "    input_df = pd.DataFrame(scaler.fit_transform(input_df), columns = num_cols)\n",
    "    \n",
    "    final_df = pd.concat([df['cum_sum'], input_df], axis = 1)\n",
    "    \n",
    "    return input_df\n",
    "\n",
    "def run_rnn(df):\n",
    "    data = df.values\n",
    "\n",
    "    examples = 72 # how far to look back\n",
    "    y_examples = 72 # how far to predict in the future\n",
    "    nb_samples = len(data) - examples - y_examples\n",
    "\n",
    "    input_list = [np.expand_dims(np.atleast_2d(data[i:examples+i,:]), axis=0) for i in range(nb_samples)]\n",
    "    input_mat = np.concatenate(input_list, axis=0)\n",
    "\n",
    "    # use the tail of the series as the test data\n",
    "    df_test = pd.DataFrame(df[-examples:])\n",
    "\n",
    "    test_data = df_test.values\n",
    "    test_input_list = [np.expand_dims(np.atleast_2d(test_data[len(test_data)-examples:len(test_data),:]), axis=0) for i in range(1)]\n",
    "    test_input_mat = np.concatenate(test_input_list, axis=0)\n",
    "\n",
    "    # target - the first column in df dataframe\n",
    "    target_list = [np.atleast_2d(data[i+examples:examples+i+y_examples,0]) for i in range(nb_samples)]\n",
    "    target_mat = np.concatenate(target_list, axis=0)\n",
    "\n",
    "    #Set up model\n",
    "    features = input_mat.shape[2]\n",
    "    hidden = 128\n",
    "    epochs = 75\n",
    "    batch_size = 100\n",
    "    validation_split = 0.05\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden, input_shape = (examples, features), return_sequences = True))\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(LSTM(hidden, return_sequences = False))\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Dense(y_examples))\n",
    "    model.add(Activation('linear'))\n",
    "    model.compile(loss = 'mse', optimizer = 'rmsprop')\n",
    "\n",
    "    #Train\n",
    "    hist = model.fit(input_mat, target_mat, epochs = epochs, batch_size = batch_size, \n",
    "                     validation_split = validation_split, verbose = 1)\n",
    "\n",
    "    predicted = model.predict(test_input_mat)\n",
    "\n",
    "    df_val_loss = pd.DataFrame(hist.history['val_loss'])\n",
    "    df_val_loss.plot()\n",
    "\n",
    "    df_predicted = pd.DataFrame(predicted).T\n",
    "    df_predicted.columns = ['Predicted 1']\n",
    "\n",
    "    df_result = pd.concat([df['cum_sum'][-200:], df_predicted], ignore_index = True)\n",
    "    df_result.plot()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "for player in player_ls[:1]:\n",
    "    df = create_df(player)\n",
    "    df_result = run_rnn(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3258 samples, validate on 576 samples\n",
      "Epoch 1/8\n",
      " 750/3258 [=====>........................] - ETA: 53s - loss: 0.1665"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-17822f851198>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m#Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m hist = model.fit(input_mat, target_mat, epochs = epochs, batch_size = batch_size, \n\u001b[1;32m---> 41\u001b[1;33m                  validation_split = validation_split, verbose = 1)\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_input_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    854\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 856\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1497\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1498\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2229\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2230\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "player = player_ls[2]\n",
    "\n",
    "df = create_df(player)\n",
    "data = df.values\n",
    "\n",
    "examples = 360 # how far to look back\n",
    "y_examples = 72 # how far to predict in the future\n",
    "nb_samples = len(data) - examples - y_examples\n",
    "\n",
    "input_list = [np.expand_dims(np.atleast_2d(data[i:examples+i,:]), axis=0) for i in range(nb_samples)]\n",
    "input_mat = np.concatenate(input_list, axis=0)\n",
    "\n",
    "# use the tail of the series as the test data\n",
    "df_test = pd.DataFrame(df[-examples:])\n",
    "\n",
    "test_data = df_test.values\n",
    "test_input_list = [np.expand_dims(np.atleast_2d(test_data[len(test_data)-examples:len(test_data),:]), axis=0) for i in range(1)]\n",
    "test_input_mat = np.concatenate(test_input_list, axis=0)\n",
    "\n",
    "# target - the first column in df dataframe\n",
    "target_list = [np.atleast_2d(data[i+examples:examples+i+y_examples,0]) for i in range(nb_samples)]\n",
    "target_mat = np.concatenate(target_list, axis=0)\n",
    "\n",
    "#Set up model\n",
    "features = input_mat.shape[2]\n",
    "hidden = 128\n",
    "epochs = 8\n",
    "batch_size = 250\n",
    "validation_split = 0.15\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden, input_shape = (examples, features)))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(y_examples))\n",
    "model.add(Activation('linear'))\n",
    "model.compile(loss = 'mse', optimizer = 'rmsprop')\n",
    "\n",
    "\n",
    "#Train\n",
    "hist = model.fit(input_mat, target_mat, epochs = epochs, batch_size = batch_size, \n",
    "                 validation_split = validation_split, verbose = 1)\n",
    "\n",
    "predicted = model.predict(test_input_mat)\n",
    "\n",
    "df_val_loss = pd.DataFrame(hist.history['val_loss'])\n",
    "df_val_loss.plot()\n",
    "\n",
    "df_predicted = pd.DataFrame(predicted).T\n",
    "df_predicted.columns = ['Predicted 1']\n",
    "\n",
    "df_result = pd.concat([df['cum_sum'][-200:], df_predicted], ignore_index = True)\n",
    "df_result.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cum_sum</th>\n",
       "      <th>days_elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.214019</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.209980</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.210446</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.203766</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.204427</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.207300</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.207961</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.208155</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.203805</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.207766</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.204543</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.206135</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.200155</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.200465</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.201553</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.206097</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.198640</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.197630</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.197591</td>\n",
       "      <td>0.994396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.201553</td>\n",
       "      <td>0.994396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.196659</td>\n",
       "      <td>0.994396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.196853</td>\n",
       "      <td>0.994396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.189746</td>\n",
       "      <td>0.994396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.178406</td>\n",
       "      <td>0.994396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.182950</td>\n",
       "      <td>0.994396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.180154</td>\n",
       "      <td>0.994396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.180814</td>\n",
       "      <td>0.994396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.180775</td>\n",
       "      <td>0.994396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.177202</td>\n",
       "      <td>0.994396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.177668</td>\n",
       "      <td>0.994396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>0.991883</td>\n",
       "      <td>0.000801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4237</th>\n",
       "      <td>0.992349</td>\n",
       "      <td>0.000801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4238</th>\n",
       "      <td>0.996194</td>\n",
       "      <td>0.000801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4239</th>\n",
       "      <td>0.996854</td>\n",
       "      <td>0.000801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4240</th>\n",
       "      <td>0.992932</td>\n",
       "      <td>0.000801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4241</th>\n",
       "      <td>0.993009</td>\n",
       "      <td>0.000801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4242</th>\n",
       "      <td>0.991223</td>\n",
       "      <td>0.000801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4243</th>\n",
       "      <td>0.995184</td>\n",
       "      <td>0.000801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4244</th>\n",
       "      <td>0.990174</td>\n",
       "      <td>0.000801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4245</th>\n",
       "      <td>0.986485</td>\n",
       "      <td>0.000801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4246</th>\n",
       "      <td>0.986563</td>\n",
       "      <td>0.000801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4247</th>\n",
       "      <td>0.986873</td>\n",
       "      <td>0.000801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>0.991068</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>0.990524</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>0.990835</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>0.991922</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252</th>\n",
       "      <td>0.992582</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4253</th>\n",
       "      <td>0.991767</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4254</th>\n",
       "      <td>0.988194</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4255</th>\n",
       "      <td>0.992543</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256</th>\n",
       "      <td>0.992505</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4257</th>\n",
       "      <td>0.989281</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4258</th>\n",
       "      <td>0.993126</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4259</th>\n",
       "      <td>0.993204</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4260</th>\n",
       "      <td>0.995301</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4261</th>\n",
       "      <td>0.995378</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4262</th>\n",
       "      <td>0.994252</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4263</th>\n",
       "      <td>0.994446</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4264</th>\n",
       "      <td>0.994524</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4265</th>\n",
       "      <td>0.994835</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4266 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cum_sum  days_elapsed\n",
       "0     0.214019      1.000000\n",
       "1     0.209980      1.000000\n",
       "2     0.210446      1.000000\n",
       "3     0.203766      1.000000\n",
       "4     0.204427      1.000000\n",
       "5     0.207300      1.000000\n",
       "6     0.207961      1.000000\n",
       "7     0.208155      1.000000\n",
       "8     0.203805      1.000000\n",
       "9     0.207766      1.000000\n",
       "10    0.204543      1.000000\n",
       "11    0.206135      1.000000\n",
       "12    0.200155      1.000000\n",
       "13    0.200465      1.000000\n",
       "14    0.201553      1.000000\n",
       "15    0.206097      1.000000\n",
       "16    0.198640      1.000000\n",
       "17    0.197630      1.000000\n",
       "18    0.197591      0.994396\n",
       "19    0.201553      0.994396\n",
       "20    0.196659      0.994396\n",
       "21    0.196853      0.994396\n",
       "22    0.189746      0.994396\n",
       "23    0.178406      0.994396\n",
       "24    0.182950      0.994396\n",
       "25    0.180154      0.994396\n",
       "26    0.180814      0.994396\n",
       "27    0.180775      0.994396\n",
       "28    0.177202      0.994396\n",
       "29    0.177668      0.994396\n",
       "...        ...           ...\n",
       "4236  0.991883      0.000801\n",
       "4237  0.992349      0.000801\n",
       "4238  0.996194      0.000801\n",
       "4239  0.996854      0.000801\n",
       "4240  0.992932      0.000801\n",
       "4241  0.993009      0.000801\n",
       "4242  0.991223      0.000801\n",
       "4243  0.995184      0.000801\n",
       "4244  0.990174      0.000801\n",
       "4245  0.986485      0.000801\n",
       "4246  0.986563      0.000801\n",
       "4247  0.986873      0.000801\n",
       "4248  0.991068      0.000000\n",
       "4249  0.990524      0.000000\n",
       "4250  0.990835      0.000000\n",
       "4251  0.991922      0.000000\n",
       "4252  0.992582      0.000000\n",
       "4253  0.991767      0.000000\n",
       "4254  0.988194      0.000000\n",
       "4255  0.992543      0.000000\n",
       "4256  0.992505      0.000000\n",
       "4257  0.989281      0.000000\n",
       "4258  0.993126      0.000000\n",
       "4259  0.993204      0.000000\n",
       "4260  0.995301      0.000000\n",
       "4261  0.995378      0.000000\n",
       "4262  0.994252      0.000000\n",
       "4263  0.994446      0.000000\n",
       "4264  0.994524      0.000000\n",
       "4265  0.994835      0.000000\n",
       "\n",
       "[4266 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1, 288, 8), (1, 288, 8), (3654, 1, 72), (3654, 72))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(test_input_list).shape, np.asarray(test_input_mat).shape, np.asarray(target_list).shape, np.asarray(target_mat).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858 1408\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "dataset = df['cum_sum'].values\n",
    "dataset = dataset.astype('float32').reshape(-1,1)\n",
    "\n",
    "mm_scaler =  MinMaxScaler(feature_range = (0,1))\n",
    "dataset = mm_scaler.fit_transform(dataset)\n",
    "\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train, test = dataset[:train_size, :], dataset[train_size: len(dataset), :]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "\n",
    "def create_dataset(dataset, look_back = 1):\n",
    "    dataX, dataY = [], []\n",
    "    \n",
    "    for i in range(len(dataset) - look_back - 1):\n",
    "        a = dataset[i: (i + look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2856/2856 [==============================] - 1s - loss: 0.0900\n",
      "Epoch 2/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0894\n",
      "Epoch 3/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0886\n",
      "Epoch 4/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0879\n",
      "Epoch 5/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0871\n",
      "Epoch 6/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0862\n",
      "Epoch 7/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0854\n",
      "Epoch 8/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0846\n",
      "Epoch 9/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0838\n",
      "Epoch 10/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0830\n",
      "Epoch 11/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0822\n",
      "Epoch 12/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0813\n",
      "Epoch 13/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0805\n",
      "Epoch 14/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0797\n",
      "Epoch 15/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0789\n",
      "Epoch 16/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0781\n",
      "Epoch 17/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0773\n",
      "Epoch 18/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0765\n",
      "Epoch 19/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0758\n",
      "Epoch 20/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0750\n",
      "Epoch 21/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0742\n",
      "Epoch 22/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0734\n",
      "Epoch 23/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0727\n",
      "Epoch 24/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0719\n",
      "Epoch 25/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0712\n",
      "Epoch 26/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0704\n",
      "Epoch 27/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0697\n",
      "Epoch 28/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0690\n",
      "Epoch 29/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0683\n",
      "Epoch 30/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0676\n",
      "Epoch 31/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0668\n",
      "Epoch 32/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0661\n",
      "Epoch 33/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0654\n",
      "Epoch 34/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0648\n",
      "Epoch 35/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0641\n",
      "Epoch 36/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0634\n",
      "Epoch 37/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0627\n",
      "Epoch 38/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0621\n",
      "Epoch 39/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0614\n",
      "Epoch 40/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0608\n",
      "Epoch 41/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0601\n",
      "Epoch 42/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0595\n",
      "Epoch 43/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0589\n",
      "Epoch 44/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0582\n",
      "Epoch 45/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0576\n",
      "Epoch 46/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0570\n",
      "Epoch 47/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0564\n",
      "Epoch 48/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0558\n",
      "Epoch 49/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0552\n",
      "Epoch 50/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0546\n",
      "Epoch 51/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0541\n",
      "Epoch 52/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0535\n",
      "Epoch 53/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0529\n",
      "Epoch 54/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0524\n",
      "Epoch 55/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0518\n",
      "Epoch 56/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0513\n",
      "Epoch 57/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0507\n",
      "Epoch 58/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0502\n",
      "Epoch 59/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0497\n",
      "Epoch 60/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0492\n",
      "Epoch 61/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0487\n",
      "Epoch 62/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0481\n",
      "Epoch 63/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0476\n",
      "Epoch 64/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0471\n",
      "Epoch 65/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0467\n",
      "Epoch 66/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0462\n",
      "Epoch 67/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0457\n",
      "Epoch 68/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0452\n",
      "Epoch 69/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0448\n",
      "Epoch 70/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0443\n",
      "Epoch 71/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0439\n",
      "Epoch 72/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0434\n",
      "Epoch 73/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0430\n",
      "Epoch 74/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0425\n",
      "Epoch 75/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0421\n",
      "Epoch 76/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0417\n",
      "Epoch 77/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0413\n",
      "Epoch 78/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0408\n",
      "Epoch 79/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0404\n",
      "Epoch 80/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0400\n",
      "Epoch 81/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0396\n",
      "Epoch 82/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0393\n",
      "Epoch 83/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0389\n",
      "Epoch 84/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0385\n",
      "Epoch 85/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0381\n",
      "Epoch 86/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0377\n",
      "Epoch 87/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0374\n",
      "Epoch 88/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0370\n",
      "Epoch 89/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0367\n",
      "Epoch 90/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0363\n",
      "Epoch 91/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0360\n",
      "Epoch 92/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0356\n",
      "Epoch 93/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0353\n",
      "Epoch 94/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0350\n",
      "Epoch 95/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0347\n",
      "Epoch 96/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0344\n",
      "Epoch 97/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0340\n",
      "Epoch 98/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0337\n",
      "Epoch 99/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0334\n",
      "Epoch 100/100\n",
      "2856/2856 [==============================] - 0s - loss: 0.0331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d4591beb00>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=10000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.18 RMSE\n",
      "Test Score: 0.57 RMSE\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = mm_scaler.inverse_transform(trainPredict)\n",
    "trainY = mm_scaler.inverse_transform(trainY)\n",
    "testPredict = mm_scaler.inverse_transform(testPredict)\n",
    "testY = mm_scaler.inverse_transform(testY)\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2569/2569 [==============================] - 2s - loss: 0.0327\n",
      "Epoch 2/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0170\n",
      "Epoch 3/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0058\n",
      "Epoch 4/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0012\n",
      "Epoch 5/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0024\n",
      "Epoch 6/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0061\n",
      "Epoch 7/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0086\n",
      "Epoch 8/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0086\n",
      "Epoch 9/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0069\n",
      "Epoch 10/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0047\n",
      "Epoch 11/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0028\n",
      "Epoch 12/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0017\n",
      "Epoch 13/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0014\n",
      "Epoch 14/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0017\n",
      "Epoch 15/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0023\n",
      "Epoch 16/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0029\n",
      "Epoch 17/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0033\n",
      "Epoch 18/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0033\n",
      "Epoch 19/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0030\n",
      "Epoch 20/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0025\n",
      "Epoch 21/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0019\n",
      "Epoch 22/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0014\n",
      "Epoch 23/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0010\n",
      "Epoch 24/100\n",
      "2569/2569 [==============================] - 0s - loss: 9.0308e-04\n",
      "Epoch 25/100\n",
      "2569/2569 [==============================] - 0s - loss: 9.6109e-04\n",
      "Epoch 26/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0011\n",
      "Epoch 27/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0013\n",
      "Epoch 28/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0015\n",
      "Epoch 29/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0015\n",
      "Epoch 30/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0013\n",
      "Epoch 31/100\n",
      "2569/2569 [==============================] - 0s - loss: 0.0011\n",
      "Epoch 32/100\n",
      "2569/2569 [==============================] - 0s - loss: 8.8321e-04\n",
      "Epoch 33/100\n",
      "2569/2569 [==============================] - 0s - loss: 7.1380e-04\n",
      "Epoch 34/100\n",
      "2569/2569 [==============================] - 0s - loss: 6.3857e-04\n",
      "Epoch 35/100\n",
      "2569/2569 [==============================] - 0s - loss: 6.5754e-04\n",
      "Epoch 36/100\n",
      "2569/2569 [==============================] - 0s - loss: 7.3790e-04\n",
      "Epoch 37/100\n",
      "2569/2569 [==============================] - 0s - loss: 8.3019e-04\n",
      "Epoch 38/100\n",
      "2569/2569 [==============================] - 0s - loss: 8.8885e-04\n",
      "Epoch 39/100\n",
      "2569/2569 [==============================] - 0s - loss: 8.8897e-04\n",
      "Epoch 40/100\n",
      "2569/2569 [==============================] - 0s - loss: 8.3291e-04\n",
      "Epoch 41/100\n",
      "2569/2569 [==============================] - 0s - loss: 7.4559e-04\n",
      "Epoch 42/100\n",
      "2569/2569 [==============================] - 0s - loss: 6.6141e-04\n",
      "Epoch 43/100\n",
      "2569/2569 [==============================] - 0s - loss: 6.0904e-04\n",
      "Epoch 44/100\n",
      "2569/2569 [==============================] - 0s - loss: 6.0037e-04\n",
      "Epoch 45/100\n",
      "2569/2569 [==============================] - 0s - loss: 6.2766e-04\n",
      "Epoch 46/100\n",
      "2569/2569 [==============================] - 0s - loss: 6.6942e-04\n",
      "Epoch 47/100\n",
      "2569/2569 [==============================] - 0s - loss: 7.0149e-04\n",
      "Epoch 48/100\n",
      "2569/2569 [==============================] - 0s - loss: 7.0792e-04\n",
      "Epoch 49/100\n",
      "2569/2569 [==============================] - 0s - loss: 6.8677e-04\n",
      "Epoch 50/100\n",
      "2569/2569 [==============================] - 0s - loss: 6.4886e-04\n",
      "Epoch 51/100\n",
      "2569/2569 [==============================] - 0s - loss: 6.1088e-04\n",
      "Epoch 52/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.8695e-04\n",
      "Epoch 53/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.8258e-04\n",
      "Epoch 54/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.9348e-04\n",
      "Epoch 55/100\n",
      "2569/2569 [==============================] - 0s - loss: 6.0908e-04\n",
      "Epoch 56/100\n",
      "2569/2569 [==============================] - 0s - loss: 6.1847e-04\n",
      "Epoch 57/100\n",
      "2569/2569 [==============================] - 0s - loss: 6.1557e-04\n",
      "Epoch 58/100\n",
      "2569/2569 [==============================] - 0s - loss: 6.0125e-04\n",
      "Epoch 59/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.8176e-04\n",
      "Epoch 60/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.6491e-04\n",
      "Epoch 61/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.5596e-04\n",
      "Epoch 62/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.5557e-04\n",
      "Epoch 63/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.6015e-04\n",
      "Epoch 64/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.6438e-04\n",
      "Epoch 65/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.6412e-04\n",
      "Epoch 66/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.5826e-04\n",
      "Epoch 67/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.4882e-04\n",
      "Epoch 68/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.3936e-04\n",
      "Epoch 69/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.3291e-04\n",
      "Epoch 70/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.3053e-04\n",
      "Epoch 71/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.3102e-04\n",
      "Epoch 72/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.3198e-04\n",
      "Epoch 73/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.3121e-04\n",
      "Epoch 74/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.2784e-04\n",
      "Epoch 75/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.2258e-04\n",
      "Epoch 76/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.1704e-04\n",
      "Epoch 77/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.1276e-04\n",
      "Epoch 78/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.1034e-04\n",
      "Epoch 79/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.0928e-04\n",
      "Epoch 80/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.0844e-04\n",
      "Epoch 81/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.0677e-04\n",
      "Epoch 82/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.0386e-04\n",
      "Epoch 83/100\n",
      "2569/2569 [==============================] - 0s - loss: 5.0010e-04\n",
      "Epoch 84/100\n",
      "2569/2569 [==============================] - 0s - loss: 4.9628e-04\n",
      "Epoch 85/100\n",
      "2569/2569 [==============================] - 0s - loss: 4.9312e-04\n",
      "Epoch 86/100\n",
      "2569/2569 [==============================] - 0s - loss: 4.9085e-04\n",
      "Epoch 87/100\n",
      "2569/2569 [==============================] - 0s - loss: 4.8915e-04\n",
      "Epoch 88/100\n",
      "2569/2569 [==============================] - 0s - loss: 4.8744e-04\n",
      "Epoch 89/100\n",
      "2569/2569 [==============================] - 0s - loss: 4.8524e-04\n",
      "Epoch 90/100\n",
      "2569/2569 [==============================] - 0s - loss: 4.8248e-04\n",
      "Epoch 91/100\n",
      "2569/2569 [==============================] - 0s - loss: 4.7943e-04\n",
      "Epoch 92/100\n",
      "2569/2569 [==============================] - 0s - loss: 4.7650e-04\n",
      "Epoch 93/100\n",
      "2569/2569 [==============================] - 0s - loss: 4.7397e-04\n",
      "Epoch 94/100\n",
      "2569/2569 [==============================] - 0s - loss: 4.7185e-04\n",
      "Epoch 95/100\n",
      "2569/2569 [==============================] - 0s - loss: 4.6991e-04\n",
      "Epoch 96/100\n",
      "2569/2569 [==============================] - 0s - loss: 4.6785e-04\n",
      "Epoch 97/100\n",
      "2569/2569 [==============================] - 0s - loss: 4.6552e-04\n",
      "Epoch 98/100\n",
      "2569/2569 [==============================] - 0s - loss: 4.6298e-04\n",
      "Epoch 99/100\n",
      "2569/2569 [==============================] - 0s - loss: 4.6042e-04\n",
      "Epoch 100/100\n",
      "2569/2569 [==============================] - 0s - loss: 4.5802e-04\n",
      "Train Score: 0.02 RMSE\n",
      "Test Score: 0.17 RMSE\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd8FMX7wPHPpCeUhN5CCCBdpIWOVEGKCIhfRVBsXxGV\nL3ZFUBQURbGgP1BEQcSGDQQF6V1AOoRQAwQSIBBKQkhPbn5/7HG5SyEHXHLJ3fN+vXgxOzu3+9yK\nTyazuzNKa40QQgjX4uHsAIQQQjieJHchhHBBktyFEMIFSXIXQggXJMldCCFckCR3IYRwQZLchRDC\nBUlyF0IIFyTJXQghXJCXs05csWJFHRoa6qzTCyFEibRjx47zWutKBbVzWnIPDQ1l+/btzjq9EEKU\nSEqpE/a0k2EZIYRwQZLchRDCBUlyF0IIFyTJXQghXJAkdyGEcEEFJnel1Gyl1Dml1L589iul1GdK\nqUil1F6lVEvHhymEEOJ62NNznwP0vsb+PkA9858RwBc3H5YQQoibUWBy11qvBy5eo8kAYK42bAGC\nlFLVHBWgEEKUdDGXkll14GyRntMRLzHVAKKttmPMdWdyNlRKjcDo3RMSEuKAUwshRPGTkp7FxL8i\n+GlrNItHd6LfZxsBqFrWjwqlfRjXtxEdbqlYqDEU6RuqWuuZwEyAsLAwWZlbCOGSGo1failfTewA\nsZdTib2cyoJdpwo9uTviaZlTQE2r7WBznRBCuJ0TF5IKbPPBvbcVehyOSO6LgOHmp2baAQla61xD\nMkII4Q5OXEjOVfdm/8ZseKUbdzapwsoXOqOUKvQ4ChyWUUr9BHQFKiqlYoA3AW8ArfUMYAnQF4gE\nkoFHCytYIYQo7rYcuwDAu4Oa0jq0HPWqlLHs+/KhsCKLo8DkrrV+oID9GnjGYREJIUQJ9vnaowDc\n07IGft6eTovDaVP+CiGEKwmPScDL0xhu6XRLRacmdpDkLoQQDtF/WvZTMV0bFLiWRqGTuWWEEOIm\nJaRk2Gz3alzVSZFkk+QuhBD5yMwyEZeYRnJ65jXbNZuw3Ga7clnfwgzLLjIsI4QQ+Wg+cQVX0ozE\nPqB5depVLs2o7vVs2lxKSs/1OWePt4MkdyGEyNfVxA6wcPdpAB7rVJsAHyN1Rp1PouuHa50RWoEk\nuQshRA4xl5KZuznvdahjLqVQv0oZMrJMNol9/cvdCKkQUEQRFkzG3IUQIodO769h5vpjlu2XetW3\nlHt9sp4sk+axOdtsPlOcEjtIchdCCP7YdYrRP+3itx0xmEy2cxpuG3cHI7vUtanbHR3PhiPnLdv3\ntKxRJHFeDxmWEUK4ved+3g3Aoj2nuXAlzWZfpTLGky/H3+tLn083cDA2kcFfbLLsbxkSxMf3NS+6\nYO0kPXchhFs7fDbRZvu9vw9ayr+ObG8pK6V4956mNm2DAryZ/3THwg3wBklyF0K4tZN5zOII8NF/\nmtE6tLxNXX2rScAAhrQuvosOybCMEMJtaa1ZEm7MUL51XA/aTFoFwMPtazG4VXCu9qV9vYia3A+A\nf49dICxH8i9OpOcuhHBLGVkm2ry7ivm7jLWFKpbyZXj7WoDxLHtB2tapgKdH4c/LfqOk5y6EcBvJ\n6ZnEJqTioRSLw88Ql5h989TDQzHh7iaM6n4Llcv4OTFKx5DkLoRwC2mZWTQevyzPfS/f2QAwbpq6\nQmIHGZYRQriJSYsP5Lvv6uOOrkSSuxDCLWyPupTvvvZ1KhRhJEVDhmWEEC6vzaSVnEtMo1yAN5eS\njbnX3x3UlO4NK5OYmkHN8sVr6gBHkOQuhHBJWSZN3bFLbOpah5Zn+f6zAAxtazyjXjXQNcbYc5Lk\nLoRwSd9uispVN7RtCBMH3Eq5Ut5FH1ARk+QuhHBJGVkmS/m24EA+H9aS4HKuN/ySH0nuQgiXk5ia\nYZkj5vA7ffDxcr9nRyS5CyFcykfLD/F/qyMB8PXycMvEDpLchRAu5Ll5u/jDvBwewNi+jZwYjXO5\n5480IYRLsk7sACEu+IijvaTnLoRwCZdTMyzlVS924edt0XRtUMmJETmXJHchhEuYvfE4ABVL+1K3\nUmm3HpIBGZYRQriIvTEJAGx//Q4nR1I8SM9dCFEi/bI9mld+2+vsMIotu3ruSqneSqlDSqlIpdSY\nPPYHKqX+VErtUUpFKKUedXyoQggBJpPms1VHJLEXoMCeu1LKE5gO9ARigG1KqUVa6/1WzZ4B9mut\n+yulKgGHlFI/aK3TCyVqIYTb6vT+ak4npOa578ikPkUcTfFlz7BMGyBSa30MQCk1DxgAWCd3DZRR\nSimgNHARyHRwrEIINzdl2UGbxD7zoVb0bFyFYV//S5PqZfH2lNuIV9mT3GsA0VbbMUDbHG2mAYuA\n00AZ4H6ttQkhhHCQpLRMpq85atke1KIGvZpUBeDHJ9o5K6xiy1E/5u4EdgPVgebANKVU2ZyNlFIj\nlFLblVLb4+LiHHRqIYSru5iUTpM3s5fIe3dQUz65v7kTIyr+7Om5nwJqWm0Hm+usPQpM1lprIFIp\ndRxoCGy1bqS1ngnMBAgLC9M3GrQQwj1cuJJGq3dW2tQ92aWOZS52kT97kvs2oJ5SqjZGUh8CDM3R\n5iTQA9iglKoCNACOOTJQIYR7eXPhPr7dfMKmbur9zRnYooaTIipZCkzuWutMpdQoYBngCczWWkco\npUaa988A3gbmKKXCAQW8qrU+X4hxCyFc2NnLqbkS+843elK+lI+TIip57HqJSWu9BFiSo26GVfk0\n0MuxoQkh3NWh2ESb7dAKAZLYr5O8oSqEKDaS0zOZvfE4M9Zlj+re2aQKU+9v4cSoSiZJ7kKIYmPO\npig+XH7Ypq5Hwyr4+3g6KaKSS5K7EKLY2HnikqX8/uCm3BYcRKNquZ6qFnaQ5C6EKDb2nbpsKd/T\nMljeOL0JktyFEE4XHpNA/2kbLdu316soif0mydUTQjjdjPXZ0wq80LM+3z2ec4aTki8qIYonVzxJ\n02+b8uOBHwv9fNJzF0I4VVJaJov3nrFs92pSxYnRFI7wuHCGLsl+97NG6cJ/EUuSuxDCqf7ck72o\ndZPqZalbqbQTo7kxyRnJBHjnvRj3+1vf5/sD31u2hzUaRpeaXQo9JknuQginyjAZ00zVCPJn8ejb\nnRxN3kzaxOJji2lZpSW7z+3m9yO/M7HDRKqXrk6zuc1ytb8l6BYCvALYez57QZFnmj/DyGYjiyxm\nSe5CCKdpM2kl5xLTAPjrf52cHE3eMk2ZfLzjY77b/51NfZ/5+S8MEhkfabM9vcd0Ogd3LpT48iPJ\nXQjhFN9uirIkdoByxWh6ge2x2/np4E80rdiU+LR4m8Re2b8ydYPqsvnMZkvdR10+4nzKeQK8A5i7\nfy5HLh0BoGnFpky+fTIhZYt+FktJ7kKIIrcnOp43F0VYtn9/qr0To4EsUxYvr3+ZFSdW2NQvP7Hc\nUv6uz3c0r5w9h/yV9CusiV5Dn9p98PLITqUDbxloOaanh/PerJXkLoQocgOm/wPAqG630KdpVZpU\nD3RKHGlZaaw+uZplUctYdXJVrv2dgzuTpbN4q/1bVC1V1WZfaZ/S9K/bP99jOzOxgyR3IUQRyzJl\nr9MzvEMtKpfxc0oc9/15HwcuHrCpm3HHDJpVasayqGXcU+8ejGWhSyZJ7kKIIrUnJt5SdlZi/3LP\nl7kS+9w+c2lR2Zh9cnD9wc4Iy6EkuQshitQ9n28C4JtHWhf5uX8//DtvbX7Lsr188HICfQM5ePGg\nJbG7CknuQogi88HSg5Zy5/qViuScyRnJrDq5irEbx9rU/zHgD6qVrgZAyyotiySWoiTJXQhRZD5f\na8whM31oSzw9Cm88W2vN6pOreW7tc7n2LRu8jOqlqxfauYsLSe5CiCLx87aTlnK/26oVyjmupF9h\nW+w2Rq8ZnWvfwFsGMrHDxBJ9k/R6SHIXQhSJV38PB2BYW8e90KO15p/T/zB562ROJZ4iU2fa7H+n\n4zsMuGWAw85XkkhyF0IUutSMLEt50qCmDjnmgiMLGL9pfJ77Xm39Kg82ftAh5ympJLkLIQpdi4nG\nm59fDQ+76WPNOziPSf9OsqlrU7UNd9W5i9uDb6eif8WbPocrkOQuhChUSWmZpJh77m3rlL+hY2it\neW/re/wR+QcpmSmW+mndp9GxRkeb1/+FQa6IEKJQbY26CMAz3epS1s/7uj8flxzHsCXDOJOUvaDH\nb/1/o0H5Bg6L0RXJMntCiEL1478nKeXjyahu9a77s+ui19H91+42if3Lnl9KYreD9NyFEIVm/eE4\nVuw/y93NquPvc30Tab287mWWRi0FYGzbsfSv05/SPiVvlSZnkeQuhLgpp+NTKOXrhY+nB8npmVQo\n7WvZN37hPgAaVy97XceMuBBhSezPNH+GBxo+4LiA3YQkdyHEDVt3OI6HZ28FIKR8ACcvJvPE7bX5\nasNxlj53O1UD/Yi6kMwTt9ex+5h74/YybMkwABYNXETtwNqFErurk+QuhLhhn648bCmfvJgMwFcb\njgPQe+oGAB5uX8vuqQbOJp21JPbRLUZLYr8JktyFEDfs/JX0Atv0aWrfVANaax5d9igAc3rPoVWV\nVjcVm7uTp2WEEDdkxrqjnLyYTJva+T+7/ljH2rSrU8Gu441eM5roxGi6BHeRxO4AdvXclVK9gU8B\nT+BrrfXkPNp0BaYC3sB5rXUXB8YphChmJv9tTN/70X+aMWdTFKV9vXiicx1OXEiieqA/209c4o5G\nle061rrodayNXgvA1G5TCytkt1JgcldKeQLTgZ5ADLBNKbVIa73fqk0Q8DnQW2t9Uill339RIUSJ\npHX2Unk1ywfwxl2NLdtX10Pt2biKXcfKyMpg1OpRAMzqNUveNnUQe4Zl2gCRWutjWut0YB6Qc5q1\nocB8rfVJAK31OceGKYQoTnZHxxfcyA6J6Ym0/N5YKOPlsJdpU62NQ44r7EvuNYBoq+0Yc521+kA5\npdRapdQOpdTwvA6klBqhlNqulNoeFxd3YxELIZwu/FQCAJPvubEZHlMyU3h/6/t0+KkDAC0rt2RY\no2EOi0847mkZL6AV0APwBzYrpbZorQ9bN9JazwRmAoSFhelcRxFClAg/bzP6e3c3v74VjZIzkskw\nZdB/QX8upV0C4LU2rzG00VCHx+ju7Enup4CaVtvB5jprMcAFrXUSkKSUWg80Aw4jhHApmVkmIk5f\nBiDAx77+4dH4owxcONCmrm3VtnzY5UOC/IIcHqOwL7lvA+oppWpjJPUhGGPs1hYC05RSXoAP0Bb4\nxJGBCiEKT5ZJk2ky4etV8Pwv7d5bDUDfplULbBufGs+AhQO4mHox176v7/z6+gMVdiswuWutM5VS\no4BlGI9CztZaRyilRpr3z9BaH1BKLQX2AiaMxyX3FWbgQgjHqTt2CQAH3+6Nn3f+CX7T0fOcv5IG\nwHuDbrvmMU9ePkm/Bf0s26+1eY0HGj7gNmuYOptdv1NprZcAS3LUzcixPQWY4rjQhBBF4WjcFUt5\nWUQsA5rnfF7C0HXKGqIuGFMMeCgIDMh/bvbFxxYzZsMYAO6uezcTOkyQRxyLmFxtIdxYYmoGK/ef\ntWw/O293nsn9s1VHLIkd4O9nO+d5vIS0BGaFz+KbiG8AmNJlCr1Dezs4amEPSe5CuKlNkecZ+vW/\nAFQL9ONMQir9m+V++kVrzccrsp+NOPRO7zzH5sPjwhm6JPt23OJBiwkpG1IIkQt7yNwyQripv/fF\nWspensY4+J97Tudq98pvey3lg2/nndgBPtz+oaW8ZNASSexOJsldCDeVkJJhKb8zMPtlpNAxizmT\nYCxCfeJCEr/uiAFg69ge+d5sPXzpMDvP7aRrza6EPxxOzbI182wnio4kdyFc3JmEFFbsP8uv26M5\nGHvZUp+YaiT3Z3vUo0v9SjzSIdSyb+OR8wBsPnoBgIHNq1O5rF++5/hq71cAvNjqRUeHL26QjLkL\n4eKe+n6nzVwwUZP7kZSWyZpDcQT4ePJ8z/oADG9fizmbogDIyDJeID8Ym4hS8PF9zfM9/vqY9SyN\nWspDjR8iNDC00L6HuD7ScxfChSWnZ+aa5Gve1pO8s9iY1DU1I8tSH1qhFAHmRax/2noSgDmbotAa\nPKxWUlpwZAHv/fseWmv+OvYXz6x6BoBHmjxSmF9FXCfpuQvhwh6ZvS1X3Zj54Zbyzjd6WsoeHor9\nE3sTOmYx4acSOBWfkucxx28aD8CPB3+01E3sMJHKATLTd3EiyV0IF7M7Op6/9pwm6kIyW6OM1/6H\nt6/FwdhEth63nQYgKMAn3+N8ue4oAHMebW1T/1m3zxi9ZrRle+GAhdQJsn8BbFE0ZFhGCBczcPo/\nfL3xOCsPZL+cNK5fI755pDWDWmS/oPRgu7wfVexnXvN07uYTAHS6paLN/m4h3dhw/waaVWrGooGL\nJLEXU9JzF8KFJCRn5Kp7d1BTfL088fWCT+5vzt3Nq3NbjUAqlPbN8xgv9qrP4vAzlm0vz9x9wCC/\nIL7v+73jAhcOJ8ldiBJq4p/78fZS9Gtajbun/cOgFjVYsCt7Nm5f0vm28s+0C34ZyO6ld2tw7bHx\nGuX8LeVfR7Z3eNyiaCjrtRCLUlhYmN6+fbtTzi1ESRc6ZvE19+94/Q4yl4yhyv7ZRsWYk+AXaPfx\n951KILRiKUr7Sv+vuFFK7dBahxXUTv7LCeFiptx7GxVOLIGrif3WweCZ9xBMfm6tYf8PAlE8yQ1V\nIUqYpLTMXHVX3y4d2jaE/zQKgGWvGztei4F7Z4N3/m+XCtckPXchSpATF5LoMmUtYPTQK5bxpZSP\nF7fWKEv3UB86zw+DPebGj/4NvmWcFqtwLknuQuTh6pj2kUl98M7jaZHr8fuOGEr5etGkell6T13P\n7093oGHVsqzcf5bWoeVJ3PkrwSuf4p/O39Px8lKo1ADajoSstFzJuf//bbSU725e3ZihMSsDpreh\n88Wj2Q3v+gRqdbipuEXJJsldiBxOXEiylA+fuUSTE99DbDiE/2pUPrkeqjWz61jboi7y4q9GV7pO\npVIkpWfRe+oGy/7H66fwxsmnAOi4/sHsD654I7s8/hJ4GD9gUjNMAGx+rbuR2JMuwJQcz5mPiwVv\nf4R7k+QuRA4HzmTPnLh2xnM08Vpk2+DLztB+FGyeBs+FQ0AFQIFPgKVJQkoGzSYst/nYsTjjh8ZL\nXj8zymshi7Pa0O/kVgAOmmrS0CM674AmloO2T5F6xyRMWvNYx9pUC/SH1ATbxN5nCjQfKoldAPIo\npBA2ziSkMH/nKcJXfMdH3l9QShmLQcd41iD40bkQuQrWvpv3h+/7DhrfDcDon3axyGrhC39SGeP1\nE7VVLJ09w20+tjyrFQdvn4aOmM/yc+WI0KFsf/0OKpbygQlB9gX+ZjzIwtNuQR6FFOI6nb2cSvv3\nVlOey+z0m2qpfzj9VdaZmrHevzEhXcPAwxNWv537AL88ZPwd0p7Kpcdaqmf6T6eX/sem6YoKw+h5\n4Qd+y+rMSxlPsr9rfXbWHskns4xl78LeWUnU5H7GTdGfH4Lk8/kH/vIxSewiF+m5C7eltab2a0ss\n27dULk3kuSts8X2GquoS+o4JZNW6nXH/evPzdmPIZMLdTbi7WXVavL3C8rmGVcvw+Pkp/MdrvX0n\n7v4GdH6JlPQsGo1fChhzrGutefK7HSy3WrD6g8G3odH8/G8Ug2I/5SGvlfDfVVCqInh4QWCwA66E\nKEns7blLchdu6889p/nfT7ts6lb5vEhdD/O8KuYbmWcvp9L23VUFHq8CCSyv/BkVLh/IvfOx5VCj\nFXja/rL81fpjdK5fiQZVjadiMrJM1Bv3d77nUAqOv9evwFiE67I3uctLTMItPfLNVj5Zediy3UCd\nJMpvaHZify7c8oRKlWssL2ftAoH80Ow7dj0WxWPpLxET1BpaPQrPbIWQtrkSO8ATnetYEjuAt6cH\ni0Z1zPccy5/rbFcsQkhyF27naNwV1h6K41hcEg2rliFyVHWW+Y4BIF6XgtfjIMh2Otz1L3djTJ+G\nlu3X+zUianI/Pr4v+5FIP28Pthy7wKDPN7Ha1JLd3edC/6nGc+vX4bbgIPa+1StX/R2NKlOviryU\nJOwjN1SF27n/y82Wcrdq6Xh93RWApVmtuf3VBeCVewGLkAoBjOxSl/OJaXy98Tg1yxuPPd7TMpik\ntEzK+nvz7LzdbDIvKA3Q99ZqNxxjWT9vIif14WxiGjWC5NFGcf0kuQu3cuFKGuevpAPggYlXDww2\ndjQeSO/7vi3w86/2aUjbOhXo0TB72tyH2ocC8Oy83Za630a2t1l39EZ4eXpIYhc3TIZlhNtISMmw\nzMuyMWgCx/zMb4SWrgp2JHYwxsR7Nq6SZ+Je81JXAO5sUoWw0PKOCFmIGyY9d+E2vlh7lCtpmQSr\ncwSnHjIqWw6H/p855Pi1K5Yynk0XohiQ5C7cxtJ9Z1CYmOg1B41CPbsHytVydlhCFAq7hmWUUr2V\nUoeUUpFKqTHXaNdaKZWplLrXcSEKcRNidsC8YbDnZ9YmDeS434N099yNCmkviV24tAJ77kopT2A6\n0BOIAbYppRZprffn0e59YHnuowjhJH88BecPwcG/bOsHfeGceIQoIvb03NsAkVrrY1rrdGAeMCCP\ndv8DfgfOOTA+IW5M+G/wVqCR2M1OmCqzKmyG8eZpuVDnxSZEEbBnzL0GYD0XaQzQ1rqBUqoGMAjo\nBrR2WHRCXKfYhFQq7Psa7xXjLHUXhi6j4+zTpOLL6rZdLG+eCuHKHHVDdSrwqtbapK4xO51SagQw\nAiAkJCTfdkLcCFN6Khs//A/3ehoTeP2a2Zm/TO3Z9O1FMjAWiK5TqbQzQxSiyNiT3E8BNa22g811\n1sKAeebEXhHoq5TK1Fr/Yd1Iaz0TmAnGxGE3GrQQNs4dJGvPPDz/+YR7PY2qlVkteDlzpLmB8U/t\n0yHNnROfEE5gT3LfBtRTStXGSOpDgKHWDbTWta+WlVJzgL9yJnYhHE1rzeNTf2N2wn/xtKofk/Ff\n5mV1p1yAN5eSMyz1LUPKFX2QQjhJgclda52plBoFLAM8gdla6wil1Ejz/hmFHKMQedp5NJbZCf+1\nbL+d8SBrTM05pqsDsGt8L84lptJmkjFdb7VA+2Z3FMIV2DXmrrVeAizJUZdnUtdaP3LzYQlxbakZ\nWUyaNY/5xlA6DVLnkIYPH9/XjEmLD/DDE8Y9/8pl/Nj5Rk/SMrPw8pQbqcJ9yBuqosQ5cjaRnp+s\nY4/vBwA0Tf2aNHzY9UZPypXy4Z6WtqsTlS+Ve5ZHIVydJHdRcphMsH0W9Za8RJR5hOWQKZhEjOl3\ny0kSF8JCkrsoOb4bAMdt1yl9ufxn7HmyMwE+nvl8SAj3JMldlAwZKTaJfXzwLMYO68vPnr74S2IX\nIhe5wyRKhlnGsnOfeD1OaOqPVAhthp9/gCR2IfIhyV0Ue5fPHofYvQBMu9IVgHpV5E1TIa5Fkrso\ntrTWhMckUPYL483SEenPk2V+XemORlWcGZoQxZ6MuYti6WDsZXpP3UB9Fc1y87Psy01hALzQsz4+\nXtIvEeJaJLmLYmnIzC14YGK576sA9Eibwh2NqvLGXY0IKR/g5OiEKP4kuYtix2TSxCdnMNf7fQCO\netXlm/89SEgFSepC2EuSuyh2pq2JBDStPYyFNio+t4HA0pLYhbgektxFsaK1ZuqKg0T5PWhUdH+d\nwNKlnBuUECWQ3JUSxcqBM4m087Banrfj884LRogSTHruolio89piTBpqqzOs8X3XqBwXC57yT1SI\nGyE9d+F0yyNiMZnX5ZrgNQeArHajwNvfeUEJUcJJchdOZTJpRny3w7JdQV0GwLP3JGeFJIRLkOQu\nnGpX9CVL+cmOwTTyOYtu86QTIxLCNciApnCqLccuArD8+c7Uv7AGdqRC7dudHJUQJZ/03IVTHThz\nmeqBftSvUgZ+eciorN3FuUEJ4QIkuQun2hMTT+PqgXDW6vFHv7LOC0gIFyHJXThNYmoG0RdTaBES\nBAcWGZUDv3BuUEK4CEnuwmkW7DoFwG3BgbD2PShVCZoPdXJUQrgGSe7CacYvjACgU9IKoyIpzonR\nCOFa5GkZUeRS0rP4cv1RALzIRP3xlLFjxDonRiWEa5Geuyhy7yzez9SVRwD45S4fo7JBP6je3IlR\nCeFaJLmLIhdx+rK5pGm5cohRHDDNafEI4YokuYsilZ5pYt+pBAC+anveqPTyh4DyToxKCNcjyV0U\nqQ6TV5Np0pTyVvTc86xR+cpR5wYlhAuS5C6KzOqDZzl/JQ2A2f0Ds3f4yGIcQjiaJHdRZA7GJlrK\nbY98ahSe2eakaIRwbSUuuadmZHE07grpmSZnh+L6Tu2A2b3B5JhrHXU+CYAtr/WAI8uMygq3OOTY\nQghbdiV3pVRvpdQhpVSkUmpMHvuHKaX2KqXClVKblFLNHB+qYem+WHp8tI7DZxMLbixuzlfd4eRm\nOLrqpg5jMmlemx/OL9tj6FC3AlW5YOzo9AJ4lLj+hRAlQoH/ZymlPIHpQB+gMfCAUqpxjmbHgS5a\n66bA28BMRwd6VSlf472ru/5vo/TeC8uycfCW1Zj4D/dCxAJjcq8lr0BG6nUdbs2hc/y09SQA5f0V\nfGL+5xPSzlERCyFysOcN1TZApNb6GIBSah4wALBM46e13mTVfgsQ7Mggrd1aI3vGwPEL9zF58G2F\ndSr3c/k0LH4RDi3Jve/XR7LLwa2h6b2gVIGHNJk0j3+73bI9LbJX9s56vfL4hBDCEez5nbgGEG21\nHWOuy8/jwN957VBKjVBKbVdKbY+Lu7F5RKoF+hMU4A3AvG3RBbQWdjuzFz5ulJ3YG/WH3pNh+MLc\nbef/FyYEGT38Ahwzj7MDHBuanL2jzQi7fjgIIW6MQ+eWUUp1w0junfLar7WeiXnIJiwsTN/oeXa+\n3pM6Y5fQro68+OIIWWs/wHOt7Zql+r7vUFeT7yvHwb+ckYy/7gkxW436zdPgzrzXOs3MMtH0reWk\nZGQBmii/YTDfvLPlw9B3SuF8GSEEYF/P/RRQ02o72FxnQyl1G/A1MEBrfcEx4eXNw0NRv0ppgvx9\nCvM0Lm9HR7qMAAAQiElEQVT+zhj+++12m8ReO/V7OvotoPZrVkMzAeWze9kPzYeK9bP3xR3O89g7\nT8abEzvUU1b/XGqEwd2fOew7CCHyZk9y3wbUU0rVVkr5AEOARdYNlFIhGP2yh7TWef/f7mCHz15h\naUQssQnXd3NPGKIvJvPCL3tYc+A0ALtNdQlN/RGNB6fiUwAIHbOYNYfOETpmMSv3nzU+6FsGRm2D\nfh8b29Nbw8q3ch3/vi83m0uaFb6vZO/o/2khfSMhhLUCk7vWOhMYBSwDDgC/aK0jlFIjlVIjzc3G\nAxWAz5VSu5VS2/M5nMP976edRXUql5GakcWyiFgAGirjvsXszN55tn30G+Mlo//OzfGftPXjxpww\nABs/MZ6uOR8JcYd5fOrvlmZRfsOyP/NWAlS91UHfQghxLUrrGx76vilhYWF6+/Yb/xmwOzqegdP/\noXeTqsx4qJUDI3NtKelZNBq/1LId5WesfDQsYAZHMipRPcif3dHx+X6+V+MqzBwell3x57OwY06u\ndk+mP89HTaMpfeg3o+KBn6FB3j9AhBD2U0rt0FqHFdSuxC7W0bxmEC1DgoiJTy64sbBYc+icpfxN\nxR/gilH+4ZUHbNp9tPwQ/7c6Mtfnl18dnjGLaPIK9WIP4nNqi039lz6fwCHzxv3fS2IXooiV6NcD\nywX4sO/UZVbkSDgif0//cHUYS9PtymKj+OT6XO1e7NWAqMn98jzG3M1RJCRnEHE6gX4zd1P/6Gh6\npb3Pb3Tn6fTRuT/QqL9jghdC2K3E9twBAsxvqz5hHg8+/l7f7Mf3RC6ZWdlv9DZX5ml22z0N1fKf\nLeJqgo+5lMxvO2KYuvII4xdGWNY/veqwrslLqf8FYESWLzN7l4GOzzr4Gwgh7FWie+5j+za02bYe\nchC5RV0wXijyIpM/wvYZle2esuuzweUCGNj8Wu+uZXtp1LOS2IVwshKd3KsF+nPs3b54ehi99cfm\nFNlDOiXSP5EXGOyxnki/4RD+C1RqCEEhdn8+tGIp9k+806bu5xHtWPF8Z8v26B71qF+ljMNiFkLc\nmBI9LAPGC02H3+lD3bFLLEle5LYk/AxfrD3KFp8Z2ZVxB6/7OAE+XkRN7sdX649Ro5w/betUsNk/\nqptM4StEcVDikzuAp4fioXa1+H1nDCaTxkOSvEV6pokHv/6XrVEX8SUd/IBKjaB8bej1zg0f94nO\ndWy2//pfJ3y8PPDxKtG/DArhMlwiuQPULO9PcnoWdcYukRurVuq/nj2H22DPDUahxxvQMO8nYW7U\nrTUCC24khCgyLtPNalI9O7l0en8NSWmZToymeHj9j3Cb7Xd8vjUKNds6IRohRFFymeTe8ZaKPNy+\nFgCn4lNo8uYyskzOefu2uPh+i7FARtMagcwa3goPnWk89liqopMjE0IUNpdJ7gATBtjOW1J37BIm\n/BlBqnl2Qndyxeo3lwVPd6DHhR+MjSpNnRSREKIouVRyBzj4dm8+uT/7pZxv/oli1QH3e/497J0V\nAHw+rCVeOgNWTTR23PGmE6MSQhQVl0vuft6eDGoRjPUDM8/8uJMoqxWBXF1iagapGcbbqL0aV4H1\nH2bvLF3ZSVEJIYqSyyX3qz4f1orn7qhn2b53xuZrtHYte2MSAPjwP83wyrgC6z8wdrxwwIlRCSGK\nkss8CplT71ur0vvWqpy9nMZPW09y/koaKelZ+Pt4Oju0Qnd1moEOdSvAmreyd5St7pyAhBBFzmV7\n7le9d0/2DcQv1uaewtYVHY5NxM/bg6pl/SDpvFH56gnnBiWEKFIun9zBGJ4AOHLuyg19vqCnbXac\nuMTqg8Vj2uGo80nsjkmgfpUyxn2HqA1Q707wD3J2aEKIIuQWyX1wS2M2w7/3xbLvVILdn9Na0/CN\nv2n4xlJOm9cVzfP4X2ziyTlbeOm79XDuAGgN/35pLD33TtWbjt9e26Iu0vXDteyJjsff2xPiT8KV\ns1C3e5HFIIQoHlx2zN2a9VQED3y1hfC3jJkNtdbXnKYgJSPL8tRJ1Pkkqgf5W1YoOv5eXxaHnyHc\nfPPyiN9wOAp8jvEGaPS/xkEyU4yEX7lRoXy3qz5efoj1R85btp/uUAW2fGFsVGpQqOcWQhQ/bpHc\nAcJqlWP7iUskpmYSOmYxQ1rXZN62aGY/Ekaz4CAqeKWCX6DR6zYn/HOX0yyfj7tilK8uPVf7tSWW\nfQGk2p7samK/6vCyQk3ue6Lj+cxqSbwVz3em3hfB2Q3K1y60cwshiie3GJYB+HVke5vteduiAXh8\nzlYqfFgZJocYwygTgsBk9NYjrcbof995ipT0vMfe9/s9lvdJx54BLz9jaORS1M1/iXwMmP6PpVzW\nz4t6OedTLxdaaOcWQhRPbpPclVKM7p57rnGNB7tNdW0rz+0HIDIuO7mvPxxHo/FLAfAki24euwDN\np7WyE2uT1FnUS52LrlAPBs8i+gqkBNaFLZ/Dp83g10cc/r2sl87rdEtFFjzTES4eMyoa3gXP7nH4\nOYUQxZ/bJHeAF3o1YMHTHSzbvZsYNzsHpr9Ns9SZzMw0T4M7oyMcWUlyjp56D48dNFFRHPV7iG98\nphDlN4wBZ6cDcPE/80nCnwy86JT0AZfq3M3tH6xhh/XMBxELjN8Odv1gqcoyaTKsEvT1upiUbil/\n93gb6vrEw2ctjIpu46TXLoSbcpsx96tahJTjqa51SU7L5K27m3A5JZMhX23hwBl4N3MoI7wWGw1/\nGMzCtI/xUFUZ168xM/7axCyfj/I+aPtRlG/Sg3UvJ9FlylpOxacwc4PRe15qakMnzwjw9IUs8xj+\nwqfBw4tNpXsw9CtjfP7qQtT2OB2fwut/7MPfx5PFe88AMKZPQ+Pm8NIx2Q0L+SauEKL4crvkDvBq\n7+yFtQMDvHm2Rz1Gfr8DUPRLm8Ri33EArPN9gV1BPWnR6Tce29gVm/umgTUhIRrejLfcgK1VoZRl\n9/ebjZeGfsjqQcItA/ikzRW8fn0w+/MLRjA09QfA/LROyiVIugAVC16mburKw6w+aDsZ2uOdzDdN\n0xKNv5+PsMQlhHA/bjUsk5/et1a19JwjdG0in8h+8qRF/AqYUg+VGm9UPPwnvHwMnt8HbyXkSqCf\nDmkOQKJ5yl2NB38eSuaFvcFM8HuF29JnW9pG+Q2jKheopWLh/VCY1grSk687/uqBfnh7esChv+HY\nWmh6HwQGF/g5IYTrkuRuZbJ5qoI7/m8rdVO/y96RZO4l1+kKtTtDqQq5PnvVgOY1LOVHOoRayov2\nnOab+OZcNvnxSPorlvotfv9jne8L2Qf412oB63wkpGTYbK9/pRtciYOfhhgVZasVeAwhhGuT5G5l\nSJsQSzkLTz5ss9G2weBZ13W8RzuG0qpWuVz1a03NWZPVLI9PADHbICMFlo2DK7nnoTeZNMsizhIU\n4M2TXeqw842eeKVegg+thnOq3nZdcQohXI9bjrlfy9F3+1J3rPGCUp2q5WDEOjBlQXAru48x+5Ew\n/j12kVoVSjH3sTa0e28ViamZeHkoXuzVgLL+XrRvuYYzyelkJSdw16driacMB1stxC/iZ5hknrJg\n8zQYuRESTkGZqlC2OvvifQCIT87gtT6NjNgm1sk++SNLoFaHPKISQrgTSe45eHoo3rirMe8s3k//\nZtXB8/rHrrs3rEL3hlUAKOXrxZbXehBzKYX6VUrbTHdQLdAfUxk/4jFeOtrr35Y2/Gx7sBmdbDbv\nSZ0LePHlQ63gzF748vbsnV5+ENrxuuMVQrgeu4ZllFK9lVKHlFKRSqkxeexXSqnPzPv3KqVaOj7U\novN4p9ocf6+fcZPSAUr5etGgapk857Hx8FAsfMZIyEM2Zq+StCIr70sY6TecAFLp9Wcb28QeGAKv\nF4+ZKYUQzldgz10p5QlMB3oCMcA2pdQirfV+q2Z9gHrmP22BL8x/CzvcFhwIgAkPQlN/tNRHeQ7N\ns/1XlX5FJeaY3fK5vYUWnxCi5LFnWKYNEKm1PgaglJoHDACsk/sAYK7WWgNblFJBSqlqWuszDo/Y\nBVn36B/vVJtZG48DmBO95uqz8F5kEuk3nI6JfxuNO78MrR4xHp+UZ9qFEFbsSe41gGir7Rhy98rz\nalMDkORup3cG3kpqRhb3tDTG+O9vXZMsk6bPpxssbcqXKUWqdzX8ks2Xtds4SepCiDwV6Q1VpdQI\nYARASEhIAa3dy4PtalnKb9zV2FJeMvp2Dp29zO87TvHuoKb4ea82XlRq9oAkdiFEvuxJ7qeAmlbb\nwea6622D1nomMBMgLCxMX1ekbqpx9bI0rl6WQS2uPrUTAM3zHosXQoir7HkcZBtQTylVWynlAwwB\nFuVoswgYbn5qph2QIOPtQgjhPAX23LXWmUqpUcAywBOYrbWOUEqNNO+fASwB+gKRQDLwaOGFLIQQ\noiB2jblrrZdgJHDruhlWZQ0849jQhBBC3CiZW0YIIVyQJHchhHBBktyFEMIFSXIXQggXJMldCCFc\nkDIedHHCiZWKA07c4McrAucdGI6rkeuTP7k21ybXJ3/F5drU0lpXKqiR05L7zVBKbddahzk7juJK\nrk/+5Npcm1yf/JW0ayPDMkII4YIkuQshhAsqqcl9prMDKObk+uRPrs21yfXJX4m6NiVyzF0IIcS1\nldSeuxBCiGsoccm9oMW6XZFSarZS6pxSap9VXXml1Aql1BHz3+Ws9r1mvj6HlFJ3WtW3UkqFm/d9\npvJasbuEUUrVVEqtUUrtV0pFKKWeNde7/fVRSvkppbYqpfaYr80Ec73bXxtrSilPpdQupdRf5m3X\nuD5a6xLzB2PK4aNAHcAH2AM0dnZcRfC9OwMtgX1WdR8AY8zlMcD75nJj83XxBWqbr5ened9WoB3G\noqx/A32c/d0ccG2qAS3N5TLAYfM1cPvrY/4epc1lb+Bf8/dz+2uT4zq9APwI/GXedonrU9J67pbF\nurXW6cDVxbpdmtZ6PXAxR/UA4Ftz+VtgoFX9PK11mtb6OMYc+22UUtWAslrrLdr41zjX6jMlltb6\njNZ6p7mcCBzAWL/X7a+PNlwxb3qb/2jk2lgopYKBfsDXVtUucX1KWnLPbyFud1RFZ692FQtUMZfz\nu0Y1zOWc9S5DKRUKtMDoocr1wTLksBs4B6zQWsu1sTUVeAUwWdW5xPUpacld5MHcW3Drx56UUqWB\n34HntNaXrfe58/XRWmdprZtjrGvcRil1a479bnttlFJ3Aee01jvya1OSr09JS+52LcTtJs6afx3E\n/Pc5c31+1+iUuZyzvsRTSnljJPYftNbzzdVyfaxoreOBNUBv5Npc1RG4WykVhTHE210p9T0ucn1K\nWnK3Z7Fud7EIeNhcfhhYaFU/RCnlq5SqDdQDtpp/zbyslGpnvpM/3OozJZb5u8wCDmitP7ba5fbX\nRylVSSkVZC77Az2Bg8i1AUBr/ZrWOlhrHYqRS1ZrrR/EVa6Ps+/oXu8fjIW4D2PcqR7n7HiK6Dv/\nBJwBMjDG8x4HKgCrgCPASqC8Vftx5utzCKu79kAYsM+8bxrml9hK8h+gE8avzXuB3eY/feX6aIDb\ngF3ma7MPGG+ud/trk8e16kr20zIucX3kDVUhhHBBJW1YRgghhB0kuQshhAuS5C6EEC5IkrsQQrgg\nSe5CCOGCJLkLIYQLkuQuhBAuSJK7EEK4oP8HfctHDMEoU48AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d463dfbda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy\n",
    "dataset = df['cum_sum'].reshape(-1,1)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 288\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=10000, verbose=1)\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential  \n",
    "from keras.layers.core import Dense, Activation  \n",
    "from keras.layers.recurrent import LSTM\n",
    "from random import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_out_neurons = 2\n",
    "hidden_neurons = 300\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_neurons, return_sequences=False,\n",
    "               input_shape=(None, in_out_neurons)))\n",
    "model.add(Dense(in_out_neurons, input_dim=hidden_neurons))  \n",
    "model.add(Activation(\"linear\"))  \n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15286 samples, validate on 805 samples\n",
      "Epoch 1/10\n",
      "15286/15286 [==============================] - 70s - loss: 0.0061 - val_loss: 2.4998e-04\n",
      "Epoch 2/10\n",
      "15286/15286 [==============================] - 58s - loss: 7.2655e-04 - val_loss: 8.7122e-04\n",
      "Epoch 3/10\n",
      "15286/15286 [==============================] - 56s - loss: 7.0106e-04 - val_loss: 0.0012\n",
      "Epoch 4/10\n",
      "15286/15286 [==============================] - 48s - loss: 5.3368e-04 - val_loss: 7.1241e-04\n",
      "Epoch 5/10\n",
      " 7200/15286 [=============>................] - ETA: 27s - loss: 5.1655e-04"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[450,100,300]\n\t [[Node: gradients_37/lstm_49/concat_1_grad/Slice_2 = Slice[Index=DT_INT32, T=DT_FLOAT, _class=[\"loc:@lstm_49/concat_1\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients_37/lstm_49/transpose_grad/transpose, gradients_37/lstm_49/concat_1_grad/ConcatOffset:2, gradients_37/lstm_49/concat_1_grad/ShapeN:2)]]\n\nCaused by op 'gradients_37/lstm_49/concat_1_grad/Slice_2', defined at:\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2793, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-116-436c7f2881be>\", line 36, in <module>\n    model.fit(X_train, y_train, batch_size=450, epochs=10, validation_split=0.05)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 856, in fit\n    initial_epoch=initial_epoch)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1481, in fit\n    self._make_train_function()\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1013, in _make_train_function\n    self.total_loss)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py\", line 197, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py\", line 47, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2264, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 560, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 368, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 560, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py\", line 194, in _ConcatGradV2\n    op, grad, start_value_index=0, end_value_index=-1, dim_index=-1)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py\", line 126, in _ConcatGradHelper\n    out_grads.append(array_ops.slice(grad, begin, size))\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 545, in slice\n    return gen_array_ops._slice(input_, begin, size, name=name)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 2933, in _slice\n    name=name)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op 'lstm_49/concat_1', defined at:\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 16 identical lines from previous traceback]\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-111-d6b7e7e9a507>\", line 6, in <module>\n    input_shape=(None, in_out_neurons)))\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 433, in add\n    layer(x)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 243, in __call__\n    return super(Recurrent, self).__call__(inputs, **kwargs)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 585, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 312, in call\n    preprocessed_input = self.preprocess_input(inputs, training=None)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 1056, in preprocess_input\n    return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1681, in concatenate\n    return tf.concat([to_dense(x) for x in tensors], axis)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1034, in concat\n    name=name)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 519, in _concat_v2\n    name=name)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[450,100,300]\n\t [[Node: gradients_37/lstm_49/concat_1_grad/Slice_2 = Slice[Index=DT_INT32, T=DT_FLOAT, _class=[\"loc:@lstm_49/concat_1\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients_37/lstm_49/transpose_grad/transpose, gradients_37/lstm_49/concat_1_grad/ConcatOffset:2, gradients_37/lstm_49/concat_1_grad/ShapeN:2)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[450,100,300]\n\t [[Node: gradients_37/lstm_49/concat_1_grad/Slice_2 = Slice[Index=DT_INT32, T=DT_FLOAT, _class=[\"loc:@lstm_49/concat_1\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients_37/lstm_49/transpose_grad/transpose, gradients_37/lstm_49/concat_1_grad/ConcatOffset:2, gradients_37/lstm_49/concat_1_grad/ShapeN:2)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-436c7f2881be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# batch_size should be appropriate to your memory size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# number of epochs should be higher for real world problems\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m450\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    854\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 856\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1497\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1498\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2229\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2230\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1052\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[450,100,300]\n\t [[Node: gradients_37/lstm_49/concat_1_grad/Slice_2 = Slice[Index=DT_INT32, T=DT_FLOAT, _class=[\"loc:@lstm_49/concat_1\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients_37/lstm_49/transpose_grad/transpose, gradients_37/lstm_49/concat_1_grad/ConcatOffset:2, gradients_37/lstm_49/concat_1_grad/ShapeN:2)]]\n\nCaused by op 'gradients_37/lstm_49/concat_1_grad/Slice_2', defined at:\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2793, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-116-436c7f2881be>\", line 36, in <module>\n    model.fit(X_train, y_train, batch_size=450, epochs=10, validation_split=0.05)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 856, in fit\n    initial_epoch=initial_epoch)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1481, in fit\n    self._make_train_function()\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1013, in _make_train_function\n    self.total_loss)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py\", line 197, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py\", line 47, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2264, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 560, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 368, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 560, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py\", line 194, in _ConcatGradV2\n    op, grad, start_value_index=0, end_value_index=-1, dim_index=-1)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py\", line 126, in _ConcatGradHelper\n    out_grads.append(array_ops.slice(grad, begin, size))\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 545, in slice\n    return gen_array_ops._slice(input_, begin, size, name=name)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 2933, in _slice\n    name=name)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op 'lstm_49/concat_1', defined at:\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 16 identical lines from previous traceback]\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-111-d6b7e7e9a507>\", line 6, in <module>\n    input_shape=(None, in_out_neurons)))\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 433, in add\n    layer(x)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 243, in __call__\n    return super(Recurrent, self).__call__(inputs, **kwargs)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 585, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 312, in call\n    preprocessed_input = self.preprocess_input(inputs, training=None)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 1056, in preprocess_input\n    return K.concatenate([x_i, x_f, x_c, x_o], axis=2)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1681, in concatenate\n    return tf.concat([to_dense(x) for x in tensors], axis)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1034, in concat\n    name=name)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 519, in _concat_v2\n    name=name)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\vales\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[450,100,300]\n\t [[Node: gradients_37/lstm_49/concat_1_grad/Slice_2 = Slice[Index=DT_INT32, T=DT_FLOAT, _class=[\"loc:@lstm_49/concat_1\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients_37/lstm_49/transpose_grad/transpose, gradients_37/lstm_49/concat_1_grad/ConcatOffset:2, gradients_37/lstm_49/concat_1_grad/ShapeN:2)]]\n"
     ]
    }
   ],
   "source": [
    "flow = (list(range(1,10,1)) + list(range(10,1,-1)))*1000  \n",
    "pdata = pd.DataFrame({\"a\":flow, \"b\":flow})  \n",
    "pdata.b = pdata.b.shift(9)  \n",
    "data = pdata.iloc[10:] * random()  # some noise\n",
    "\n",
    "def _load_data(data, n_prev = 100):  \n",
    "    \"\"\"\n",
    "    data should be pd.DataFrame()\n",
    "    \"\"\"\n",
    "\n",
    "    docX, docY = [], []\n",
    "    for i in range(len(data)-n_prev):\n",
    "        docX.append(data.iloc[i:i+n_prev].as_matrix())\n",
    "        docY.append(data.iloc[i+n_prev].as_matrix())\n",
    "    alsX = np.array(docX)\n",
    "    alsY = np.array(docY)\n",
    "\n",
    "    return alsX, alsY\n",
    "\n",
    "def train_test_split(df, test_size=0.1):  \n",
    "    \"\"\"\n",
    "    This just splits data to training and testing parts\n",
    "    \"\"\"\n",
    "    ntrn = round(len(df) * (1 - test_size))\n",
    "\n",
    "    X_train, y_train = _load_data(df.iloc[0:ntrn])\n",
    "    X_test, y_test = _load_data(df.iloc[ntrn:])\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = train_test_split(data)  # retrieve data\n",
    "\n",
    "# and now train the model\n",
    "# batch_size should be appropriate to your memory size\n",
    "# number of epochs should be higher for real world problems\n",
    "model.fit(X_train, y_train, batch_size=450, epochs=10, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)  \n",
    "rmse = np.sqrt(((predicted - y_test) ** 2).mean(axis=0))\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (17, 9)\n",
    "plt.plot(predicted[:100][:,0],\"--\")\n",
    "plt.plot(predicted[:100][:,1],\"--\")\n",
    "plt.plot(y_test[:100][:,0],\":\")\n",
    "plt.plot(y_test[:100][:,1],\":\")\n",
    "plt.legend([\"Prediction 0\", \"Prediction 1\", \"Test 0\", \"Test 1\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
