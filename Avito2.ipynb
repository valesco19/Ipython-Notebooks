{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from operator import itemgetter\n",
    "import zipfile\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import os\n",
    "random.seed(2016)\n",
    "os.chdir('/home/valesco/Datasets/Avito/')\n",
    "from gensim.models import Word2Vec\n",
    "import nltk.corpus\n",
    "from nltk import SnowballStemmer\n",
    "import random as rnd, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load ItemPairs_train.csv\n",
      "Load ItemInfo_train.csv\n"
     ]
    }
   ],
   "source": [
    "def create_feature_map(features):\n",
    "    outfile = open('xgb.fmap','w')\n",
    "    for i, feat in enumerate(features):\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i,feat))\n",
    "    outfile.close()\n",
    "    \n",
    "def get_importance(gbm, features):\n",
    "    create_feature_map(features)\n",
    "    importance = gbm.get_fscore(fmap = 'xgb.fmap')\n",
    "    importance = sorted(importance.items(), key = itemgetter(1), reverse = True)\n",
    "    return importance\n",
    "\n",
    "def intersect(a, b):\n",
    "    return list(set(a) & set(b))\n",
    "\n",
    "def print_features_importance(imp):\n",
    "    for i in range(len(imp)):\n",
    "        print('#' + str(imp[i][1]))\n",
    "        print('output.remove(\\'' + imp[i][0] + '\\')')\n",
    "        \n",
    "def run_default_test(train, test, features, target, random_state = 0):\n",
    "    eta = 0.1\n",
    "    max_depth = 5\n",
    "    subsample = 0.8\n",
    "    colsample_bytree = 0.8\n",
    "    start_time = time.time()\n",
    "    print('XGBoost params. ETA: {}, MAX_DEPTH: {}, SUBSAMPLE: {}, COLSAMPLE_BY_TREE: {}'.format(eta, \n",
    "            max_depth, subsample, colsample_bytree))\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'booster': 'gbtree',\n",
    "        'eval_metric': 'auc',\n",
    "        'eta': eta,\n",
    "        'max_depth': max_depth,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'silent': 1,\n",
    "        'seed': random_state\n",
    "    }\n",
    "    \n",
    "    num_boost_round = 260\n",
    "    early_stopping_rounds = 20\n",
    "    test_size = 0.1\n",
    "    \n",
    "    X_train, X_valid = train_test_split(train, test_size = test_size, random_state = random_state)\n",
    "    y_train = X_train[target]\n",
    "    y_valid = X_valid[target]\n",
    "    dtrain = xgb.DMatrix(X_train[features], y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid[features], y_valid)\n",
    "    \n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    gbm = xgb.train(params, dtrain, num_boost_round, evals = watchlist, early_stopping_rounds = early_stopping_rounds,\n",
    "                   verbose_eval = True)\n",
    "    \n",
    "    print('Validating...')\n",
    "    check = gbm.predict(xgb.DMatrix(X_valid[features]), ntree_limit = gbm.best_ntree_limit)\n",
    "    score = roc_auc_score(X_valid[target].values, check)\n",
    "    print('Check error value: {:.6f}'.format(score))\n",
    "    \n",
    "    imp = get_importance(gbm, features)\n",
    "    print('Importance array: ', imp)\n",
    "    \n",
    "    print('Predict test set...')\n",
    "    test_prediction = gbm.predict(xgb.DMatrix(test[features]), ntree_limit = gbm.best_ntree_limit)\n",
    "    \n",
    "    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "    return test_prediction.tolist(), score\n",
    "\n",
    "def create_submission(score, test, prediction):\n",
    "    now = datetime.datetime.now()\n",
    "    sub_file = 'submission_' + str(score) + '_' + str(now.strftime('%Y-%m-%d-%H-%M')) + '.csv'\n",
    "    print('Writing submission: ', sub_file)\n",
    "    f = open(sub_file, 'w')\n",
    "    f.write('id, probability\\n')\n",
    "    total = 0\n",
    "    for id in test['id']:\n",
    "        str1 = str(id) + ',' + str(prediction[total])\n",
    "        str1 += '\\n'\n",
    "        total += 1\n",
    "        f.write(str1)\n",
    "    f.close()\n",
    "    \n",
    "def get_features(train, test):\n",
    "    trainval = list(train.columns.values)\n",
    "    testval = list(test.columns.values)\n",
    "    output = intersect(trainval, testval)\n",
    "    output.remove('itemID_1')\n",
    "    output.remove('itemID_2')\n",
    "    return output\n",
    "\n",
    "stopw_r = [word for word in nltk.corpus.stopwords.words(\"russian\") if word!=\"не\"]\n",
    "stopw_e = [word for word in nltk.corpus.stopwords.words(\"english\")]\n",
    "stopw = stopw_r + stopw_e\n",
    "stem_r = SnowballStemmer('russian')\n",
    "stem_e = SnowballStemmer('english')\n",
    "\n",
    "def str_stem(s):\n",
    "    s = re.sub(u'[^a-zа-я0-9]', ' ', str(s).lower())\n",
    "    s = (' ').join([stem_r.stem(w) if not re.search(\"[0-9a-z]\", w) else stem_e.stem(w) for w in s.split() if len(w)>1 and w not in stopw])\n",
    "    return s\n",
    "\n",
    "def similarity(bow1, bow2):\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    for word1 in bow1:\n",
    "        for word2 in bow2:\n",
    "            try:\n",
    "                temp = model.similarity(word1, word2)\n",
    "                if temp > .30 or word1 == word2:\n",
    "                    count1 += 1\n",
    "            except:\n",
    "                count2 += 1\n",
    "                pass\n",
    "    \n",
    "    return count1/(len(bow1))\n",
    "\n",
    "def prep_train():\n",
    "    testing = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    types1 = {\n",
    "        'itemID_1': np.dtype(int),\n",
    "        'itemID_2': np.dtype(int),\n",
    "        'isDuplicate': np.dtype(int),\n",
    "        'generationMethod': np.dtype(int),\n",
    "    }\n",
    "\n",
    "    types2 = {\n",
    "        'itemID': np.dtype(int),\n",
    "        'categoryID': np.dtype(int),\n",
    "        'title': np.dtype(str),\n",
    "        'description': np.dtype(str),\n",
    "        'images_array': np.dtype(str),\n",
    "        'attrsJSON': np.dtype(str),\n",
    "        'price': np.dtype(float),\n",
    "        'locationID': np.dtype(int),\n",
    "        'metroID': np.dtype(float),\n",
    "        'lat': np.dtype(float),\n",
    "        'lon': np.dtype(float),\n",
    "    }\n",
    "\n",
    "    print(\"Load ItemPairs_train.csv\")\n",
    "    pairs = pd.read_csv(\"ItemPairs_train.csv\", dtype=types1)\n",
    "    # Add 'id' column for easy merge\n",
    "    print(\"Load ItemInfo_train.csv\")\n",
    "    items = pd.read_csv(\"ItemInfo_train.csv\", dtype=types2)\n",
    "    items.fillna(-1, inplace=True)\n",
    "    location = pd.read_csv(\"Location.csv\")\n",
    "    category = pd.read_csv(\"Category.csv\")\n",
    "\n",
    "    train = pairs\n",
    "    train = train.drop(['generationMethod'], axis=1)\n",
    "\n",
    "    print('Add text features...')\n",
    "    items['len_title'] = items['title'].str.len()\n",
    "    items['len_description'] = items['description'].str.len()\n",
    "    items['len_attrsJSON'] = items['attrsJSON'].str.len()\n",
    "    items['num_words_title'] = items['title'].map(lambda x: len(str(x).split()))\n",
    "    items['num_words_description'] = items['description'].map(lambda x: len(str(x).split()))\n",
    "    start = time.time()\n",
    "    items['title'] = items['title'].map(lambda x: str_stem(x))\n",
    "    items['description'] = items['description'].map(lambda x: str_stem(x))\n",
    "    finish = time.time()\n",
    "    print('Time to stem/stopword removal on training data: ', str(start/finish)/60)\n",
    "\n",
    "    print('Merge item 1...')\n",
    "    item1 = items[['itemID', 'categoryID', 'price', 'locationID', 'metroID', 'lat', 'lon', \n",
    "    'len_title', 'len_description', 'len_attrsJSON', 'num_words_title', \n",
    "    'num_words_description', 'title', 'description']]\n",
    "    item1 = pd.merge(item1, category, how='left', on='categoryID', left_index=True)\n",
    "    item1 = pd.merge(item1, location, how='left', on='locationID', left_index=True)\n",
    "\n",
    "    item1 = item1.rename(\n",
    "        columns={\n",
    "            'itemID': 'itemID_1',\n",
    "            'categoryID': 'categoryID_1',\n",
    "            'parentCategoryID': 'parentCategoryID_1',\n",
    "            'price': 'price_1',\n",
    "            'locationID': 'locationID_1',\n",
    "            'regionID': 'regionID_1',\n",
    "            'metroID': 'metroID_1',\n",
    "            'lat': 'lat_1',\n",
    "            'lon': 'lon_1',\n",
    "            'len_title': 'len_title_1',\n",
    "\t\t\t'len_description': 'len_description_1',\n",
    "\t\t\t'len_attrsJSON': 'len_attrsJSON_1',\n",
    "            'num_words_title': 'num_words_title_1',\n",
    "            'num_words_description': 'num_words_description_1',\n",
    "            'title': 'title_1',\n",
    "            'description': 'description_1'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add item 1 data\n",
    "    train = pd.merge(train, item1, how='left', on='itemID_1', left_index=True)\n",
    "\n",
    "    print('Merge item 2...')\n",
    "    item2 = items[['itemID', 'categoryID', 'price', 'locationID', 'metroID', 'lat', 'lon', \n",
    "    'len_title', 'len_description', 'len_attrsJSON', 'num_words_title', \n",
    "    'num_words_description', 'title', 'description']]\n",
    "    item2 = pd.merge(item2, category, how='left', on='categoryID', left_index=True)\n",
    "    item2 = pd.merge(item2, location, how='left', on='locationID', left_index=True)\n",
    "\n",
    "    item2 = item2.rename(\n",
    "        columns={\n",
    "            'itemID': 'itemID_2',\n",
    "            'categoryID': 'categoryID_2',\n",
    "            'parentCategoryID': 'parentCategoryID_2',\n",
    "            'price': 'price_2',\n",
    "            'locationID': 'locationID_2',\n",
    "            'regionID': 'regionID_2',\n",
    "            'metroID': 'metroID_2',\n",
    "            'lat': 'lat_2',\n",
    "            'lon': 'lon_2',\n",
    "            'len_title': 'len_title_2',\n",
    "\t\t\t'len_description': 'len_description_2',\n",
    "\t\t\t'len_attrsJSON': 'len_attrsJSON_2',\n",
    "            'num_words_title': 'num_words_title_2',\n",
    "            'num_words_description': 'num_words_description_2',\n",
    "            'title': 'title_2',\n",
    "            'description': 'description_2'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add item 2 data\n",
    "    train = pd.merge(train, item2, how='left', on='itemID_2', left_index=True)\n",
    "\n",
    "    # Create same arrays\n",
    "    print('Create same arrays')\n",
    "    train['price_same'] = np.equal(train['price_1'], train['price_2']).astype(np.int32)\n",
    "    train['locationID_same'] = np.equal(train['locationID_1'], train['locationID_2']).astype(np.int32)\n",
    "    train['categoryID_same'] = np.equal(train['categoryID_1'], train['categoryID_2']).astype(np.int32)\n",
    "    train['regionID_same'] = np.equal(train['regionID_1'], train['regionID_2']).astype(np.int32)\n",
    "    train['metroID_same'] = np.equal(train['metroID_1'], train['metroID_2']).astype(np.int32)\n",
    "    train['lat_same'] = np.equal(train['lat_1'], train['lat_2']).astype(np.int32)\n",
    "    train['lon_same'] = np.equal(train['lon_1'], train['lon_2']).astype(np.int32)\n",
    "    train['desc_sim'] = train['description_1'].map(lambda x: (similarity(x.split(' '), str(train['description_2'][2]).split(' '))))\n",
    "    train.drop('description', axis = 1 , inplace = True)\n",
    "    train.drop('title', axis = 1, inplace = True)\n",
    "\n",
    "    # print(train.describe())\n",
    "    print('Create train data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    return train\n",
    "\n",
    "\n",
    "def prep_test():\n",
    "    start_time = time.time()\n",
    "\n",
    "    types1 = {\n",
    "        'itemID_1': np.dtype(int),\n",
    "        'itemID_2': np.dtype(int),\n",
    "        'id': np.dtype(int),\n",
    "    }\n",
    "\n",
    "    types2 = {\n",
    "        'itemID': np.dtype(int),\n",
    "        'categoryID': np.dtype(int),\n",
    "        'title': np.dtype(str),\n",
    "        'description': np.dtype(str),\n",
    "        'images_array': np.dtype(str),\n",
    "        'attrsJSON': np.dtype(str),\n",
    "        'price': np.dtype(float),\n",
    "        'locationID': np.dtype(int),\n",
    "        'metroID': np.dtype(float),\n",
    "        'lat': np.dtype(float),\n",
    "        'lon': np.dtype(float),\n",
    "    }\n",
    "\n",
    "    print(\"Load ItemPairs_test.csv\")\n",
    "    pairs = pd.read_csv(\"ItemPairs_test.csv\", dtype=types1)\n",
    "    print(\"Load ItemInfo_testcsv\")\n",
    "    items = pd.read_csv(\"ItemInfo_test.csv\", dtype=types2)\n",
    "    items.fillna(-1, inplace=True)\n",
    "    location = pd.read_csv(\"Location.csv\")\n",
    "    category = pd.read_csv(\"Category.csv\")\n",
    "\n",
    "    train = pairs\n",
    "\n",
    "    print('Add text features...')\n",
    "    items['len_title'] = items['title'].str.len()\n",
    "    items['len_description'] = items['description'].str.len()\n",
    "    items['len_attrsJSON'] = items['attrsJSON'].str.len()\n",
    "    items['num_words_title'] = items['title'].map(lambda x: len(str(x).split()))\n",
    "    items['num_words_description'] = items['description'].map(lambda x: len(str(x).split()))\n",
    "    items['title'] = items['title'].map(lambda x: str_stem(x))\n",
    "    items['description'] = items['description'].map(lambda x: str_stem(x))\n",
    "    \n",
    "    print('Merge item 1...')\n",
    "    item1 = items[['itemID', 'categoryID', 'price', 'locationID', 'metroID', 'lat', 'lon', \n",
    "    'len_title', 'len_description', 'len_attrsJSON', 'num_words_title', \n",
    "    'num_words_description', 'title', 'description']]\n",
    "    item1 = pd.merge(item1, category, how='left', on='categoryID', left_index=True)\n",
    "    item1 = pd.merge(item1, location, how='left', on='locationID', left_index=True)\n",
    "\n",
    "    item1 = item1.rename(\n",
    "        columns={\n",
    "            'itemID': 'itemID_1',\n",
    "            'categoryID': 'categoryID_1',\n",
    "            'parentCategoryID': 'parentCategoryID_1',\n",
    "            'price': 'price_1',\n",
    "            'locationID': 'locationID_1',\n",
    "            'regionID': 'regionID_1',\n",
    "            'metroID': 'metroID_1',\n",
    "            'lat': 'lat_1',\n",
    "            'lon': 'lon_1',\n",
    "            'len_title': 'len_title_1',\n",
    "\t\t\t'len_description': 'len_description_1',\n",
    "\t\t\t'len_attrsJSON': 'len_attrsJSON_1',\n",
    "            'num_words_title': 'num_words_title_1',\n",
    "            'num_words_description': 'num_words_description_1',\n",
    "            'title': 'title_1',\n",
    "            'description': 'description_1'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add item 1 data\n",
    "    train = pd.merge(train, item1, how='left', on='itemID_1', left_index=True)\n",
    "\n",
    "    print('Merge item 2...')\n",
    "    item2 = items[['itemID', 'categoryID', 'price', 'locationID', 'metroID', 'lat', 'lon', \n",
    "    'len_title', 'len_description', 'len_attrsJSON', 'num_words_title', \n",
    "    'num_words_description', 'title', 'description']]\n",
    "    item2 = pd.merge(item2, category, how='left', on='categoryID', left_index=True)\n",
    "    item2 = pd.merge(item2, location, how='left', on='locationID', left_index=True)\n",
    "\n",
    "    item2 = item2.rename(\n",
    "        columns={\n",
    "            'itemID': 'itemID_2',\n",
    "            'categoryID': 'categoryID_2',\n",
    "            'parentCategoryID': 'parentCategoryID_2',\n",
    "            'price': 'price_2',\n",
    "            'locationID': 'locationID_2',\n",
    "            'regionID': 'regionID_2',\n",
    "            'metroID': 'metroID_2',\n",
    "            'lat': 'lat_2',\n",
    "            'lon': 'lon_2',\n",
    "            'len_title': 'len_title_2',\n",
    "\t\t\t'len_description': 'len_description_2',\n",
    "\t\t\t'len_attrsJSON': 'len_attrsJSON_2',\n",
    "            'num_words_title': 'num_words_title_2',\n",
    "            'num_words_description': 'num_words_description_2',\n",
    "            'title': 'title_2',\n",
    "            'description': 'description_2'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add item 2 data\n",
    "    train = pd.merge(train, item2, how='left', on='itemID_2', left_index=True)\n",
    "\n",
    "    # Create same arrays\n",
    "    print('Create same arrays')\n",
    "    train['price_same'] = np.equal(train['price_1'], train['price_2']).astype(np.int32)\n",
    "    train['locationID_same'] = np.equal(train['locationID_1'], train['locationID_2']).astype(np.int32)\n",
    "    train['categoryID_same'] = np.equal(train['categoryID_1'], train['categoryID_2']).astype(np.int32)\n",
    "    train['regionID_same'] = np.equal(train['regionID_1'], train['regionID_2']).astype(np.int32)\n",
    "    train['metroID_same'] = np.equal(train['metroID_1'], train['metroID_2']).astype(np.int32)\n",
    "    train['lat_same'] = np.equal(train['lat_1'], train['lat_2']).astype(np.int32)\n",
    "    train['lon_same'] = np.equal(train['lon_1'], train['lon_2']).astype(np.int32)\n",
    "    train['desc_sim'] = train['description_1'].map(lambda x: (similarity(x.split(' '), str(train['description_2'][2]).split(' '))))\n",
    "    train.drop('description', axis = 1 , inplace = True)\n",
    "    train.drop('title', axis = 1, inplace = True)\n",
    "\n",
    "    # print(train.describe())\n",
    "    print('Create test data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    return train\n",
    "\n",
    "\n",
    "def read_test_train():\n",
    "    train = prep_train()\n",
    "    test = prep_test()\n",
    "    train.fillna(-1, inplace=True)\n",
    "    test.fillna(-1, inplace=True)\n",
    "    # Get only subset of data\n",
    "    if 1:\n",
    "        len_old = len(train.index)\n",
    "        train = train.sample(frac=0.5)\n",
    "        len_new = len(train.index)\n",
    "        print('Reduce train from {} to {}'.format(len_old, len_new))\n",
    "    features = get_features(train, test)\n",
    "    return train, test, features\n",
    "\n",
    "\n",
    "train, test, features = read_test_train()\n",
    "print('Length of train: ', len(train))\n",
    "print('Length of test: ', len(test))\n",
    "print('Features [{}]: {}'.format(len(features), sorted(features)))\n",
    "test_prediction, score = run_default_test(train, test, features, 'isDuplicate')\n",
    "print('Real score = {}'.format(score))\n",
    "create_submission(score, test, test_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "types1 = {\n",
    "    'itemID_1': np.dtype(int),\n",
    "    'itemID_2': np.dtype(int),\n",
    "    'isDuplicate': np.dtype(int),\n",
    "    'generationMethod': np.dtype(int),\n",
    "}\n",
    "\n",
    "types2 = {\n",
    "    'itemID': np.dtype(int),\n",
    "    'categoryID': np.dtype(int),\n",
    "    'title': np.dtype(str),\n",
    "    'description': np.dtype(str),\n",
    "    'images_array': np.dtype(str),\n",
    "    'attrsJSON': np.dtype(str),\n",
    "    'price': np.dtype(float),\n",
    "    'locationID': np.dtype(int),\n",
    "    'metroID': np.dtype(float),\n",
    "    'lat': np.dtype(float),\n",
    "    'lon': np.dtype(float),\n",
    "}\n",
    "\n",
    "items = pd.read_csv(\"ItemInfo_train.csv\", dtype=types2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load_word2vec_format('ruscorpora.model.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = items[:100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
